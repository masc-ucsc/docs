{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LiveHD and Pyrope","text":"<p>LiveHD is the compiler infrastructure that allows to work with Hardware Description Languages (HDLs) like Verilog, CHISEL, and Pyrope.</p> <p>Pyrope is the new HDL built on top of LiveHD.</p> <p>Hardware Design provides an introduction to hardware design for software designers.</p>"},{"location":"livehd/00-intro/","title":"Introduction","text":"<p>LiveHD is an infrastructure designed for Live Hardware Development. By live, we mean that small changes in the design should have the synthesis and simulation results in a few seconds.</p> <p>As the goal of \"seconds,\" we do not need to perform too fine grain incremental work. Notice that this is a different goal from having an typical incremental synthesis, where many edges are added and removed in the order of thousands of nodes/edges.</p>"},{"location":"livehd/00-intro/#goal","title":"Goal","text":"<p>LiveHD: a fast and friendly hardware development flow that you can trust</p> <ul> <li>To be \"Fast\", LiveHD aims to be parallel, scalable, and incremental/live flow.</li> <li>To be \"friendly\", LiveHD aims to build new models to have good error reporting.</li> <li>To \"trust\", LiveHD has CI and many random tests with logic equivalence tests (LEC).</li> </ul>"},{"location":"livehd/00-intro/#livehd-framework","title":"LiveHD Framework","text":"<p>LiveHD is optimized for synthesis and simulation. The main components of LiveHD includes LGraph, LNAST, integrated 3rd-party tools, code generation, and \"live\" techniques.</p> <p>A compilation goes through the following steps:</p> <ul> <li> <p>Source code goes through a lexer/parser to create a parse tree or language specific AST.</p> </li> <li> <p>The parse tree or language specific AST is translated to LNAST. LNAST is a AST-like representation that it is independent of the specific language. Pyrope, CHISEL, and Verilog translate to LNAST.</p> </li> <li> <p>The are several passes on LNAST to infer the correct type and bitsizes. The goal is to expand tuples, macros at LNAST level, but this code is still not finished.</p> </li> <li> <p>The LNAST is a tree-like representation which is translated to Lgraph. In a way, LNAST is a HIR (High-level IR) and Lgraph is a LIR (Low-level IR). For each Lgraph node, there is an equivalent LNAST, but not the opposite.</p> </li> <li> <p>LGraph has a hierarchical graph representation designed for fast synthesis and simulation. It interfaces other tools like Yosys, ABC, OpenTimer, and Mockturtle.</p> </li> <li> <p>For code generation, it is possible to translate back to LNAST or to directly output from Lgraph.</p> </li> </ul> <p></p>"},{"location":"livehd/01-install/","title":"Installation","text":"<p>This is a high level description of how to build LiveHD.</p>"},{"location":"livehd/01-install/#requirements","title":"Requirements","text":"<p>Although LiveHD should run on most common Linux distributions, it is heavily tested on both Arch and Kali (Debian based).</p> <p>The following programs are assumed to be present when building LiveHD:</p> <ul> <li>GCC 12+ or Clang 12+ (C++20 support is required)</li> <li>Bazel</li> <li>python3</li> </ul> <p>It is also assumed that bash is used to compile LiveHD.</p> <p>gcc and clang offers better warnings and execution speed dependent of the benchmark.</p> <p>If you're unsure if your copy of gcc or clang is new enough, you can check the version by typing</p> <pre><code>g++ --version\n</code></pre> <p>or</p> <pre><code>clang++ --version\n</code></pre>"},{"location":"livehd/01-install/#steps","title":"Steps","text":"<p>Download LiveHD source</p> <pre><code>git clone https://github.com/masc-ucsc/livehd\n</code></pre> <p>Install Bazelisk</p> <p>Bazelisk is a wrapper around bazel that allows you to use a specific version.</p> <p>If you do not have system permissions, you can install a local bazelisk</p> <pre><code>npm install  @bazel/bazelisk\nalias bazel=$(pwd)/node_modules/\\@bazel/bazelisk/bazelisk.js\n</code></pre> <p>You can also install it directly if you have administrative permissions:</p> <p>macos: <pre><code>brew install bazelisk.\n</code></pre></p> <p>Linux: <pre><code>npm install -g @bazel/bazelisk\n</code></pre></p> <pre><code>go install github.com/bazelbuild/bazelisk@latest\nexport PATH=$PATH:$(go env GOPATH)/bin\n</code></pre> <p>Arch linux: <pre><code>pacaur -S bazelisk  # or yay or paru installers\n</code></pre></p> <p>Build LiveHD</p> <p>LiveHD has several build options, detailed below. All three should result in a working executable, but may differ in speed or output.</p> <p>A binary will be created in <code>livehd/bazel-bin/main/lgshell</code>.</p> <pre><code>bazel build       //main:all # fast build, no debug symbols, slow execution (default)\nbazel build -copt //main:all # fastest execution speed, no debug symbols, no assertions\nbazel build -cdbg //main:all # moderate execution speed, debug symbols\n</code></pre>"},{"location":"livehd/01-install/#potential-issues","title":"Potential issues","text":"<p>If you have multiple gcc versions, you may need to specify the latest. E.g:</p> <pre><code>CXX=g++-8 CC=gcc-8 bazel build //main:all -c opt # fast execution for benchmarking\nCXX=g++-8 CC=gcc-8 bazel build //main:all -c dbg # debugging/development\n</code></pre> <p>If you want to run clang specific version:</p> <pre><code>CXX=clang++-10 CC=clang-10 bazel build //main:all -c dbg # debugging/development\n</code></pre> <p>Make sure that the openJDK installed is compatible with bazel and has the certificates to use tools. E.g in debian:</p> <pre><code>dpkg-reconfigure openjdk-11-jdk\n/var/lib/dpkg/ca-certificates-java.postinst configure\n</code></pre> <p>If you fail to build for the first time, you may need to clear the cache under your home directory before rebuilding:</p> <pre><code>rm -rf ~/.cache/bazel\n</code></pre> <p>Make sure to have enough memory (4+GB at least)</p>"},{"location":"livehd/01-install/#next-steps","title":"Next Steps","text":"<p>To start using LiveHD, check out Usage. If you're interested in working on LiveHD, refer to Creating a pass.</p>"},{"location":"livehd/02-usage/","title":"Usage","text":"<p>This is a high level description of how to use LiveHD.</p>"},{"location":"livehd/02-usage/#sample-usage","title":"Sample usage","text":"<p>Below are some sample usages of the LiveHD shell (lgshell). A bash prompt is indicated by <code>$</code>, an lgshell prompt is indicated by <code>livehd&gt;</code>, and a Yosys prompt is indicated by a <code>yosys&gt;</code>. Lgshell supports color output and autocompletion using the tab key.</p>"},{"location":"livehd/02-usage/#general-concepts","title":"General concepts","text":"<p>Currently, LiveHD can interface Verilog, FIRRTL, and Pyrope HDLs through different front-end commands. After the parsing step, the HDL source can be be transformed into LiveHD's internal intermediate representations, LNAST and LGraph, and perform mid-end optimizations based on the two IRs, and generates the optimized (or synthesized) Verilog code at the back-end. </p>"},{"location":"livehd/02-usage/#starting-and-exiting-the-shell","title":"Starting and exiting the shell","text":"<pre><code>$ ./bazel-bin/main/lgshell\nlivehd&gt; help\n  ...\nlivehd&gt; help pass.sample\nlivehd&gt; exit\n</code></pre>"},{"location":"livehd/02-usage/#verilog-compilation","title":"Verilog Compilation","text":"<p>The following uses Verilog as the example to demonstrate the compilations commands.  It imports a Verilog file with a specified the database path, translates to the LNAST IR, lowers the LNAST IR to LGraph IR, executes some compiler optimizations, and generates the optimized Verilog code to the <code>tmp</code> directory. By default, a database called <code>lgdb</code> will be set in the <code>livehd</code> directory to store the internal representations, but users can optionally specify a prefered path. </p> <pre><code>livehd&gt; inou.liveparse path:/your/path/lgdb_foo files:/your/path/bar.v |&gt; inou.verilog |&gt; pass.lnast_tolg |&gt; pass.cprop |&gt; pass.bitwidth |&gt; inou.cgen.verilog odir:tmp\n</code></pre> <p>A command <code>lgraph.match</code> can also be used to specify a (sub)hierarchy to operate over, which can then be moved from pass to pass using the pipe (<code>|&gt;</code>) operator.</p> <p>When Verilog file(s) are compiled through a series of commands in lgshell, if a problem occurs while compiling Verilog files (due to a syntax error, use of un-synthesizable Verilog, or something else), the corresponding error will be printed. Once a hierarchy has been created, other lgshell commands can read, modify, or export this hierarchy freely.</p>"},{"location":"livehd/02-usage/#pyrope-compilation","title":"Pyrope Compilation","text":"<p>The Pyrope compilation flow is similar to the Verilog commands except the front-end Pyrope parser pass <code>inou.pyrope</code></p> <pre><code>livehd&gt; inou.pyrope path:/your/path/lgdb_foo files:/your/path/bar.prp |&gt; pass.lnast_tolg |&gt; pass.cprop |&gt; pass.bitwidth |&gt; inou.cgen.verilog odir:tmp\n</code></pre> <p>Additionally, users can compile a Pyrope code with a mid-end command of <code>pass.compiler</code> which integrates standard compilation passes in LiveHD such as (1) <code>pass.lnast_tolg</code> for IR lowering, (2) <code>pass.cprop</code> for legacy compiler optimizations such as copy and constant propagation, peep-hole optimization, deadcode elimination, and high-level data-structure resolving (3) <code>pass.bitwidth</code> for circuit bitwidth optimization.</p> <pre><code>livehd&gt; inou.pyrope path:/your/path/lgdb_foo files:/your/path/bar.prp |&gt; pass.compiler |&gt; inou.cgen.verilog odir:tmp\n</code></pre>"},{"location":"livehd/02-usage/#firrtl-compilation","title":"FIRRTL Compilation","text":"<p>LiveHD compiles FIRRTL code with the protocal buffer format. Users can reference this doc to generate the protocol buffers file from Chisel/FIRRTL compiler.</p> <p>The LiveHD FIRRTL compiler uses a integrated mid-end commands as explained in the previous Pyrope example to compiles the FIRRTL HDL. Set <code>gviz</code> option to true to automatically generate the visiual Graphviz for individual steps. Set <code>hier</code> option to true for hierarchical Chisel design in most cases. Specify the top module name with the <code>top</code> option.</p> <pre><code>livehd&gt; inou.firrtl.tolnast path:/your/path/lgdb_foo files:/your/path/bar.pb |&gt; pass.compiler gviz:false top:top_module_name hier:true |&gt; inou.cgen.verilog odir:tmp\n</code></pre>"},{"location":"livehd/02-usage/#textual-lnast-ir-dump","title":"Textual LNAST IR Dump","text":"<p>To display the content of the LNAST IR after parse (Pyrope as the example)</p> <p><code>inou.pyrope files:foo.prp |&gt; lnast.dump</code></p>"},{"location":"livehd/02-usage/#textual-lgraph-ir-dump","title":"Textual LGraph IR Dump","text":"<p>To display the content of the LGraph IR (Pyrope as the example)</p> <p><code>inou.pyrope files:foo.prp |&gt; pass.lnast_tolg |&gt; lgraph.dump</code></p>"},{"location":"livehd/02-usage/#lgraph-serialization","title":"LGraph serialization","text":"<p>To serialize the content of the LGraph IR (Pyrope as the example), default path is <code>lgdb</code> <code>inou.pyrope files:foo.prp |&gt; pass.lnast_tolg |&gt; lgraph.save</code></p>"},{"location":"livehd/02-usage/#lgraph-deserialization","title":"LGraph deserialization","text":"<p>To deserialize a stored LGraph, and pass it to some lgraph passes  <code>lgraph.open name:foo |&gt; pass.cprop |&gt; pass.bitwidth |&gt; inou.cgen.verilog odir:tmp</code></p>"},{"location":"livehd/02-usage/#graphviz-lgraph-ir-dump","title":"Graphviz LGraph IR Dump","text":"<p>To display the content of the LGraph IR (Pyrope after the <code>cprop</code> pass as the example) <code>inou.pyrope files:foo.prp |&gt; pass.lnast_tolg |&gt; pass.cprop |&gt; inou.graphviz.from</code></p> <ul> <li>Print information about an existing LGraph:   <pre><code>$ ./bazel-bin/main/lgshell\nlivehd&gt; inou.liveparse files:./inou/yosys/tests/trivial.v |&gt; inou.verilog\nlivehd&gt; lgraph.match |&gt; lgraph.stats\nlivehd&gt; lgraph.match |&gt; lgraph.dump\n</code></pre> <code>lgraph.match</code> picks up any LGraphs matching the regex passed (or everything if no regex is provided) and treats every single one as the top of the hierarchy, whereas <code>lgraph.open name:&lt;root module&gt;</code> will just open the root module as the top of the hierarchy.</li> </ul>"},{"location":"livehd/02-usage/#running-a-custom-pass","title":"Running a custom pass","text":"<pre><code>$ ./bazel-bin/main/lgshell\nlivehd&gt; inou.pyrope files:./inou/pyrope/tests/if1.prp\nlivehd&gt; lgraph.match |&gt; &lt;pass name&gt;\n</code></pre>"},{"location":"livehd/02-usage/#low-level-directed-build","title":"Low level directed build","text":"<ul> <li>To compile an individual pass:   <pre><code>$ bazel build -c dbg //pass/sample:pass_sample\n$ bazel build -c dbg //inou/yosys:all\n</code></pre></li> <li>To build a direct Yosys executable that has LiveHD embedded:   <pre><code>$ bazel build -c dbg //inou/yosys:all\n$./bazel-bin/inou/yosys/yosys2\n</code></pre></li> </ul>"},{"location":"livehd/02-usage/#parallel-compilation","title":"Parallel Compilation","text":"<p>LiveHD support parallel compilation, it will set the thread number to the highest available resources in your system. To manually set the compilation thread number, for example, 1, set the following environment variable <pre><code>export LIVEHD_THREADS=1\n</code></pre></p>"},{"location":"livehd/03-memory/","title":"Memory and concurrency","text":"<p>This section explains the memory management or garbage collect principles used and the relationship with concurrency models.</p>"},{"location":"livehd/03-memory/#memory-management-models","title":"Memory management models","text":"<p>Memory management in a multi-threaded environment main challenge arises from deletes and additions that can trigger memory allocation/de-allocation. In LiveHD, we address this problem the following way:</p> <ul> <li> <p>Only one thread can allocate/deallocate memory in an object at a given time.   To allow updates, a RW access is required. Otherwise, a RD access is enough.   Both return a std::unique_ptr. The API guarantees that only one thread can do   RW-access for a given object. Notice that the calling thread can have   multiple RD access and WR access to the same object simulnateusly. The check   is only against \"other threads\". There are 2 APIs: </p> </li> <li> <p>ref_rd_snapshot(): gets a RD access, and an assertion checks that     there is no other thread has wr_snapshot during the lifetime of the     returned std::unique_ptr</p> </li> <li> <p>ref_wr_snapshot(): gets a RW access, and an assertion checks that     there is no other thread has rd_snapshot during the lifetime of the     returned std::unique_ptr.</p> </li> </ul> <p>Note</p> <p>Updating an atomic counter inside an object does not require a RW-access, but a rd_snapshot because it does not trigger memory allocation/deallocation. Adding/deleting elements requires a wr_snapshot.</p> <ul> <li> <p>Creating an object requires a ref_wr_snapshot.</p> </li> <li> <p>The only way to pass references across threads is with std::unique_ptr or   calling to the library which will create a std::unique_ptr. </p> </li> </ul> <p>Note</p> <p>If a std::unique_ptr created from a snapshot is passed to another thread, the creator thread is still the owner of the thread pointer. If this is not the intention, it may be safer for the new thread to call the library to access ownership.</p> <p>This approach is somewhat similar to a hazzard pointer. The snapshot API indicates intention to modify (which can delete), but instead of failing with a nullptr return, we trigger a compile failure because it should never be the case. Each lgraph/lnast can be updated in parallel, but only if they are independent. The assertion is to check that there is no bug.</p> <p>LiveHD uses 3 main techniques to perform memory management.</p>"},{"location":"livehd/03-memory/#raii-or-stdunique_ptr","title":"RAII or std::unique_ptr","text":"<p>When an object is created and there is a single user, the code should use RAII or std::unique_ptr. RAII means that when the object is out of scope, the memory is recycled. std::unique_ptr will automatically call the destructor when the reference use is zero.</p> <ul> <li>In LiveHD, RAII references should not be passed between threads.</li> <li>std::unique_ptr can be passed between threads.</li> </ul>"},{"location":"livehd/03-memory/#snapshot","title":"snapshot","text":"<p>For objects that can be shared across threads, the snapshot API must be used.</p>"},{"location":"livehd/03-memory/#stdshared_ptr","title":"std::shared_ptr","text":"<p>std::shared_ptr is one of the \"heavy\" cost options for garbage collection. In LiveHD, we do not use the atomic std::shared_ptr. The std::shared_ptr are NOT allowed to be passed between threads. It is a memory management for data only within a thread or compiler pass.</p> <p>To avoid reference counting overheads, when passed to methods, a <code>const std::shared&lt;XX&gt; &amp;</code> should be used nearly in all the cases.</p>"},{"location":"livehd/04-api/","title":"C++ API","text":"<p>LiveHD is built on C++17, LGraph and LNAST are the two key data structures inside LiveHD to support new hardware design. * LGraph stands for Live Graph. It is graph or netlist data   structure at the core of LiveHD. * LNAST stands for Language Neutral AST. It is an Abstract   Syntax Tree (AST) designed to be simple but to allow the translation from   multiple languages like CHIRRTL, Verilog, and Pyrope.</p> <p>While LNAST could be seen as a high level API with control flow information, LGraph is a lower level graph API where many LNAST high level constructs are simplified.</p> <p>There is a division of functionality between LNAST and LGraph:</p> <ul> <li> <p>LNAST: Language Neutral AST, the high level tree based representation/API</p> <ul> <li>Bundles:<ul> <li>Flatten fields (only flat attributes passed to LGraph)</li> <li>Find IOs (inputs and outputs). Populate the sub_node accordingly.</li> <li>Detect as array if legal bundle index.</li> </ul> </li> <li>Constant propagation (comptime decision)</li> <li>Linear time compiler passes (dead code elimination, constant folding) but not complex (GVN, SAT...)</li> <li>Lgraph creation<ul> <li>Inline small LNASTs</li> <li>Partition too large LGraphs</li> </ul> </li> <li>Type checking </li> <li>Unroll <code>for</code> and <code>while</code> loops </li> </ul> </li> <li> <p>LGraph: Live Graph, the low level graph/netlist level based representation/API</p> <ul> <li>Attributes<ul> <li>Bitwidth</li> <li>Debug flag</li> </ul> </li> <li>Complex optimizations<ul> <li>cprop (Peephole, constant folding, ...)</li> <li>lecopt, Logic Equivalence based optimizations</li> <li>synthesis</li> </ul> </li> </ul> </li> </ul>"},{"location":"livehd/05-lgraph/","title":"LGraph","text":"<p>Warning</p> <p>LiveHD is beta under active development and we keep improving the API. Semantic versioning is a 0.+, significant API changes are expect.</p> <p>The LGraph is built directly through LNAST to LGraph translations. The LNAST builds a gated-SSA which is translated to LGraph. Understanding the LGraph is needed if you want to build a LiveHD pass.</p> <p>LGraph is a graph or netlist where each vertex is called a node, and it has a cell type and a set of input/output pins.</p>"},{"location":"livehd/05-lgraph/#api","title":"API","text":"<p>A single LGraph represents a single netlist module. LGraph is composed of nodes, node pins, edges, cell types, and tables of attributes. An LGraph node is affiliated with a cell node type and each type defines different amounts of input and output node pins. For example, a node can have 3 input ports and 2 output pins. Each of the input/output pins can have many edges to other graph nodes. Every node pin has an affiliated node pid. In the code, every node_pin has a <code>Port_ID</code>.</p> <p>A pair of driver pin and sink pin constitutes an edge.  The bitwidth of the driver pin determines the edge bitwidth.</p>"},{"location":"livehd/05-lgraph/#node-node_pin-and-edge-construction","title":"Node, Node_pin, and Edge Construction","text":"<ul> <li>create a node without associated type (edges can not be created until a type is associated)</li> </ul> <pre><code>new_node = lg-&gt;create_node()\n//note: type and/or bits still need to be assigned later\n</code></pre> <ul> <li>create node with node type assigned</li> </ul> <pre><code>new_node = lg-&gt;create_node(Node_type_Op)\n//note: recommended way if you know the target node type\n</code></pre> <ul> <li>create a constant node</li> </ul> <pre><code>new_node = lg-&gt;create_node_const(value)\n//note: recommended way to create a const node\n</code></pre> <ul> <li>setup default driver pin for pin_0 of a node</li> </ul> <pre><code>driver_pin = new_node.setup_driver_pin();\n//note: every cell in LGraph has only one driver pin, pin0\n</code></pre> <ul> <li>setup default sink pin for pin_0 of a node</li> </ul> <pre><code>sink_pin = new_node.setup_sink_pin()\n//note: when you know the node type only has one input pin\n</code></pre> <ul> <li> <p>setup sink pin for pin_x of a node, for more information, please refer to the   Cell type section. For quick reference of the sink pin names of each cell   type, please see   cell.cpp <pre><code>sink_pin = new_node.setup_sink_pin(\"some_name\")\n</code></pre></p> </li> <li> <p>add an edge between driver_pin and sink_pin</p> </li> </ul> <pre><code>driver_pin.connect(sink_pin);\n</code></pre> <ul> <li>get the driver node of an edge</li> </ul> <pre><code>driver_node = edge.driver.get_node()\n</code></pre> <ul> <li>use node as the index/key for a container</li> </ul> <pre><code>absl::flat_hash_map&lt;Node::Compact, int&gt; my_map;\nmy_map[node1.get_compact()] = 77;\nmy_map[node2.get_compact()] = 42;\n...\n</code></pre> <ul> <li>use node_pin as the index/key for a container</li> </ul> <pre><code>absl::flat_hash_map&lt;Node_pin::Compact, int&gt; my_map;\nmy_map[node_pin1.get_compact()] = 14;\nmy_map[node_pin2.get_compact()] = 58;\n...\n</code></pre> <ul> <li>get the node_pin back from a Node_pin::Compact</li> </ul> <pre><code>Node_pin dpin(lg, some_dpin.get_compact())\n</code></pre> <ul> <li>get the node back from a Node::Compact</li> </ul> <pre><code>Node node(lg, some_node.get_compact())\n</code></pre> <ul> <li>create a LGraph input(output) with the name</li> </ul> <pre><code>new_node_pin = lg-&gt;add_graph_input(std::string_view)\n</code></pre> <ul> <li>debug information of a node</li> </ul> <pre><code>node.debug_name()\n</code></pre> <ul> <li>debug information of a node_pin</li> </ul> <pre><code>node_pin.debug_name()\n</code></pre> <ul> <li>iterate output edges and get node/pin information from it</li> </ul> <pre><code>for (auto &amp;out : node.out_edges()) {\n  auto  dpin       = out.driver;\n  auto  dpin_pid   = dpin.get_pid();\n  auto  dnode_name = dpin.get_node().debug_name();\n  auto  snode_name = out.sink.get_node().debug_name();\n  auto  spin_pid   = out.sink.get_pid();\n  auto  dpin_name  = dpin.has_name() ? dpin.get_name() : \"\";\n  auto  dbits      = dpin.get_bits();\n\n  fmt::print(\" {}-&gt;{}[label=\\\"{}b :{} :{} :{}\\\"];\\n\"\n      , dnode_name, snode_name, dbits, dpin_pid, spin_pid, dpin_name);\n}\n</code></pre>"},{"location":"livehd/05-lgraph/#non-hierarchical-traversal-iterators","title":"Non-Hierarchical Traversal Iterators","text":"<p>LGraph allows forward and backward traversals in the nodes (bidirectional graph). The reason is that some algorithms need a forward and some a backward traversal, being bidirectional would help. Whenever possible, the fast iterator should be used.</p> <pre><code>for (const auto &amp;node:lg-&gt;fast())     {...} // unordered but very fast traversal\n\nfor (const auto &amp;node:lg-&gt;forward())  {...} // propagates forward from each input/constant\n\nfor (const auto &amp;node:lg-&gt;backward()) {...} // propagates backward from each output\n</code></pre> <p>The LGraph iterator such as <code>for(auto node: g-&gt;forward())</code> do not visit graph input and outputs.</p> <pre><code>// simple way using lambda\nlg-&gt;each_graph_input([&amp;](const Node_pin &amp;pin){\n\n\u00a0\u00a0//your operation with graph_input node_pin;\n\n});\n</code></pre>"},{"location":"livehd/05-lgraph/#hierarchical-traversal-iterators","title":"Hierarchical Traversal Iterators","text":"<p>LGraph supports hierarchical traversal. Each sub-module of a hierarchical design will be transformed into a new LGraph and represented as a sub-graph node in the parent module. If the hierarchical traversal is used, every time the iterator encounters a sub-graph node, it will load the sub-graph persistent tables to the memory and traverse the subgraph recursively, ignoring the sub-graph input/outputs. This cross-module traversal treats the hierarchical netlist just like a flattened design. In this way, all integrated third-party tools could automatically achieve global design optimization or analysis by leveraging the LGraph hierarchical traversal feature.</p> <pre><code>for (const auto &amp;node:lg-&gt;forward(true)) {...}\n</code></pre>"},{"location":"livehd/05-lgraph/#edge-iterators","title":"Edge Iterators","text":"<p>To iterate over the input edges of node, simply call:</p> <pre><code>for (const auto &amp;inp_edge : node.inp_edges()) {...}\n</code></pre> <p>And for output edges:</p> <pre><code>for (const auto &amp;out_edge : node.out_edges()) {...}\n</code></pre>"},{"location":"livehd/05-lgraph/#attribute-design","title":"Attribute Design","text":"<p>Design attribute stands for the characteristic given to a LGraph node or node pin. For instance, the characteristic of a node name and node physical placement. Despite a single LGraph stands for a particular module, it could be instantiated multiple times. In this case, same module could have different attribute at different hierarchy of the netlist. A good design of attribute structure should be able to represent both non-hierarchical and hierarchical characteristic.</p>"},{"location":"livehd/05-lgraph/#non-hierarchical-attribute","title":"Non-Hierarchical Attribute","text":"<p>Non-hierarchical LGraph attributes include pin name, node name and line of source code. Such properties should be the same across different LGraph instantia- tions. Two instantiations of the same LGraph module will have the exact same user-defined node name on every node. For example, instantiations of a subgraph-2 in both top and subgraph-1 would maintain the same non-hierarchical attribute table.</p> <pre><code>node.set_name(std::string_view name);\n</code></pre>"},{"location":"livehd/05-lgraph/#hierarchical-attribute","title":"Hierarchical attribute","text":"<p>LGraph also support hierarchical attribute. It is achieved by using a tree data structure to record the design hierarchy. In LGraph, every graph has a unique id (lg_id), every instantiation of a graph would form some nodes in the tree and every tree node is indexed by a unique hierarchical id (hid). We are able to identify a unique instantiation of a graph and generate its own hierarchical attribute table. An example of hierarchical attribute is wire-delay.</p> <pre><code>node_pin.set_delay(float delay);\n</code></pre>"},{"location":"livehd/05-lgraph/#cell-type","title":"Cell type","text":"<p>For each LGraph node, there is a specific cell type. This section explains the operation to perform for each node. It includes a precise way to compute the maximum and minimum value for the output.</p> <p>In LGraph, the cell types operate like having unlimited precision with signed numbers. Most HDL IRs have a type for signed inputs and another for unsigned. LiveHD handles the superset (sign and unlimited precision) with a single node. In LGraph, an unsigned value is signed value that is always positive. This simplifies the mixing and conversions which simplifies the passes. The drawback is that the export may have to convert back to signed/unsigned for some languages like Verilog.</p> <p>Maybe even more important is that all the LGraph cell types generate the same result if the input is sign-extended. This has implications, for example a typical HDL IR type like \"concat\" does not exist because the result is dependent on the inputs size. This has the advantage of simplifying the decisions of when to drop bits in a value. It also makes it easier to guarantee no loss of precision. Any drop of precision requires explicit handling with operations like and-gate with masks or Shifts. </p> <p>The document also explains corner cases in relationship to Verilog and how to convert to/from Verilog semantics. These are corner cases to deal with sign and precision. Each HDL may have different semantics, the Verilog is to showcase the specifics because it is a popular HDL.</p> <p>All the cell types are in <code>core/cell.hpp</code> and <code>core/cell.cpp</code>. The type enumerate is called <code>Ntype</code>. In general the nodes have a single output with the exception of complex nodes like subgraphs or memories. The inputs is a string in lower case or upper case. Upper case ('A') means that many edges (or output drivers) can connect to the same node input or sink pin, lower case ('a') means that only a driver can connect to the input or sink pin.</p> <p>Each cell type can be called directly with Pyrope using a low level RTL syntax. This is useful for debugging not for general use as it can result in less efficient LNAST code.</p> <p>An example of a multi-driver sink pin is the <code>Sum</code> cell which can do <code>Y=3+20+a0+a3</code> where <code>A_{0} = 3</code>, <code>A_{1} = 20</code>, <code>A_{2} = a0</code>, and <code>A_{3} = a3</code>. Another way to represent in valid Pyrope RTL syntax is:</p> <pre><code>Y = __sum(A=(3,20,a0,a3))\n</code></pre> <p>An example if single driver sink pin is the <code>SRA</code> cell which can do <code>Y=20&gt;&gt;3</code>. It is lower case because only one driver pin can connect to 'a' and 'b'. Another way to represent a valid Pyrope RTL syntax is:</p> <pre><code>Y = __sra(a=20,b=3)\n</code></pre> <p>The section includes description on how to compute the maximum (<code>max</code>) and minimum (<code>min</code>) allowed result range. This is used by the bitwidth inference pass. To ease the explanation, a <code>sign</code> value means that the result may be negative (<code>a.sign == a.min&lt;0</code>). <code>known</code> is true if the result sign is known (<code>a.known == a.max&lt;0 or a.min&gt;=0</code>), either positive or negative (<code>neg == a.max&lt;0</code>). The cells explanation also requires the to compute the bit mask (<code>a.mask == (1&lt;&lt;a.bits)-1</code>).</p> <p>For any value (<code>a</code>), the number of bits required (<code>bits</code>) is <code>a.bits = log2(absmax(a.max,a.min))+1</code>.</p>"},{"location":"livehd/05-lgraph/#sum","title":"Sum","text":"<p>Addition and substraction node is a single cell Ntype that performs 2-complement additions and substractions with unlimited precision.</p> graph LR     cell  --Y--&gt; c(fa:fa-spinner)     a(fa:fa-spinner) --A--&gt; cell[Sum]:::cell     b(fa:fa-spinner) --B--&gt; cell     classDef cell stroke-width:3px  <p>If the inputs do not have the same size, they are sign extended to all have the same length.</p> <p>Forward Propagation</p> <ul> <li>Value: <pre><code>%Y = A.reduce('+') - B.reduce('+')\n</code></pre></li> <li>Max/min: <pre><code>%max = 0\n%min = 0\nfor a in A {\n  %max += A.max\n  %min += A.min\n}\nfor b in B {\n  %max -= b.min\n  %min -= b.max\n}\n</code></pre></li> </ul> <p>Backward Propagation</p> <p>Backward propagation is possible when all the inputs but ONE are known. The algorithm can check and look for the inputs that have more precision than needed and reduce the max/min backwards.</p> <p>For example, if and all the inputs but one A are known (max/min has the max/min computed for all the inputs but the unknown one)</p> <pre><code>A_{unknown}.max = Y.max - max \nA_{unknown}.min = Y.min - min \n</code></pre> <p>If the unknow is in port <code>B</code>:</p> <pre><code>B_{unknown}.max = min - T.min\nB_{unknown}.min = max - Y.max\n</code></pre> <p>Verilog Considerations</p> <p>In Verilog, the addition is unsigned if any of the inputs is unsigned. If any input is unsigned. all the inputs will be \"unsigned extended\" to match the largest value. This is different from Sum_Op semantics were each input is signed or unsigned extended independent of the other inputs. To match the semantics, when mixing signed and unsigned, all the potentially negative inputs must be converted to unsign with the Ntype_op::Tposs.</p> <pre><code>logic signed [3:0] a = -1\nlogic signed [4:0] c;\n\nassign c = a + 1'b1;\n</code></pre> <p>The previous Verilog example extends everything to 5 bits (c) UNSIGNED extended because one of the inputs is unsigned (1b1 is unsigned in verilog, and 2sb1 is signed +1). LGraph semantics are different, everything is signed.</p> <pre><code>c = 5b01111 + 5b0001 // this is the Verilog semantics by matching size\nc == -16 (!!)\n</code></pre> <p>The Verilog addition/substraction output can have more bits than the inputs. This is the same as in LGraph <code>Sum</code>. Nevertheless, Verilog requires to specify the bits for all the input/outputs. This means that whenever Verilog drops precision an AND gate must be added (or a SEXT for signed output). In the following examples only the 'g' and 'h' variables needed.</p> <pre><code>  wire [7:0] a;\n  wire [7:0] b;\n  wire [6:0] c;\n  wire [8:0] f = a + b; // f = __sum(a,b)  // a same size as b\n  wire [8:0] f = a + c; // f = __sum(a,__get_mask(c,-1))\n  wire [7:0] g = a + b; // g = __and(__sum(a,b),0x7F)\n  wire [6:0] h = a + b; // h = __and(__sum(a,b),0x3F)\n</code></pre> <p>Peephole Optimizations</p> <ul> <li><code>Y = x-0+0+...</code> becomes <code>Y = x+...</code></li> <li><code>Y = x-x+...</code> becomes <code>Y = ...</code></li> <li><code>Y = x+x+...</code> becomes <code>Y = (x&lt;&lt;1)+...</code></li> <li><code>Y = (x&lt;&lt;n)+(y&lt;&lt;m)</code> where m&gt;n becomes <code>Y = (x+y&lt;&lt;(m-n)&lt;&lt;n</code></li> <li><code>Y = (~x)+1+...</code> becomes <code>Y = ...-x</code></li> <li><code>Y = a + (b&lt;&lt;n)</code> becomes <code>Y = {(a&gt;&gt;n)+b, a&amp;n.mask}</code></li> <li><code>Y = a - (b&lt;&lt;n)</code> becomes <code>Y = {(a&gt;&gt;n)-b, a&amp;n.mask}</code></li> <li>If every x,y... lower bit is zero <code>Y=x+y+...</code> becomes <code>Y=((x&gt;&gt;1)+(y&gt;&gt;1)+..)&lt;&lt;1</code></li> </ul>"},{"location":"livehd/05-lgraph/#mult","title":"Mult","text":"<p>Multiply operator. There is no cell type that combines multiplication and division because unlike in <code>Sum</code>. The reason is that with integers the order of multiplication/division changes the result even with unlimited precision integers (<code>a*(b/c) != (a*b)/c</code>).</p> graph LR     cell  --Y--&gt; c(fa:fa-spinner)     a(fa:fa-spinner) --A--&gt; cell[Mult]:::cell     classDef cell stroke-width:3px  <p>Forward Propagation</p> <ul> <li>Value: <pre><code>Y = A.reduce('*')\n</code></pre></li> <li>Max/min: <pre><code>var tmax = 1\nvat tmin = 1\nvar sign  = 0\nfor i in A {\n  tmax *= maxabs(A.max, A.min)\n  tmin *= minabs(A.max, A.min)\n  known = false                when min&lt;0 and max&gt;0\n  sign += 1                    when max&lt;0\n}\nif know { // sign is know\n  if sign &amp; 1 { // negative\n    %max = -tmin\n    %min = -tmax\n  }else{\n    %max = tmax\n    %min = tmin\n  }\n}else{\n  %max =  tmax\n  %min = -tmax\n}\n</code></pre></li> </ul> <p>Backward Propagation</p> <p>If only one input is missing, it is possible to infer the max/min from the output and the other inputs. Like in the <code>sum</code> case, if all the inputs but one and the output is known, it is possible to backward propagate to further constraint the unknown input.</p> <pre><code>A_{unknown}.max = Y.max / A.min\nA_{unknown}.min = Y.min / A.max\n</code></pre> <p>Verilog Considerations</p> <p>Unlike the <code>Sum</code>, the Verilog 2 LiveHD translation does not need to extend the inputs to have matching sizes. Multiplying/dividing signed and unsigned numbers has the same result. The bit representation is the same if the result was signed or unsigned.</p> <p>LiveHD mult node result (Y) number of bits can be more efficient than in Verilog. E.g: if the max value of A0 is 3 (2 bits) and A1 is 5 (3bits). If the result is unsigned, the maximum result is 15 (4 bits). In Verilog, the result will always be 5 bits. If the Verilog result was to an unsigned variable. Either all the inputs were unsigned, or there should pass to an <code>get_mask</code> to force the MSB as positive. This extra bit will be simplified but it will notify LGraph that the output is to be treated as unsigned.</p> <p>Peephole Optimizations</p> <ul> <li><code>Y = a*1*...</code> becomes <code>Y=a*...</code></li> <li><code>Y = a*0*...</code> becomes <code>Y=0</code></li> <li><code>Y = power2a*...</code> becomes <code>Y=(...)&lt;&lt;log2(power2a)</code></li> <li><code>Y = (power2a+power2b)*...</code> becomes <code>tmp=... ; Y = (tmp+tmp&lt;&lt;power2b)&lt;&lt;(power2a-power2b)</code> when power2a&gt;power2b</li> <li><code>Y = (power2a-power2b)*...</code> becomes <code>tmp=... ; Y = (tmp-tmp&lt;&lt;power2b)&lt;&lt;(power2a-power2b)</code> when power2a&gt;power2b</li> </ul>"},{"location":"livehd/05-lgraph/#div","title":"Div","text":"<p>Division operator. The division operation is quite similar to the inverse of the multiplication, but a key difference is that only one driver is allowed for each input ('a' vs 'A').</p> graph LR     cell  --Y--&gt; c(fa:fa-spinner)     a(fa:fa-spinner) --a--&gt; cell[Div]:::cell     b(fa:fa-spinner) --b--&gt; cell     classDef cell stroke-width:3px  <p>Forward Propagation</p> <ul> <li>Value: <pre><code>Y = a/b\n</code></pre></li> <li>Max/min: <pre><code>%max = a.max/b.min\n%min = a.min/b.max\n\nfor i in a.max,a.min {\n  for j in b.max,b.min {\n     next        when j == 0\n     tmp = i / j\n     %max = tmp   when tmp &gt; max\n     %min = tmp   when tmp &lt; min\n  }\n}\n</code></pre></li> </ul> <p>Backward Propagation</p> <p>The backward propagation from the division can extracted from the forward propagation. It is a simpler case of multiplication backward propagation.</p> <p>Verilog Considerations</p> <p>The same considerations as in the multiplication should be applied.</p> <p>Peephole Optimizations</p> <ul> <li><code>Y = a/1</code> becomes <code>Y=a</code></li> <li><code>Y = 0/b</code> becomes <code>Y=0</code></li> <li><code>Y = a/power2b</code> becomes <code>Y=a&gt;&gt;log2(power2b)</code> if <code>Y.known and !Y.neg</code></li> <li><code>Y = a/power2b</code> becomes <code>Y=1+~(a&gt;&gt;log2(power2b))</code> if <code>Y.known and Y.neg</code></li> <li><code>Y = (x*c)/a</code> if c.bits&gt;a.bits becomes <code>Y = x * (c/a)</code> which should be a smaller division.</li> <li>If b is a constant and <code>Y.known and !Y.neg</code>. From the hackers delight, we</li> <li>know that the division can be changed for a multiplication</li> <li><code>Y=(a*(((1&lt;&lt;(a.bits+2)))/b+1))&gt;&gt;(a.bits+2)</code> If a sign is not <code>known</code>. Then `Y</li> <li>= Y.neg? (~Y_unsigned+1):Y_unsigned`</li> </ul>"},{"location":"livehd/05-lgraph/#modulo-how-to-model","title":"Modulo (how to model)","text":"<p>There is no mod cell (Ntype_op::Mod) in LGraph. The reason is that a modulo different from a power of 2 is very rare in hardware. If the language supports modulo operations, they must be translated to division/multiplication.</p> <pre><code>y = a mod b\n</code></pre> <p>It is the same as:</p> <pre><code>y = a-b*(a/b)\n</code></pre> <p>If b is a power of 2, the division optimization will transform the modulo operation to:</p> <pre><code>y = a - (a&gt;&gt;n)&lt;&lt;n\n</code></pre> <p>The add optimization should reduce it to:</p> <pre><code>y = a &amp; n.mask\n</code></pre>"},{"location":"livehd/05-lgraph/#not","title":"Not","text":"<p>Bitwise Not operator</p> graph LR     cell  --Y--&gt; c(fa:fa-spinner)     a(fa:fa-spinner) --a--&gt; cell[Div]:::cell     classDef cell stroke-width:3px  <p>Forward Propagation</p> <ul> <li>Value: <pre><code>Y = ~a\n</code></pre></li> <li>Max/min: <pre><code>%max = max(~a.max,~a.min)\n%min = min(~a.max,~a.min)\n</code></pre></li> </ul> <p>Backward Propagation</p> <pre><code>a.max = max(~Y.max,~Y.min)\na.min = min(~Y.max,~Y.min)\n</code></pre> <p>Verilog Considerations*</p> <p>Same semantics as verilog</p> <p>Peephole Optimizations</p> <p>No optimizations by itself, it has a single input. Other operations like Sum_Op can optimize when combined with Not_Op.</p>"},{"location":"livehd/05-lgraph/#and","title":"And","text":"<p><code>And</code> is a typical AND gate with multiple inputs. All the inputs connect to pin 'A' because input order does not matter. The result is always a signed number.</p> <pre><code>digraph And {\n    rankdir=LR;\n    size=\"1,0.5\"\n\n    node [shape = circle]; And;\n    node [shape = point ]; q0\n    node [shape = point ]; q\n\n    q0 -&gt; And [ label =\"A\" ];\n    And  -&gt; q [ label = \"Y\" ];\n}\n</code></pre>"},{"location":"livehd/05-lgraph/#forward-propagation","title":"Forward Propagation","text":"<ul> <li>\\(Y = \\forall_{i=0}^{\\infty} Y \\&amp; A_{i}\\)</li> <li>\\(m = \\forall_{i=0}^{\\infty} min(m,A_{i}.bits)\\)</li> <li>\\(Y.max = (1\\ll m)-1\\)</li> <li>\\(Y.min = -Y.max-1\\)</li> </ul>"},{"location":"livehd/05-lgraph/#backward-propagation","title":"Backward Propagation","text":"<p>The And cell has a significant backpropagation impact. Even if some inputs had more bits, after the And cell the upper bits are dropped. This allows the back propagation to indicate that those bits are useless.</p> <ul> <li>$a.max = Y.max $</li> <li>$a.min = -Y.max-1 $</li> </ul>"},{"location":"livehd/05-lgraph/#other-considerations","title":"Other Considerations","text":""},{"location":"livehd/05-lgraph/#peephole-optimizations","title":"Peephole Optimizations","text":""},{"location":"livehd/05-lgraph/#comparators","title":"Comparators","text":"<p>LT, GT, EQ</p> <p>There are only 3 comparators. Other typically found like LE, GE, and NE can be created by simply negating one of the LGraph comparators. <code>GT = ~LE</code>, <code>LT = ~GE</code>, and <code>NE = ~EQ</code>.</p>"},{"location":"livehd/05-lgraph/#forward-propagation_1","title":"Forward Propagation","text":"<ul> <li> <p><code>Y = A LT B</code></p> </li> <li> <p><code>Y = A0 LT B and A1 LT B</code></p> </li> <li> <p><code>Y = A0 LT B0 and A1 LT B0 and A0 LT B1 and A1 LT B1</code></p> </li> </ul>"},{"location":"livehd/05-lgraph/#backward-propagation_1","title":"Backward Propagation","text":""},{"location":"livehd/05-lgraph/#peephole-optimizations_1","title":"Peephole Optimizations","text":""},{"location":"livehd/05-lgraph/#other-considerations_1","title":"Other Considerations","text":"<p>Verilog treats all the inputs as unsigned if any of them is unsigned. LGraph treats all the inputs as signed all the time.</p> size A B Operation a==b S S EQ(a,b) a==b S U EQ(a,b) a==b U S EQ(a,b) a==b U U EQ(a,b) a&lt; b S S LT(a,b) a&lt; b S U LT(a,Tposs(b)) a&lt; b U S LT(Tposs(a),b) a&lt; b U U LT(Tposs(a),Tposs(b))"},{"location":"livehd/05-lgraph/#shl_op","title":"SHL_op","text":"<p>Shift Left performs the typical shift left when there is a single amount (<code>a&lt;&lt;amt</code>). The allow supports multiple left shift amounts. In this case the shift left is used to build one hot encoding mask. (<code>1&lt;&lt;(1,2) == (1&lt;&lt;1)|(1&lt;&lt;2)</code>)</p> <p>The result for when there are not amounts (<code>a&lt;&lt;()</code>) is <code>-1</code>. Notice that this is not ZERO but -1. The -1 means that all the bits are set. The reason is that when there are no offsets in the onehot encoding, the default functionality is to select all the bit masks, and hence -1.</p>"},{"location":"livehd/05-lgraph/#sra_op","title":"SRA_op","text":"<p>Logical or sign extension shift right.</p>"},{"location":"livehd/05-lgraph/#verilog-considerations","title":"Verilog Considerations","text":"<p>Verilog has 2 types of shift <code>&gt;&gt;</code> and <code>&gt;&gt;&gt;</code>. The first is unsigned right shift, the 2nd is arithmetic right shift. LGraph only has arithmetic right shift (ShiftRigt_op). The verilog translation should make the value unsigned (<code>ShiftRigt(Join(0,a),b)</code>) before calling the shift operation. Conversely, for a <code>&gt;&gt;&gt;</code> if the input is Verilog unsigned (<code>ShiftRigt(a,b)</code>)</p>"},{"location":"livehd/05-lgraph/#mux_op","title":"Mux_op","text":""},{"location":"livehd/05-lgraph/#forward-propagation_2","title":"Forward Propagation","text":"<ul> <li>\\(Y = P_{(1+P_{0}}\\)</li> <li>\\(Y.max = (1\\ll m)-1\\)</li> <li>\\(Y.max = \\forall_{i=0}^{\\infty} P_{i}.max\\)</li> <li>\\(Y.max = \\forall_{i=0}^{\\infty} P_{i}.min\\)</li> </ul>"},{"location":"livehd/05-lgraph/#backward-propagation_2","title":"Backward Propagation","text":""},{"location":"livehd/05-lgraph/#peephole-optimizations_2","title":"Peephole Optimizations","text":""},{"location":"livehd/05-lgraph/#other-considerations_2","title":"Other Considerations","text":""},{"location":"livehd/05-lgraph/#lut_op","title":"LUT_op","text":""},{"location":"livehd/05-lgraph/#and_op","title":"And_op","text":"<p>reduce AND <code>a =u= -1</code> // unsigned equal</p>"},{"location":"livehd/05-lgraph/#or_op","title":"Or_op","text":"<p>reduce OR <code>a != 0</code></p>"},{"location":"livehd/05-lgraph/#xor_op","title":"Xor_op","text":"<p>reduce xor is a chain of XORs.</p>"},{"location":"livehd/05-lgraph/#const_op","title":"Const_op","text":""},{"location":"livehd/05-lgraph/#sflop_op","title":"SFlop_op","text":""},{"location":"livehd/05-lgraph/#aflop_op","title":"AFlop_op","text":""},{"location":"livehd/05-lgraph/#fflop_op","title":"FFlop_op","text":""},{"location":"livehd/05-lgraph/#latch_op","title":"Latch_op","text":""},{"location":"livehd/05-lgraph/#get_mask_op","title":"Get_mask_op","text":"<p>Inputs - a, mask Get_mask (a, mask) Functionality - Output contains only those bits a[i], for which mask[i] = 1, other bits a[i] for which mask[i] = 0, are dropped. a &amp; mask are interpreted as signed numbers and sign extended to the size of the other, if required. eg - Get_mask (0sb11000011, 0sb10101010) = 0sb1001       Get_mask (0sb11110000, 0sb00001111) = 0sb0000      Get_mask (0sb0011, 0sb10) = 0sb001      Get_mask (0sb10, 0sb1010) = 0sb11</p>"},{"location":"livehd/05-lgraph/#set_mask_op","title":"Set_mask_op","text":"<p>Inputs - a, value, mask Set_mask(a, mask, value) Functionality - Replaces all bits a[i] for which mask[i] = 1, with value[i]  Retains all bits a[i] for which mask[i] = 0. // Check - if a, value are signed, actually none of them should be extended and their signs should not matter, but a might need to retain it's sign eg - Set_mask (0b101 01 010, 0sb000 11 000, 0b001 10 011) = 0sb 101 10 010</p>"},{"location":"livehd/05-lgraph/#sext_op-sign-extend","title":"Sext_op  (Sign extend)","text":"<p>Inputs - a, b Sext (a, b) Selects only bits a[b:0] dropping all remaining MSBs. The selected a[b:0] is interpretded as a signed value, a's sign does not matter,b conyains the MSB index and hence is always unsigned/ positive eg Sext (0b10101010, 4) = 0sb01010 = 0xA = +10 Sext (0b10101010, 5) = 0sb101010 = 0x2A = -22</p>"},{"location":"livehd/05-lgraph/#memory_op","title":"Memory_op","text":"<p>Memory is the basic block to represent SRAM-like structures. Any large storage will benefit from using memory arrays instead of flops, which are slower to simulate. These memories are highly configurable.</p> <pre><code>digraph Memory {\n    rankdir=LR;\n    size=\"2,1\"\n\n    node [shape = circle]; Memory;\n    node [shape = point ]; q0\n    node [shape = point ]; q1\n    node [shape = point ]; q2\n    node [shape = point ]; q3\n    node [shape = point ]; q4\n    node [shape = point ]; q5\n    node [shape = point ]; q6\n    node [shape = point ]; q7\n    node [shape = point ]; q8\n    node [shape = point ]; q9\n    node [shape = point ]; q10\n    node [shape = point ]; q\n\n    q0 -&gt; Memory [ label =\"a (addr)\" ];\n    q1 -&gt; Memory [ label =\"b (bits)\" ];\n    q2 -&gt; Memory [ label =\"c (clock)\" ];\n    q3 -&gt; Memory [ label =\"d (data in)\" ];\n    q4 -&gt; Memory [ label =\"e (enable)\" ];\n    q5 -&gt; Memory [ label =\"f (fwd)\" ];\n    q6 -&gt; Memory [ label =\"l (latency)\" ];\n    q7 -&gt; Memory [ label =\"m (wmask)\" ];\n    q8 -&gt; Memory [ label =\"p (posedge)\" ];\n    q9 -&gt; Memory [ label =\"s (size)\" ];\n    q10 -&gt; Memory [ label =\"w (wmode)\" ];\n    Memory  -&gt; q [ label =\"Q (data out)\" ];\n}\n</code></pre> <ul> <li><code>s</code> (<code>size</code>) is for the array size in number of entries</li> <li><code>b</code> (<code>bits</code>) is the number of bits per entry</li> <li><code>f</code> (<code>fwd</code>) points to a 0/1 constant driver pin to indicate if writes forward value (<code>0b0</code> for write-only ports). Effectively, it means zero cycles read latency when enabled. <code>fwd</code> is more than just setting <code>latency=0</code>. Even with latency zero, the write delay affects until the result is visible. With <code>fwd</code> enabled, the write latency does not matter to observe the results. This requires a costly forwarding logic.</li> <li><code>c</code>,<code>d</code>,<code>e</code>,<code>q</code>... are the memory configuration, data, address ports</li> </ul> <p>Ports (<code>a</code>,<code>c</code>...<code>p</code>,<code>w</code>) are arrays/vectors to support multiported memories. If a single instance exists in a port, the same is used across all the ports. E.g: if clock (<code>c</code>) is populated:</p> <pre><code>mem1.c = clk1 // clk for all the memory ports\n\nmem2.c[0] = clk1 // clock for memory port 0\nmem2.c[1] = clk2 // clock for memory port 1\nmem2.c[2] = clk2 // clock for memory port 2\n</code></pre> <p>Each memory port (rd, wr, or rd/wr) has the following ports:</p> <ul> <li><code>a</code> (<code>addr</code>) points to the driver pin for the address. The address bits should match the array size (<code>ceil(log2(s))</code>)</li> <li><code>c</code> (<code>clock</code>) points to the clock driver pin</li> <li><code>d</code> (<code>data_in</code>) points to the write data driver pin (read result is in <code>q</code> port).</li> <li><code>e</code> (<code>enable</code>) points to the driver pin for read/write enable.</li> <li><code>l</code> (<code>latency</code>) points to an integer constant driver pin (2 bits always). For writes <code>latency from 1 to 3</code>, for reads <code>latency from 0 to 3</code></li> <li><code>w</code> (<code>wmask</code>) Points to the write mask (1 == write, 0==no write). The mask bust be a big as the number of bits per entry (<code>b</code>). The <code>wmask</code> pin can be disconnected which means no write mask (a write will write all the bits).</li> <li><code>p</code> (<code>posedge</code>) points to a 1/0 constant driver pin</li> <li><code>m</code> (<code>mode</code>) points to the driver pin or switching between read (0) and write mode (1) (single bit)</li> <li><code>Q</code> (<code>data_out</code>) is a driver pin with the data read from the memory</li> </ul> <p>All the entries but the <code>wmask</code> must be populated. If the <code>wmask</code> is not set, a full write size is expected. Read-only ports do not have <code>data</code> and <code>wmask</code> fields if the write use the low ports (0,1...). By placing the read-only ports to the high numbers, we can avoid populating the wmask (<code>m</code>) and data out (<code>q</code>) ports. If the read ports use low port numbers those fields must be populated to allow the correct matching between write port (<code>a[n]</code>) and write result (<code>q[n]</code>).</p> <p>All the ports must be populated with the correct size. This is important because some modules access the field by bit position. If it is not used, it will point to a zero constant with the correct number of bits. The exception to this is <code>wmask</code> which, if <code>b</code> indicates 8 bits per entry, will be equivalent to <code>0xFF</code>. Setting wmask to <code>0b1</code> will mean a 1 bit zero, and the memory will be incorrectly operated.</p> <p>The memory usually has power of two sizes. If the size is not a power of 2, the address is rounded up. Writes to the invalid addresses will generated random memory updates. Reads should read random data.</p>"},{"location":"livehd/05-lgraph/#forward-propagation_3","title":"Forward Propagation","text":""},{"location":"livehd/05-lgraph/#backward-propagation_3","title":"Backward Propagation","text":""},{"location":"livehd/05-lgraph/#other-considerations_3","title":"Other Considerations","text":""},{"location":"livehd/05-lgraph/#peephole-optimizations_3","title":"Peephole Optimizations","text":""},{"location":"livehd/05-lgraph/#subgraph_op","title":"SubGraph_op","text":"<p>And_Op: bitwise AND with 2 outputs single bit reduction (RED) or bitwise Y = VAL&amp;..&amp;VAL ; RED= &amp;Y</p>"},{"location":"livehd/05-lgraph/#forward-propagation_4","title":"Forward Propagation","text":"<ul> <li>\\(Y = \\left\\{\\begin{matrix} VAL&gt;&gt;OFF &amp; SZ==0 \\\\ (VAL&gt;&gt;OFF) \\&amp; (1&lt;&lt;SZ)-1) &amp; otherwise \\end{matrix}\\right.\\)</li> <li>\\(Y.max = \\left\\{\\begin{matrix} VAL.max&gt;&gt;OFF &amp; SZ==0 \\\\ (VAL.max&gt;&gt;OFF) \\&amp; (1&lt;&lt;SZ)-1) &amp; otherwise \\end{matrix}\\right.\\)</li> <li>\\(Y.min = 0\\)</li> <li>\\(Y.sign = 0\\)</li> </ul>"},{"location":"livehd/05-lgraph/#backward-propagation_4","title":"Backward Propagation","text":"<p>The sign can not be backward propagated because Pick_Op removes the sign no matter the input sign.</p>"},{"location":"livehd/05-lgraph/#to-be-continued","title":"To be continued ...","text":""},{"location":"livehd/05-lgraph/#optimization","title":"Optimization","text":"<p>Not all the nodes have the same complexity overhead. When performing peephole optimization is possible to trade one set of nodes for others. In general, we have this set of overheads:</p> <ul> <li> <p>0 overhead: not, get_mask, set_mask, sext, and SHL/SRA with constant shift   amounts. The rational is that those are just \"wiring\" cells to connect or   extract wires across. The NOT gate is not really zero, but it could be easily   mixed with sorrounding cells.</p> </li> <li> <p>1 overhead: And, Or, Xor, LUT, Mux</p> </li> <li> <p>3 overhead: LT, GT, EQ, Ror</p> </li> <li> <p>4 overhead: Less than 4 bit output Sum, and SHL/SRA with non-compile time   shift amount. This can be costly an require hardware like barrel shifters.</p> </li> <li> <p>5 overhead: large Sum, SHL/SRA.</p> </li> <li> <p>6 Overhead: Mult/Div</p> </li> </ul> <p>If a overhead level can be elininated having a small number of different cells with a smaller overhead level,the translation makes sense. Notice the \"small number of cells\", after all everything can be translated to nand gates. A 3x factor is somewhat reasonable. This means that a 5-level overhead is fine to be replaced for 3 4-level (or 3 3-level) but not for 4 4-level overhead. Zero overhead cells are not included in the list of cells in the replacement.</p> <p>This is a heuristic. Once works, it is a nice target to use AI to decide when/if a transformation is worth.</p>"},{"location":"livehd/06-lnast/","title":"LNAST","text":"<p>LNAST stands for Language-Neutral Abstract Syntax Tree, which is constituted of Lnast_nodes and indexed by a tree structure.  </p> <p>LiveHD has two main data structures: LNAST and LGraph. The LNAST is the higher level representation with a tree structure. The LGraph is the lower level representation with a graph structure.  Each node in LGraph has a LNAST equivalent node, but LNAST is more high level and several nodes in LNAST may not have a one-to-one mapping to LGraph.</p> <p>Each Lnast_node should has a specific node type and contain the following information from source code tokens  </p> <p>(a) line number  (b) pos_start, pos_end (c) string_view (optional)  </p>"},{"location":"livehd/06-lnast/#function-overloadings-of-node-data-construction","title":"Function Overloadings of Node Data Construction","text":"<p>Every node construction method has four function overloadings. For example, to construct a Lnast_node with a type of reference, we could use one of the following functions:  </p> <pre><code>// C++\nauto node_ref = Lnast_node::create_ref(\"foo\");     \nauto node_ref = Lnast_node::create_ref(\"foo\", line_num);     \nauto node_ref = Lnast_node::create_ref(\"foo\", line_num, pos1, pos2);     \nauto node_ref = Lnast_node::create_ref(token);   \n</code></pre> <p>In case (1), you only knows the variable name is \"foo\". In case (2), you know the variable name and the corresponding line number. In case (3), you know the variable name, the line number, and the charactrer position. In case (4), you are building LNAST from your HDL AST and you already have the Token.  The toke should have line number, positions, and string_view information.  </p>"},{"location":"livehd/06-lnast/#another-example","title":"Another Example","text":"<p>If you don't care the string_view to be stored in the lnast node, just leave it empty for set \"foo\" for it. This is true for many of the operator node, for example, to build a node with type of assign.  </p> <pre><code>// C++\nauto node_assign = Lnast_node::create_assign();   \nauto node_assign = Lnast_node::create_assign(line_num);     \nauto node_assign = Lnast_node::create_assign(line_num, pos1, pos2);   \nauto node_assign = Lnast_node::create_assign(token); // The token is not necessary to have a string_view  \n</code></pre>"},{"location":"livehd/06-lnast/#lnast-node-types","title":"LNAST Node Types","text":"<code>top</code> <code>stmts</code> <code>if</code> <code>uif</code> <code>for</code> <code>func_call</code> <code>func_def</code> <code>assign</code> <code>dp_assign</code> <code>mut</code> <code>bit_and</code> <code>bit_or</code> <code>bit_not</code> <code>bit_xor</code> <code>reduce_or</code> <code>logical_and</code> <code>logical_or</code> <code>logical_not</code> <code>plus</code> <code>minus</code> <code>mult</code> <code>div</code> <code>mod</code> <code>shl</code> <code>sra</code> <code>sext</code> <code>set_mask</code> <code>get_mask</code> <code>mask_and</code> <code>mask_popcount</code> <code>mask_xor</code> <code>is</code> <code>ne</code> <code>eq</code> <code>lt</code> <code>le</code> <code>gt</code> <code>ge</code> <code>ref</code> <code>const</code> <code>range</code> <code>tuple_concat</code> <code>tuple_add</code> <code>tuple_get</code> <code>tuple_set</code> <code>attr_set</code> <code>attr_get</code> <code>err_flag</code> <code>phi</code> <code>hot_phi</code> <code>invalid</code>"},{"location":"livehd/06-lnast/#scope","title":"Scope","text":""},{"location":"livehd/06-lnast/#top","title":"<code>top</code>","text":"<p>Every LNAST has a <code>top</code> node as the root. A <code>top</code> node has one or more child nodes, which can only be <code>stmts</code>.</p> <pre><code>&lt;top&gt; --| &lt;stmts&gt;\n        | &lt;stmts&gt;\n        | &lt;stmts&gt;\n        |  ...\n</code></pre>"},{"location":"livehd/06-lnast/#stmts","title":"<code>stmts</code>","text":"<p>A <code>stmts</code> node represents a sequence of statements.</p> <pre><code>&lt;stmts&gt; --| &lt;const&gt;     : scope name\n          | &lt;assign&gt;\n          | &lt;plus&gt;\n          | &lt;func_def&gt;\n          | ...\n</code></pre>"},{"location":"livehd/06-lnast/#statements","title":"Statements","text":""},{"location":"livehd/06-lnast/#if","title":"<code>if</code>","text":"<p>An <code>if</code> node represents a conditional branch, which can be a statement or an expression.</p> <pre><code>&lt;if&gt; --| &lt;ref/const&gt; : if condition variable\n       | &lt;stmts&gt;     : if branch\n       | &lt;ref/const&gt; : elif condition variable  \\  N times\n       | &lt;stmts&gt;     : elif branch              /\n       | &lt;stmts&gt;     : else branch\n</code></pre>"},{"location":"livehd/06-lnast/#uif","title":"<code>uif</code>","text":"<p>Unique <code>if</code>. Similar to <code>if</code>, but add additional assertions to check if at most one condition is true.</p> <pre><code>&lt;uif&gt; --| &lt;ref/const&gt; : if condition variable\n        | &lt;stmts&gt;     : if branch\n        | &lt;ref/const&gt; : elif condition variable  \\  N times\n        | &lt;stmts&gt;     : elif branch              /\n        | &lt;stmts&gt;     : else branch\n</code></pre>"},{"location":"livehd/06-lnast/#for","title":"<code>for</code>","text":"<p>A <code>for</code> node represents a for-loop over a <code>range</code> or <code>tuple</code>. Note that the loop must be unrolled during compilation.</p> <pre><code>&lt;for&gt; --| &lt;ref&gt;   : iterator variable\n        | &lt;ref&gt;   : iterated variable (tuple or range)\n        | &lt;stmts&gt; : for-loop body\n</code></pre>"},{"location":"livehd/06-lnast/#func_def","title":"<code>func_def</code>","text":"<p>A <code>func_def</code> node represents a functional block with input/output arguments.</p> <pre><code>&lt;func_def&gt; --| &lt;ref/const&gt; : input arguments\n             | &lt;ref/const&gt; : output arguments\n             | &lt;stmts&gt;     : function body\n</code></pre>"},{"location":"livehd/06-lnast/#func_call","title":"<code>func_call</code>","text":"<p>A <code>func_call</code> node represents an instantiation of a functional block. </p> <pre><code>&lt;func_call&gt; --| &lt;ref/const&gt; : Lvalue\n              | &lt;ref&gt;       : function reference\n              | &lt;ref/const&gt; : input arguments\n</code></pre>"},{"location":"livehd/06-lnast/#assign","title":"<code>assign</code>","text":"<p>An <code>assign</code> node represents a variable assignment. Note that the Rvalue can only be a <code>const</code> or <code>ref</code>.</p> <pre><code>&lt;assign&gt; --| &lt;ref&gt;       : Lvalue\n           | &lt;ref/const&gt; : Rvalue\n</code></pre>"},{"location":"livehd/06-lnast/#dp_assign","title":"<code>dp_assign</code>","text":"<p>the \"lhs := rhs\" assignment (dp_assign) is like the \"=\" assignment but there is no check for overflow. If the rhs has more bits than the lhs, the upper bits will be dropped.</p> <pre><code>&lt;dp_assign&gt; --| &lt;ref&gt;       : Lvalue\n              | &lt;ref/const&gt; : Rvalue\n</code></pre>"},{"location":"livehd/06-lnast/#primitives","title":"Primitives","text":""},{"location":"livehd/06-lnast/#const","title":"<code>const</code>","text":"<p>Constant value.</p> <pre><code>&lt;const&gt; \"0x1234\"\n</code></pre>"},{"location":"livehd/06-lnast/#ref","title":"<code>ref</code>","text":"<p>Variable.</p> <pre><code>&lt;ref&gt; \"variable_name\"\n</code></pre>"},{"location":"livehd/06-lnast/#range","title":"<code>range</code>","text":"<p>Range.</p> <pre><code>&lt;range&gt; --| &lt;ref&gt; or &lt;const&gt; : from-value\n          | &lt;ref&gt; or &lt;const&gt; : to-value\n</code></pre>"},{"location":"livehd/06-lnast/#unary-expressions","title":"Unary Expressions","text":"<pre><code>&lt;op&gt; --| &lt;ref&gt;       : Lvalue\n       | &lt;ref/const&gt; : Rvalue\n</code></pre>"},{"location":"livehd/06-lnast/#bit_not","title":"<code>bit_not</code>","text":"<p>Bitwise not. Flip all Rvalue bits.</p>"},{"location":"livehd/06-lnast/#reduce_or","title":"<code>reduce_or</code>","text":"<p>Or all Lvalue bits.</p>"},{"location":"livehd/06-lnast/#logical_not","title":"<code>logical_not</code>","text":"<p>Logical Not. Flip Rvalue where Rvalue must be a boolean.</p>"},{"location":"livehd/06-lnast/#binary-expressions","title":"Binary Expressions","text":"<pre><code>&lt;op&gt; --| &lt;ref&gt;       : Lvalue\n       | &lt;ref/const&gt; : R-1\n       | &lt;ref/const&gt; : R-2\n</code></pre>"},{"location":"livehd/06-lnast/#mod","title":"<code>mod</code>","text":"<p>Modulo of R-1 over R-2.</p>"},{"location":"livehd/06-lnast/#shl","title":"<code>shl</code>","text":"<p>Left-shift R-1 by R-2.</p>"},{"location":"livehd/06-lnast/#sra","title":"<code>sra</code>","text":"<p>Right-shift R-1 by R-2.</p>"},{"location":"livehd/06-lnast/#ne","title":"<code>ne</code>","text":"<p>Not equal to.</p>"},{"location":"livehd/06-lnast/#eq","title":"<code>eq</code>","text":"<p>Equal to.</p>"},{"location":"livehd/06-lnast/#lt","title":"<code>lt</code>","text":"<p>Less than.</p>"},{"location":"livehd/06-lnast/#le","title":"<code>le</code>","text":"<p>Less than or equal to.</p>"},{"location":"livehd/06-lnast/#gt","title":"<code>gt</code>","text":"<p>Greater than.</p>"},{"location":"livehd/06-lnast/#ge","title":"<code>ge</code>","text":"<p>Greater than or equal to.</p>"},{"location":"livehd/06-lnast/#n-ary-expressions","title":"N-ary Expressions","text":"<pre><code>&lt;op&gt; --| &lt;ref&gt;       : Lvalue\n       | &lt;ref/const&gt; : R-1     \\\n       | &lt;ref/const&gt; : R-2      \\\n       | &lt;ref/const&gt; : R-3       2 or more values\n       | ...                    /\n       | &lt;ref/const&gt; : R-N     /\n</code></pre>"},{"location":"livehd/06-lnast/#bit_and","title":"<code>bit_and</code>","text":"<p>Bitwise and.</p>"},{"location":"livehd/06-lnast/#bit_or","title":"<code>bit_or</code>","text":"<p>Bitwise or.</p>"},{"location":"livehd/06-lnast/#bit_xor","title":"<code>bit_xor</code>","text":"<p>Bitwise xor.</p>"},{"location":"livehd/06-lnast/#plus","title":"<code>plus</code>","text":"<p>Summation of R-1 to R-N.</p>"},{"location":"livehd/06-lnast/#minus","title":"<code>minus</code>","text":"<p>R-1 minus summation of R-2 to R-N.</p>"},{"location":"livehd/06-lnast/#mult","title":"<code>mult</code>","text":"<p>Product of R-1 to R-N.</p>"},{"location":"livehd/06-lnast/#div","title":"<code>div</code>","text":"<p>R-1 divided by product of R-2 to R-N</p>"},{"location":"livehd/06-lnast/#tuples","title":"Tuples","text":""},{"location":"livehd/06-lnast/#tuple_concat","title":"<code>tuple_concat</code>","text":"<pre><code>&lt;tuple_concat&gt; --| &lt;ref&gt; : Lvalue\n                 | &lt;ref&gt; : R-1 (tuple)\n                 | &lt;ref&gt; : R-2 (tuple)\n                 | ...\n                 | &lt;ref&gt; : R-N (tuple)\n</code></pre>"},{"location":"livehd/06-lnast/#tuple_add","title":"<code>tuple_add</code>","text":"<pre><code>&lt;tuple_add&gt; --| &lt;ref&gt; : Lvalue\n              | &lt;ref/const&gt;\n              | &lt;assign&gt; --| &lt;ref&gt;       \\ Field 0\n                           | &lt;ref/const&gt; /\n              | &lt;assign&gt; --| &lt;ref&gt;       \\ Field 1\n                           | &lt;ref/const&gt; /\n              |  ...\n              | &lt;assign&gt; --| &lt;ref&gt;       \\ Field N\n                           | &lt;ref/const&gt; /\n</code></pre>"},{"location":"livehd/06-lnast/#tuple_set","title":"<code>tuple_set</code>","text":"<pre><code>&lt;tuple_set&gt; --| &lt;ref&gt;        : Lvalue\n              | &lt;ref/&lt;const&gt; : 1st-level selection   \\\n              | ...                                   1..N selections\n              | &lt;ref/&lt;const&gt; : Nth-level selection   /\n              | &lt;ref/&lt;const&gt; : Rvalue\n</code></pre>"},{"location":"livehd/06-lnast/#tuple_get","title":"<code>tuple_get</code>","text":"<pre><code>&lt;tuple_get&gt; --| &lt;ref&gt;       : Lvalue\n              | &lt;ref&gt;       : Rvalue (selected from this value)\n              | &lt;ref/const&gt; : 1st-level selection   \\\n              | ...                                  1..N selections\n              | &lt;ref/const&gt; : Nth-level selection   /\n</code></pre>"},{"location":"livehd/06-lnast/#module-input-output-and-register-declaration","title":"Module Input, Output, and Register Declaration","text":"<p>In LNAST, all input/output/register are defined in the node type reference with differenct prefix of string_view, \"$\" stands for input, \"%\" stands for output, and \"#\" stands for register. </p>"},{"location":"livehd/06-lnast/#input","title":"Input","text":"<pre><code>// Pyrope\nfoo = $a\n</code></pre> <pre><code>// Verilog\ninput a;\n</code></pre> <pre><code>// C++\nauto node_input = Lnast_node::create_ref(\"$a\", line_num, pos1, pos2);\n</code></pre>"},{"location":"livehd/06-lnast/#output","title":"Output","text":"<pre><code>// Pyrope\n%out\n</code></pre> <pre><code>// Verilog\noutput out;\n</code></pre> <pre><code>// C++\nauto node_output = Lnast_node::create_ref(\"%out\", line_num, pos1, pos2);\n</code></pre>"},{"location":"livehd/06-lnast/#register","title":"Register","text":"<pre><code>// Pyrope\nreg_foo\n</code></pre> <pre><code>// Verilog\nreg reg_foo;\n</code></pre> <pre><code>// C++\nauto node_reg = Lnast_node::create_ref(\"reg_foo\", line_num, pos1, pos2);\n</code></pre>"},{"location":"livehd/10-bazel/","title":"Bazel build","text":"<p>Bazel is a relatively new build system open sourced by google. The main difference with traditional Makefiles is that it checks to make sure that dependences are not lost and the builds are reproducible and hermetic. This document explains how to use Bazel in the LGraph project.</p> <p>Build targets are referred to using the syntax <code>//&lt;relative path to BUILD file&gt;:&lt;executable&gt;</code>, where <code>//</code> is the path of the root livehd directory.</p> <p>To build the LiveHD shell and supporting libraries, the target would be <code>//main:all</code>. To build every target in LiveHD (helpful for checking if changes cause compilation failures), the target would be <code>//:...</code>.  For more details on target syntax, see this page.</p>"},{"location":"livehd/10-bazel/#release-vs-fastbuild-default-vs-debug","title":"Release vs fastbuild (default) vs debug","text":"<p>For debugging/development use <code>-c dbg</code>, for benchmarking and testing <code>-c opt</code>.</p> <ul> <li> <p>Fast build: no optimization, minimal debugging information (no local variable information), assertions turned on (default) <pre><code>$ bazel build &lt;target&gt;\n</code></pre></p> </li> <li> <p>Debug build: some optimization, full debugging information, assertions turned on <pre><code>$ bazel build -c dbg &lt;target&gt;\n</code></pre></p> </li> </ul> <p>or use address sanitizer to detect memory leaks <pre><code>$ bazel build -c dbg --config asan //...\n</code></pre></p> <p>or use thread sanitizer to detect data races <pre><code>$ bazel build -c dbg --config tsan //...\n</code></pre></p> <ul> <li> <p>Release build: most optimization, no debug symbols, assertions turned off <pre><code>$ bazel build -c opt &lt;target&gt;\n</code></pre></p> </li> <li> <p>Benchmarking build: aggressive optimization for the current architecture (binary may not be portable!) <pre><code>$ bazel build --config=bench &lt;target&gt;\n</code></pre></p> </li> </ul>"},{"location":"livehd/10-bazel/#see-the-commands-executed","title":"See the commands executed","text":"<p>The bazel '-s' option prints the command executed. The sandbox may still be deleted.</p> <pre><code>$ bazel build -s //main:all\n</code></pre>"},{"location":"livehd/10-bazel/#keep-all-the-files-in-bazel-run-for-debugging","title":"Keep all the files in bazel run for debugging","text":"<p>Bazel runs process in a sandbox what it is deleted after each run. To preserve it for debugging a failing test.</p> <pre><code>bazel test --sandbox_debug -c dbg //YOUR_TEST_HERE\n</code></pre> <p>Check the failing log, it will show you the sandbox location. You can change directory to it, and debug as usual.</p>"},{"location":"livehd/10-bazel/#to-run-fixme-tests","title":"To run FIXME tests","text":"<p>Many times, we have new tests that make the regression fail. We use \"fixme\" if the test is a new one and LGraph is still not patched. We want the test in the system, but we do not want to make fail the regressions.</p> <p>Those tests are marked with tags \"fixme\" in the BUILD. E.g:</p> <pre><code>sh_test(\n    name = \"my_test.sh\",\n    tags = [\"fixme\"],  # This is a fixme test. It fails, but we should fix it\n    srcs = [\"tests/pyrope_test.sh\"],\n</code></pre> <p>To run all the fixme tests <pre><code>$ bazel test --test_tag_filters \"fixme\" &lt;target&gt;\n</code></pre> To list all the fixme tests (the goal is to have zero) <pre><code>$ bazel query 'attr(tags, fixme, tests(&lt;target&gt;))'\n</code></pre></p>"},{"location":"livehd/10-bazel/#list-bazel-targets-starting-from-top-directory","title":"List bazel targets starting from top directory","text":"<pre><code>$ bazel query &lt;target&gt;\n</code></pre>"},{"location":"livehd/10-bazel/#list-bazel-targets-starting-from-any-directory","title":"List bazel targets starting from any directory","text":"<pre><code>$ bazel query &lt;target&gt;\n</code></pre>"},{"location":"livehd/10-bazel/#list-files-needed-for-a-given-target","title":"List files needed for a given target","text":"<pre><code>$ bazel query \"deps(&lt;target&gt;)\"\n</code></pre>"},{"location":"livehd/10-bazel/#list-all-the-passes-that-use-core-those-should-be-listed-at-mainbuild-deps","title":"List all the passes that use core (those should be listed at main/BUILD deps)","text":"<pre><code>$ bazel query \"rdeps(//pass/..., //core:all)\" | grep pass_\n</code></pre>"},{"location":"livehd/10-bazel/#clear-out-cache-not-needed-in-most-cases","title":"Clear out cache (not needed in most cases)","text":"<p>This command is useful for benchmarking build time, and when system parameters change (the compiler gets upgraded, for example) <pre><code>$ bazel clean --expunge\n</code></pre></p>"},{"location":"livehd/10-bazel/#to-run-long-tests","title":"To run LONG tests","text":"<p>In addition to the short tests, there are sets of long tests that are run frequently but not before every push to main line. The reason is that those are multi-hour tests. <pre><code>$ bazel test --test_tag_filters \"long1\" &lt;target&gt;\n</code></pre> There are up to 8 long tests categories (long1, long2, long3...). Each of those tests groups should last less than 4 hours when running in a dual core machine (travis or azure).</p> <p>To list the tests under each tag. E.g., to list all the tests with long1 tag. <pre><code>$ bazel query 'attr(tags, long1, tests(&lt;target&gt;))'\n</code></pre></p>"},{"location":"livehd/10-bazel/#debugging-with-bazel","title":"Debugging with bazel","text":"<p>First run the tests to see the failing one. Then run with debug options the failing test. E.g: <pre><code>$ bazel run -c dbg //eprp:all\n</code></pre> Increase logging level if wanted <pre><code>$ LGRAPH_LOG=info bazel run -c dbg //eprp:all\n</code></pre> To run with gdb <pre><code>$ bazel build -c dbg //eprp:eprp_test\n$ gdb bazel-bin/eprp/eprp_test\n(gdb) b Eprp::run\n(gdb) r\n</code></pre> (lldb is also supported.)</p>"},{"location":"livehd/10-bazel/#to-create-a-fully-static-binary","title":"To create a fully static binary","text":"<p>In the cc_binary of the relevant BUILD file, add <code>linkopts = ['-static']</code></p> <p>Notice that the lgshell still needs the directory inside <code>bazel-bin/main/lgshell.runfiles when using inou.yosys.\\*</code></p>"},{"location":"livehd/10b-3rdparty/","title":"3rd Party","text":"<p>This section is for more advanced users that want to build LiveHD with some external 3rd party tool.</p> <p>When integrating LiveHD with a 3rd party tool (nextpnr in this example), you can either bring the 3rd party tool to LiveHD and hence build it with bazel, or you can export the LiveHD code/libraries and integrate with the 3rd party project. This document covers the later case.</p>"},{"location":"livehd/10b-3rdparty/#requirements","title":"Requirements","text":"<p>Bazel pulls a specific set of library dependences, if you export, you must ensure that the 3rd party tool uses the same library version. The 3 main source of likely conflict is \"boost\", \"abseil\", and \"fmt\". The \"fmt\" library is unlikely to be a conflict because LiveHD uses it as \"header\" only to avoid conflicts with other tools like slang.</p> <p>To check the boost and abseil version, the easiest way: <pre><code>bazel build -c dbg //main:all\n# boost version 1.71 in this case\ngrep -i \"define BOOST_VERSION \" bazel-*/external/boost//boost/version.hpp\n#define BOOST_VERSION 107100\n\n# abseil version 20210324\ngrep \"define ABSL_OPTION_INLINE_NAMESPACE_NAME\" bazel-*/external/com_google_absl/absl/base/options.h\n#define ABSL_OPTION_INLINE_NAMESPACE_NAME lts_20210324\n</code></pre></p>"},{"location":"livehd/10b-3rdparty/#nextpnr-example","title":"nextpnr example","text":"<p>nextpnr uses boost, in the previous example, you need to compile it with boost 1.71, with the usual requirements:</p> <pre><code># nextpnr ice40 needs icestorm, so install it first\ngit clone https://github.com/cliffordwolf/icestorm.git\ncd icestorm\nmake\nsudo make install\n\n# compile nextpnr itself\ngit clone https://github.com/YosysHQ/nextpnr.git\ncd nextpnr\nmkdir build\ncd build\ncmake -DARCH=ice40 ../\nmake -j $(ncpus)\n</code></pre> <p>The previous steps should compile before you attempt to integrate LiveHD to nextpnr.</p> <p>Then, you need to clone and compile LiveHD. If you clone and compile parallel to nextpnr</p> <pre><code>git clone https://github.com/masc-ucsc/livehd.git\ncd livehd\nbazel build -c dbg //main:all  # You could use -c opt for faster/optimized compilation\ncd ../nextpnr/build/\nln -s ../../livehd/\nln -s livehd/bazel-out\nln -s livehd/bazel-livehd\n</code></pre> <p>Then, we need to copy the bazel gcc build instructions and combine with the nextpnr build</p> <p>Copy this to a file called <code>pp</code>: <pre><code>--- livehd.params   2021-09-25 17:47:36.656724997 -0700\n+++ livehd.params   2021-09-25 17:40:24.365650808 -0700\n@@ -1,16 +1,17 @@\n--o\n-bazel-out/k8-dbg/bin/main/lgshell\n+-std=c++17\n+-Wno-unknown-pragmas\n+-I livehd/eprp -I livehd/elab -I bazel-livehd/external/com_google_absl -I bazel-livehd/external/fmt/include/ -I bazel-livehd/external/iassert/src -I livehd/mmap_lib/include -I livehd/core -I livehd/task -I livehd/lemu -I ./bazel-livehd/external/rapidjson -I livehd/pass/common -I ./bazel-livehd/external/replxx/include\n+./extra.cpp\n -pie\n -fuse-ld=gold\n -Wl,-no-as-needed\n -Wl,-z,relro,-z,now\n -B/usr/bin\n -pass-exit-codes\n -lstdc++\n -lm\n-bazel-out/k8-dbg/bin/main/_objs/lgshell/main.pic.o\n -Wl,--start-lib\n bazel-out/k8-dbg/bin/main/_objs/main/inou_lef_api.pic.o\n bazel-out/k8-dbg/bin/main/_objs/main/main_api.pic.o\n bazel-out/k8-dbg/bin/main/_objs/main/meta_api.pic.o\n bazel-out/k8-dbg/bin/main/_objs/main/top_api.pic.o\n</code></pre></p> <p>The patch adds a new c++ file to compile (<code>extra.cpp</code>). It will be nicer if the file is in the nextpnr directory structure, but this is as an example of how to integrate. <code>extra.cpp</code> has a call to LiveHD to open a database as example.</p> <pre><code>cp livehd/bazel-bin/main/lgshell-2.params livehd.params\npatch &lt;pp\n</code></pre> <p>This example uses <code>extra.cpp</code> as a sample LiveHD call inside nextpnr. The <code>extra.cpp</code> contents:</p> <pre><code>#include \"lgraph.hpp\"\n\nvoid some_func() {\n  Lgraph *lg = Lgraph::open(\"lgdb\",\"top\");\n\n  lg-&gt;dump();\n}\n</code></pre> <p>Then you need to add the <code>@livehd.params</code> to the end of the <code>nextpnr-ice40</code> link step. A way to get the command line is to use the <code>VERBOSE=1</code> option.</p> <pre><code>rm -f nextpnr-ice40\nmake VERBOSE=1 nextpnr-ice40\n</code></pre> <p>Cut and paste the command, it will end with something like <code>thon3.9.so @livehd.params</code> to be something like: <pre><code>/usr/bin/c++ -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-array-bounds -fPIC -O3 -g -pipe -flto -fno-fat-lto-objects CMakeFiles/nextpnr-ice40.dir/common/archcheck.cc.o CMakeFiles/nextpnr-ice40.dir/common/basectx.cc.o CMakeFiles/nextpnr-ice40.dir/common/bits.cc.o CMakeFiles/nextpnr-ice40.dir/common/command.cc.o CMakeFiles/nextpnr-ice40.dir/common/context.cc.o CMakeFiles/nextpnr-ice40.dir/common/design_utils.cc.o CMakeFiles/nextpnr-ice40.dir/common/embed.cc.o CMakeFiles/nextpnr-ice40.dir/common/handle_error.cc.o CMakeFiles/nextpnr-ice40.dir/common/idstring.cc.o CMakeFiles/nextpnr-ice40.dir/common/idstringlist.cc.o CMakeFiles/nextpnr-ice40.dir/common/log.cc.o CMakeFiles/nextpnr-ice40.dir/common/nextpnr.cc.o CMakeFiles/nextpnr-ice40.dir/common/nextpnr_assertions.cc.o CMakeFiles/nextpnr-ice40.dir/common/nextpnr_namespaces.cc.o CMakeFiles/nextpnr-ice40.dir/common/nextpnr_types.cc.o CMakeFiles/nextpnr-ice40.dir/common/place_common.cc.o CMakeFiles/nextpnr-ice40.dir/common/placer1.cc.o CMakeFiles/nextpnr-ice40.dir/common/placer_heap.cc.o CMakeFiles/nextpnr-ice40.dir/common/property.cc.o CMakeFiles/nextpnr-ice40.dir/common/pybindings.cc.o CMakeFiles/nextpnr-ice40.dir/common/report.cc.o CMakeFiles/nextpnr-ice40.dir/common/router1.cc.o CMakeFiles/nextpnr-ice40.dir/common/router2.cc.o CMakeFiles/nextpnr-ice40.dir/common/sdf.cc.o CMakeFiles/nextpnr-ice40.dir/common/str_ring_buffer.cc.o CMakeFiles/nextpnr-ice40.dir/common/svg.cc.o CMakeFiles/nextpnr-ice40.dir/common/timing.cc.o CMakeFiles/nextpnr-ice40.dir/common/timing_opt.cc.o CMakeFiles/nextpnr-ice40.dir/3rdparty/json11/json11.cpp.o CMakeFiles/nextpnr-ice40.dir/json/jsonwrite.cc.o CMakeFiles/nextpnr-ice40.dir/frontend/json_frontend.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/arch.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/arch_place.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/arch_pybindings.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/bitstream.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/cells.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/chains.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/delay.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/gfx.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/main.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/pack.cc.o CMakeFiles/nextpnr-ice40.dir/ice40/pcf.cc.o CMakeFiles/chipdb-ice40.dir/ice40/chipdb/chipdb-384.cc.o CMakeFiles/chipdb-ice40.dir/ice40/chipdb/chipdb-1k.cc.o CMakeFiles/chipdb-ice40.dir/ice40/chipdb/chipdb-5k.cc.o CMakeFiles/chipdb-ice40.dir/ice40/chipdb/chipdb-u4k.cc.o CMakeFiles/chipdb-ice40.dir/ice40/chipdb/chipdb-8k.cc.o -o nextpnr-ice40  -ltbb /usr/lib/x86_64-linux-gnu/libboost_filesystem.so /usr/lib/x86_64-linux-gnu/libboost_program_options.so /usr/lib/x86_64-linux-gnu/libboost_iostreams.so /usr/lib/x86_64-linux-gnu/libboost_system.so /usr/lib/x86_64-linux-gnu/libboost_thread.so -lpthread /usr/lib/x86_64-linux-gnu/libboost_regex.so /usr/lib/x86_64-linux-gnu/libboost_chrono.so /usr/lib/x86_64-linux-gnu/libboost_date_time.so /usr/lib/x86_64-linux-gnu/libboost_atomic.so -lpthread /usr/lib/x86_64-linux-gnu/libpython3.9.so @livehd.params\n</code></pre></p> <p>You can check that the new binary includes liveHD with something like: <pre><code>nm nextpnr-ice40 | grep -i Lgraph\n</code></pre></p>"},{"location":"livehd/11-pass/","title":"Creating a Pass","text":"<p>This document provides some minimal suggestion on how to build a new LiveHD pass.</p> <p>Most LiveHD passes reside inside <code>inou</code> or <code>pass</code>. The only difference is that <code>inou</code> focuses on translation from some external tool to/from LiveHD while <code>pass</code> works on transformations from LiveHD to LiveHD.</p>"},{"location":"livehd/11-pass/#create-a-pass","title":"Create a pass","text":"<p>Check the <code>pass/sample</code> directory for how to create a trivial pass.</p> <ul> <li>Create pass/XXX directory</li> </ul> <p>The typical is to have these files:</p> <ul> <li>pass/XXX/pass_XXX.[cpp|hpp]: C++ and Header file to interface with lgshell</li> <li>pass/XXX/XXX.[cpp|hpp]: C++ file to perform the pass over a Lgraph or LNAST API</li> <li>pass/XXX/BUILD: the Bazel build configuration file</li> <li>pass/XXX/tests/XXX_test.cpp: A google test checking the pass</li> </ul> <p>Finally, add the new pass to <code>main/BUILD</code></p>"},{"location":"livehd/11-pass/#pass-parameters-and-common-variables","title":"Pass Parameters and Common variables","text":"<p>One of the main goals is to have a uniform set of passes in lgshell. lgshell should use this common variable names when possible</p> <pre><code>    name:foo        lgraph name\n    path:lgdb       lgraph database path (lgdb)\n    files:foo,var   comma separated list of files used for INPUT\n    odir:.          output directory to generate files like verilog/pyrope...\n</code></pre>"},{"location":"livehd/11-pass/#some-hintscomments-useful-for-developers","title":"Some hints/comments useful for developers","text":""},{"location":"livehd/11-pass/#using-clang-when-building","title":"Using clang when building","text":"<p>The regression system builds for both gcc and clang. To force a clang build, set the following environment variables before building:</p> <pre><code>CXX=clang++ CC=clang bazel build -c dbg //...\n</code></pre>"},{"location":"livehd/11-pass/#perf-in-lgbench","title":"Perf in lgbench","text":"<p>Use lgbench to gather statistics in your code block. It also allows to run perf record for the code section (from lgbench construction to destruction). To enable perf record set LGBENCH_PERF environment variable</p> <pre><code>export LGBENCH_PERF=1\n</code></pre>"},{"location":"livehd/11-pass/#gdblldb-usage","title":"GDB/LLDB usage","text":"<p>For most tests, you can debug with</p> <pre><code>gdb ./bazel-bin/main/lgshell\n</code></pre> <p>or</p> <pre><code>lldb ./bazel-bin/main/lgshell\n</code></pre> <p>Note that breakpoint locations may not resolve until lgshell is started and the relevant LGraph libraries are loaded.</p>"},{"location":"livehd/11-pass/#address-sanitizer","title":"Address Sanitizer","text":"<p>LiveHD has the option to run it with address sanitizer to detect memory leaks.</p> <pre><code>bazel build -c dbg --config asan //...\n</code></pre>"},{"location":"livehd/11-pass/#thread-sanitizer","title":"Thread Sanitizer","text":"<p>To debug with concurrent data race.</p> <pre><code>bazel build -c dbg --config tsan //...\n</code></pre>"},{"location":"livehd/11-pass/#debugging-a-broken-docker-image","title":"Debugging a broken Docker image","text":"<p>The travis/azure regressions run several docker images. To debug the issue, run the same as the failing docker image. c++ OPT with archlinux-masc image</p> <ol> <li>Create some directory to share data in/out the docker run (to avoid    mistakes/issues, I would not share home directory unless you have done it    several times before)</li> </ol> <pre><code>mkdir $HOME/docker\n</code></pre> <ol> <li>Run the docker image (in some masc docker images you can change the user to not being root)</li> </ol> <pre><code>docker run --rm --cap-add SYS_ADMIN -it  -e LOCAL_USER_ID=$(id -u $USER) -v ${HOME}/docker:/home/user mascucsc/archlinux-masc\n\n# Once inside docker image. Create local \"user\" at /home/user with your userid\n/usr/local/bin/entrypoint.sh\n</code></pre> <ol> <li>If the docker image did not have the livehd repo, clone it</li> </ol> <pre><code>git clone https://github.com/masc-ucsc/livehd.git\n</code></pre> <ol> <li>Build with the failing options and debug</li> </ol> <pre><code>CXX=g++ CC=gcc bazel build -c opt //...\n</code></pre> <p>A docker distro that specially fails (address randomizing and muslc vs libc) is alpine. The command line to debug it:</p> <pre><code>docker run --rm --cap-add SYS_ADMIN -it -e LOCAL_USER_ID=$(id -u $USER) -v $HOME:/home/user -v/local/scrap:/local/scrap mascucsc/alpine-masc\n</code></pre>"},{"location":"livehd/12-github/","title":"GitHub Guide","text":"<p>LiveHD is the synthesis/emulation flow primarily maintained and developed by the MASC lab at UC Santa Cruz. Since LiveHD is used for computer architecture and VLSI research, the MASC lab has an internal private repo for some still in progress works. This is done using a private repo so that we can wait until the research is published before pushing changes to the public repo hosted on GitHub.</p> <p>This guide explains how we use git at the MASC group, and how you could setup a similar flow to contribute to the LiveHD project. Other groups may choose to adapt this technique for their own use.</p> <p>LiveHD uses bazel as a build system, as a result, we no longer use submodules. Instead we use the built-in bazel support to pull specific repositories.</p>"},{"location":"livehd/12-github/#github-configuration-and-commands","title":"Github Configuration and Commands","text":"<p>This section is for git first time users and to show the git configuration used by the MASC group.</p>"},{"location":"livehd/12-github/#configuration","title":"Configuration","text":"<p>Suggested options for git first time users</p> <pre><code># Rebase no merge by default\ngit config --global pull.rebase true\n# Set your name and email\ngit config --global user.email \"perico@lospalotes.com\"\ngit config --global user.name \"Perico LosPalotes\"\ngit config --global pull.rebase true\ngit config --global rebase.autoStash true\n</code></pre>"},{"location":"livehd/12-github/#rebase-vs-no-rebase","title":"Rebase vs No-Rebase","text":"<p>Rebase creates cleaner logs, but sometimes it gets difficult to fix conflicts with rebase. For cases that you are struggling to merge a conflict, you could do this:</p> <pre><code># undo the failed rebase merge\ngit rebase --abort\n\n# make sure that your code changes were committed\ngit commit -a -m \"Your commit message\"\ngit pull --no-rebase\n\n# Fix the conflict without rebase (easier)\ngit commit -a -m \"your merge message\"\ngit pull --no-rebase\ngit push\n</code></pre>"},{"location":"livehd/12-github/#typical-git-commands","title":"Typical git commands","text":"<p>Clean the directory from any file not in git (it will remove all the files not committed)</p> <pre><code>git clean -fdx\n</code></pre> <p>Save and restore un-committed changes to allow a new git pull. stash is like a \"push\" and \"pop\" replays the changes in the current directory. This will happen automatically if you have the autoStash configuration option.</p> <pre><code>git stash\ngit pull\ngit stash pop\n</code></pre> <p>See the differences against the server (still not pushed). Everything may be committed, so git diff may be empty</p> <pre><code>git diff @{u}\n</code></pre>"},{"location":"livehd/12-github/#git-hercules-statistics","title":"Git Hercules statistics","text":"<pre><code>hercules --languages C++ --burndown --burndown-people --pb https://github.com/masc-ucsc/livehd &gt;hercules1.data\nlabours -f pb -m overwrites-matrix -o hercules1a.pdf &lt;hercules1.data\nlabours -f pb -m ownership -o hercules1b.pdf &lt;hercules1.data\n\nhercules --languages C++ --burndown --first-parent --pb https://github.com/masc-ucsc/livehd &gt;hercules2.data\nlabours -f pb -m burndown-project -o hercules2.pdf &lt;hercules2.data\n\nhercules --languages C++ --devs --pb https://github.com/masc-ucsc/livehd &gt;hercules3.data\nlabours -f pb -m old-vs-new -o hercules3a.pdf &lt;hercules3.data\nlabours -f pb -m devs -o hercules3b.pdf &lt;hercules3.data\nlabours -f pb -m devs-efforts -o hercules3c.pdf &lt;hercules3.data\n</code></pre>"},{"location":"livehd/12-github/#testdeveloper-livehd-case-no-commits","title":"Test/Developer LiveHD case (no commits)","text":"<p>If you do not plan to do many changes, and just wants to try LiveHD or be a LiveHD user, the easiest way is to just clone the repo:</p> <pre><code>git clone https://github.com/masc-ucsc/livehd\ncd livehd\n</code></pre> <p>From time to time, you should get the latest version to have the latest bug fixes/patches. Just a typical git pull should suffice:</p> <pre><code>git pull\n</code></pre>"},{"location":"livehd/12-github/#infrequent-contributor-flow-advanced-users","title":"Infrequent Contributor Flow (ADVANCED USERS)","text":"<p>These are instructions for advanced users, more likely other university/company institutions with a team working on this project. The larger team may want to have some private repository with internal development and some pushes/pulls to the main LiveHD repo. For single external users, I would suggest to just fork the repository and do pull requests.</p> <p>If you work outside UCSC and/or you are an infrequent contributor, you have two main options: fork or private clone. The fork approach requires you to have your repository public, if you have publications or work-in-progress that you do not want to share the best option is to have a private repo (livehd-private).</p> <p>The simplest way to contribute to LiveHD is to create a public repo or a public fork, and a pull request. Most git guides use the origin/master (in fork or private repo) to synchronize with upstream/master (upstream main LiveHD repo). This means that your local changes should NOT go to your origin/master. Instead, you should create a branch for your local work. This works like a charm if you do pull requests, and it is reasonable if you have a long development branch without intention to push upstream.</p> <p>Although it is possible to create a different setup, we recommend that you keep the origin/master clean to synchronize with upstream/origin. You should create a new branch for each feature that you may want to upstream (origin/feature-x), and a local development branch (dev) for all your team members.</p> <ol> <li> <p>Clone the repo:</p> <pre><code>git clone https://github.com/masc-ucsc/livehd.git livehd\ncd livehd\n</code></pre> </li> <li> <p>Create development branch (dev)</p> <pre><code>git checkout -b dev\n</code></pre> </li> <li> <p>Create a branch from origin/master to create a pull request to upstream/master</p> <pre><code>git checkout -b pull_request_xxx\n</code></pre> </li> <li> <p>Create a branch from dev for internal development if needed</p> <pre><code>git checkout -b feature_xx_4dev dev\n</code></pre> </li> <li> <p>Synchronize origin/master from main upstream/master</p> <p>Add remote upstream (if not added before)</p> <pre><code>git remote -v\n</code></pre> <p>If remote -v did not list upstream. Add them</p> <pre><code>git remote add upstream https://github.com/masc-ucsc/livehd.git\ngit fetch upstream\n</code></pre> <p>Make sure that you are in origin/master</p> <pre><code>git checkout master\n</code></pre> <p>Bring the changes from the remote upstream/master to local master/origin</p> <pre><code>git merge upstream/master\n</code></pre> <p>Push to repo origin/master if everything was fine</p> <pre><code>git push origin master\n</code></pre> <p>To see the difference with upstream (it should be empty)</p> <pre><code>git diff @{upstream}\n</code></pre> </li> <li> <p>Synchronize the dev branch with the latest master sync</p> <pre><code>git checkout dev\ngit merge origin/master\ngit push # same as \"push origin dev\" because dev is checkout\n</code></pre> </li> <li> <p>In case that you did not, push to the corresponding branch to the server</p> <pre><code>git push origin dev\ngit push origin pull_request_xxx\ngit push origin feature_xx_4dev\n</code></pre> </li> <li> <p>Create new pull request to upstream</p> <p>Make sure that origin/master is in sync (step 5)</p> <pre><code>git diff @{upstream} # should be empty\n</code></pre> <p>Rebase/merge the feature request with latest origin master</p> <pre><code>git checkout pull_request_xxx\ngit rebase master\ngit push upstream pull_request_xxx\n</code></pre> </li> </ol> <p>Now create a pull request through github, and the UCSC/MASC team will review it.</p>"},{"location":"livehd/12-github/#occasional-pull-request-steps","title":"Occasional Pull Request steps","text":"<p>If you just want to do some small contributions to LiveHD doing a public fork is the easiest way to contribute. Just fork, commit to forked master, and click on the web link after you push.</p>"},{"location":"livehd/12-github/#frequent-contributor","title":"Frequent Contributor","text":"<p>If you are working on LiveHD at UC Santa Cruz, contact Jose Renau to be added to the MASC organization on GitHub so that you have write access to the repo. The setup is similar to the infrequent contributor flow but you have access to directly commit to the public repository. Even the upstream/master.</p>"},{"location":"livehd/13-style/","title":"Coding Style","text":"<p>These are the coding style rules for LiveHD C++. Each rule can be broken, but it should be VERY rare, and a small comment should be placed explaining why.</p>"},{"location":"livehd/13-style/#overall","title":"Overall","text":"<ul> <li>When possible keep the system simple. Complexity is the enemy of maintenance.</li> <li>Deprecate no longer used features.</li> <li>Try to reduce friction. This means to avoid hidden/complex steps.</li> <li>Every main API should have a unit test for testing but also to demonstrate usage.</li> </ul>"},{"location":"livehd/13-style/#comments","title":"comments","text":"<p>Code should be the comments, try to keep comments concise. They should explain the WHY not the HOW. The code is the HOW.</p> <p>Labels used in comments:</p> <pre><code>// FIXME: Known bug/issue but no time to fix it at the moment\n\n// TODO: Code improvement that will improve perf/quality/??? but no time at the moment\n\n// WARNING: message for some \"strange\" \"weird\" code that if changes has effects\n// (bug). Usually, this is a \"not nice\" code that must be kept for some reason.\n\n// NOTE: Any comment that you want to remember something about (not critical)\n\n// STYLE: why you broke a style rule (pointers, iterator...)\n</code></pre>"},{"location":"livehd/13-style/#strings","title":"strings","text":"<p>We use std::string and std::string_view. These are the rules:</p> <ul> <li> <p>Arguments for functions are always std::string_view (no const std::string &amp;)</p> </li> <li> <p>If the return argument is not allocated in the function, we return a std::string_view</p> </li> <li> <p>If the return argument can be a new string, the function returns std::string</p> </li> <li> <p>Use the absl::StrCat, absl::StrAppend, absl::StrSplit when possible</p> </li> <li> <p>To convert to/from integers use str_tools::to_i to_hex ...</p> </li> <li> <p>Use str_tools for sub-string operations like str_tools::ends_with </p> </li> </ul>"},{"location":"livehd/13-style/#variable-naming-rules","title":"Variable naming rules","text":"<ul> <li>No camelCase. Use underscores to separate words: <pre><code>foo_bar = Foo_bar(3);\n</code></pre></li> <li>Use plural for containers with multiple entries like vector, singular otherwise <pre><code>elem = entries[index];\n</code></pre></li> <li>Classes/types/enums start with uppercase. Lowercase otherwise <pre><code>val = My_enum::Big;\nclass Sweet_potato {\n</code></pre></li> </ul>"},{"location":"livehd/13-style/#error-handling-and-exceptions","title":"Error handling and exceptions","text":"<p>Use the Pass::error or Pass:warn for error and likely error (warn). Internally, error generates and exception capture by the main lgshell to move to the next task.</p> <pre><code>Pass::error(\"inou_yaml: can only have a yaml_input or a graph_name, not both\");\nPass::warn(\"inou_yaml.to_lg: output:{} input:{} graph:{}\", output, input, graph_name);\n</code></pre>"},{"location":"livehd/13-style/#no-tabs-indentation-is-2-spaces","title":"No tabs, indentation is 2 spaces","text":"<p>Make sure to configure your editor to use 2 spaces</p> <p>You can configure your text editor to do this automatically</p>"},{"location":"livehd/13-style/#include-order","title":"Include order","text":"<p>First do C includes (try to avoid when possible), then an empty line with C++ includes, then an empty line followed with lgraph related includes. E.g:</p> <pre><code>#include &lt;sys/types.h&gt;\n#include &lt;dirent.h&gt;\n\n#include &lt;iostream&gt;\n#include &lt;set&gt;\n\n#include \"graph_library.hpp\"\n#include \"lgedgeiter.hpp\"\n</code></pre>"},{"location":"livehd/13-style/#keep-column-widths-short","title":"Keep column widths short","text":"<ul> <li>Less than 120 characters if at all possible (meaning not compromising   readability)</li> </ul> <p>You can configure your text editor to do this automatically</p>"},{"location":"livehd/13-style/#avoid-trailing-spaces","title":"Avoid trailing spaces","text":"<p>You can configure your text editor to highlight them.  https://github.com/ntpeters/vim-better-whitespace</p>"},{"location":"livehd/13-style/#use-c14-iterators-not-iterator","title":"Use C++14 iterators not ::iterator","text":"<pre><code>for(auto idx:g-&gt;unordered()) {\n}\n</code></pre> <p>Use structured returns when iterator is returned for cleaner code:</p> <pre><code>for(const auto &amp;[name, id]:name2id) {\n  // ...\n</code></pre>"},{"location":"livehd/13-style/#use-auto-or-const-auto-when-possible","title":"Use \"auto\", or \"const auto\", when possible.","text":"<pre><code>for(auto idx:g-&gt;unordered()) {\n  for(const auto &amp;c:g-&gt;out_edges(idx)) {\n</code></pre>"},{"location":"livehd/13-style/#const-and-local-variables","title":"const and local variables","text":"<p>It may be too verbose to write const all the time. The coding style request to use  const (when possible) in iterators and pointers. The others are up to the programmer.</p>"},{"location":"livehd/13-style/#do-not-use-stdunordered_set-stdmap-use-flat_hash_map-or-flat_hash_set-from-abseil","title":"Do not use std::unordered_set, std::map, use flat_hash_map or flat_hash_set from abseil","text":"<pre><code>#include \"absl/container/flat_hash_map.h\"\n#include \"absl/container/flat_hash_set.h\"\n\nabsl::flat_hash_map&lt;Index_ID, RTLIL::Wire *&gt;   my_example;\n</code></pre>"},{"location":"livehd/13-style/#some-common-idioms-to-handle-mapsets","title":"Some common idioms to handle map/sets","text":"<p>Traverse the map/set, and as it traverses decide to erase some of the entries: <pre><code>for (auto it = m.begin(), end = m.end(); it != end;) {\n  if (condition_to_erase_it) {\n    m.erase(it++);\n  } else {\n    ++it;\n  }\n}\n</code></pre></p> <p>To check if a key is present: <pre><code>if (set.contains(key_value)) {\n}\n</code></pre></p>"},{"location":"livehd/13-style/#use-abslspan-instead-of-stdvector-as-return-argument","title":"Use absl::Span instead of std::vector as return argument","text":"<p>absl::Span is the equivalent of string_view for a string but for vectors. Like string_view, it does not have ownership, and the size in the span can decrease (not increase) without changing the original vector with \"subspan\". Faster and more functional, no reason to return \"const std::vector &amp;\", instead return \"absl::Span\". <pre><code>#include \"absl/types/span.h\"\n\nabsl::Span&lt;Sub_node&gt;    get_sub_nodes() const {\n  I(sub_nodes.size()&gt;=1);\n  return absl::MakeSpan(sub_nodes).subspan(1); // Skip first element from vector\n};\n</code></pre>"},{"location":"livehd/13-style/#pass-by-reference-and-use-const-when-possible","title":"Pass by reference and use \"const\" when possible","text":"<pre><code>void print(const Sub_node&amp; g); //or\n\nvoid edit(Sub_node&amp; g);\n</code></pre> <p>Note that older code still uses pointers, this is no longer allowed.</p>"},{"location":"livehd/13-style/#avoid-dynamic-allocation-as-much-as-possible","title":"Avoid dynamic allocation as much as possible","text":"<p>The idea is to RARELY directly allocate pointer allocation</p> <p>Use:</p> <pre><code>foo = Sweet_potato(3, 7)\n</code></pre> <p>instead of</p> <pre><code>foo = new Sweet_potato(3, 7)\n</code></pre>"},{"location":"livehd/13-style/#do-not-use-newdelete-keywords-use-smart-pointers-if-needed-very-very-rare","title":"Do not use \"new\"/\"delete\" keywords. Use smart pointers if needed (VERY VERY rare)","text":"<p>Use: <pre><code>foo = std::make_unique&lt;Sweet_potato&gt;(3,7);\n</code></pre></p> <p>instead of</p> <pre><code>foo = new Sweet_potato(3, 7)\n</code></pre>"},{"location":"livehd/13-style/#use-fmtprint-to-print-messages-for-debugging","title":"Use fmt::print to print messages for debugging","text":"<pre><code>fmt::print(\"This is a debug message, name = {}, id = {}\\n\",g-&gt;get_name(), idx);\n</code></pre>"},{"location":"livehd/13-style/#use-accessors-consistently","title":"Use accessors consistently","text":"<ul> <li>get_XX(): gets \"const XX &amp;\" from object without side effects (assert if it does not exist)<ul> <li>operator(Y) is an alias for get_XX(Y)</li> </ul> </li> <li>ref_XX(): gets \"XX * \" (nullptr if it does not exist)</li> <li>find_XX(): similar to get_XX but, if it does not exist return invalid object (is_invalid())</li> <li>setup_XX(): gets XX from object, if it does not exists, it creates it</li> <li>create_XX(): clears previous XX from object, and creates a new and returns it</li> <li>set_XX(): sets XX to object, it creates if it does not exist. Similar to   create, but does not return reference.</li> </ul> <p>If a variable is const, it can be exposed directly without get/set accessors</p> <p>foo = x.const_var;  // No need to have x.get_const_var()</p>"},{"location":"livehd/13-style/#use-bitarray-class-to-have-a-compact-bitvector-marker","title":"Use bitarray class to have a compact bitvector marker","text":"<pre><code>bitarray visited(g-&gt;max_size());\n</code></pre>"},{"location":"livehd/13-style/#use-iassert-extensively-be-meaningful-whenever-possible-in-assertions","title":"Use iassert extensively / be meaningful whenever possible in assertions","text":"<p>This usually means use meaningful variable names and conditions that are easy to understand. If the meaning is not clear from the assertion, use a comment in the same line. This way, when the assertion is triggered it is easy to identify the problem.</p> <pre><code>I(n_edges &gt; 0); //at least one edge needed to perform this function\n</code></pre> <p>We use the https://github.com/masc-ucsc/iassert package. Go to the iassert for more details on the advantages and how to allow it to use GDB with assertions.</p>"},{"location":"livehd/13-style/#develop-in-debug-mode-and-benchmark-in-release-mode","title":"Develop in debug mode and benchmark in release mode","text":"<p>Extra checks should be only in debug. Debug and release must execute the same, only checks (not behavior change) allowed in debug mode.</p> <p>Benchmark in release. It is 2x-10x faster.</p>"},{"location":"livehd/13-style/#use-compact-ifelse-brackets","title":"Use compact if/else brackets","text":"<p>Use clang-format as configured to catch style errors. LGraph clang-format is based on google format, but it adds several alignment directives and wider terminal.</p> <pre><code>   cd XXXX\n   clang-format -i *pp\n</code></pre> <pre><code>std::vector&lt;LGraph *&gt; Inou_yaml::generate() {\n\n  if (opack.graph_name != \"\") {\n     // ...\n  } else {\n     // ..\n  }\n</code></pre>"},{"location":"livehd/13-style/#decide-how-to-use-attributes","title":"Decide how to use attributes","text":"<p>Attributes are parameters or information that an be per Node, Node_pin or Edge. In LGraph, attributes are persistent. This means that they are kept across execution runs in the LGraph database (E.g: in lgdb).</p> <p>For persistent attributes, the structures to use are defined in core/annotate.hpp. Any new attribute must be added to \"annotate.hpp\" to preserve persistence and to make sure that they are cleared when needed.</p> <p>Many times it is important to have information per node, but that it is not persistent across runs. For example, when building a LGraph from Yosys, there is a need to remember pointers from yosys to LGraph. This by definition can not be persistent because pointers change across runs. For this case, there are several options.</p>"},{"location":"livehd/13-style/#the-non-persistent-annotations","title":"The Non-Persistent Annotations","text":"<p>If the data structure needs to keep most of the node/pins in the Lgraph, use the compact_class notation: <pre><code>absl::flat_hash_map&lt;SomeData, Node_pin::Compact_class&gt; s2pin;\nabsl::flat_hash_map&lt;SomeData, Node::Compact_class&gt;     s2node;\n\nSomeData d1;\nLgraph *lg; // LGraph owning the node\ns2pin[d1]  = node.get_driver_pin().get_compact_class(); // Example of use getting a pint\ns2node[d1] = node.get_compact_class();\nauto name = s2pin[d1].get_node(lg).get_name();   // Pick previously set driver name\n</code></pre></p> <p>Another example:</p> <pre><code>absl::flat_hash_map&lt;Node_pin::Compact, RTLIL::Wire *&gt;  input_map;\n\ninput_map[pin.get_compact()] = wire;\n\nauto *wire = input_map[pin.get_compact()];\n\nfor(const auto &amp;[key, value]:input_map) {\n  Node_pin pin(lg, key); // Key is a ::Compact, not a Node_pin\n  auto name  = pin.get_name();\n  // ... Some use here\n}\n</code></pre> <p>If the data structure just holds a small subset of the graph, you can keep the metadata, and use Node/Node_pin directly. E.g:</p> <pre><code>absl::flat_hash_map&lt;SomeData, Node_pin&gt; s2pin;\nabsl::flat_hash_map&lt;SomeData, Node&gt;     s2node;\n\nSomeData d1;\ns2pin[d1]  = node.get_driver_pin(); // Example of use getting a pint\ns2node[d1] = node;\nauto name = s2pin[d1].get_name();   // Pick previously set driver name\n</code></pre> <p>In this case, it is fine to use the full Node, Node_pin, or Edge. This has some pointers inside, but it is OK because it is not persistent.</p>"},{"location":"livehd/13-style/#avoid-code-duplication","title":"Avoid code duplication","text":"<p>The rule is that if the same code appears in 3 places, it should be refactored</p> <p>Tool to detect duplication <pre><code>    find . -name '*.?pp' | grep -v test &gt;list.txt\n    duplo -ml 12 -pt 90 list.txt report.txt\n</code></pre></p>"},{"location":"pyrope/00-hwdesign/","title":"Hardware Design","text":"<p>Most language manuals/guides do not include a chapter on \"what is programming?\", but Pyrope is a hardware description language or HDL.  This document is a guideline for hardware design for software designers. The idea is to be high level to explain the differences without going to syntactic details in different languages.</p>"},{"location":"pyrope/00-hwdesign/#no-von-neumann","title":"No Von Neumann","text":"<p>Most software programming languages are built to program Von Neumann CPUs. When dealing with single-threaded code, the programmer has a sequence of \"commands\" or \"statements\" specified in a programming language. The machine executes one of those statements after another. There are \"control flow\" instructions to select what is the following statement to execute.</p> <p>Potentially more restricting, software languages have a central or distributed \"memory\" concept where program data resides. For single-threaded code, there tends to be a main unified memory.</p> <p>Current CPUs follow a Von Neumann approach<sup>1</sup>. Languages designed to program CPUs have the same model to map existing hardware efficiently. Since the languages are Von Neumann, it is logical that CPUs also evolve to keep the same model and further improve the performance. Since CPUs and languages follow the same Von Neumann model, software designers' mindset assumes this same concept. This feedback loop has resulted in most languages, hardware, and developers evolving around this model.</p> <p>Neither FPGAs nor ASICs is Von Neumann machines. There is no program counter to execute one statement after another, and there is no central memory. Those concepts could be built on top, and this is in fact what CPUs are about. CPUs are all about making efficient Von Neumann machines on top of FPGAs and/or ASICs given some power/area/performance constraints.</p> <p>To design a CPU or any hardware without a Von Neumann model, the standard concepts that all languages use of main memory or single-threaded execution do not make much sense. The reason is that the cells available in hardware are always there. The result can be used or not, but it is always there.</p> <p>At a high level, hardware designers decide the basic hardware constructs to include in the design (adders, logic gates...) and how to connect them. Those hardware blocks will be there all the time, and the connection is fixed too. In contrast, a software designer needs to build efficient programs for one or more Von Neumann CPUs.</p>"},{"location":"pyrope/00-hwdesign/#optimization-knobs","title":"Optimization knobs","text":"<p>Programming hardware and software are all about solving a problem to meet some performance/power/cost constraints using the available resources. The difference is that the resources in hardware and software are not the same. In software, there are instructions; in hardware, there are cells<sup>2</sup>. This results in different optimization knobs.</p> <p>Designing an efficient software program is all about deciding the sequence of instructions to be small and fast. The computer architecture Iron's law summarizes it well. The performance is \"Instruction Count\" x \"Instructions Per Cycle\" x \"Frequency\". Since software programmers do not tend to consider the frequency, it is all about executing fewer instructions and doing each instruction as fast as possible. The software designer has to create a sequence of instructions to solve a problem. Those instructions could use resources like memory.</p> <p>Instead of instructions, most hardware designers have hardware blocks or cells like adders, multiplexors, flops, SRAMs... There are no central resources like memory, and they have to consider frequency.</p> <p>Like a software designer, the hardware designer needs to solve a problem. Still, instead of selecting a sequence of instructions, the designer selects the cells or hardware blocks and their interconnection. The designer divides problems into smaller pipeline stages to have a high frequency. A design with small cells that can achieve the desired frequency is an exemplary hardware design<sup>3</sup>. So it is all about instantiating blocks and connecting them.</p> <p>In hardware, there are two big categories of blocks: Combinational and Sequential. Combinational do not have a clock and perform operations like adding, and, xor... Sequential has a clock.  The clock is used to remember a value. Hence the output of a sequential block retains or memoizes the value of previous cycles while combinational blocks have no memory or clock concept.</p> <p>The hardware blocks are physical devices that need time to generate a valid output given a change in their inputs. When combinational blocks are connected, their maximum speed or frequency can be decided by finding the slowest path in the combinational blocks connected. This means that sequential blocks should separate combinational blocks to achieve higher frequency. This is called pipelining. There are overheads of adding more sequential blocks, and the hardware designer needs to find the correct balance given a set of constraints like area/frequency/power.</p> <p>A significant effort in hardware design goes to pipelining. Not only to find the correct spot separating combinational blocks but because the sequential block adds a concept of \"state\" or memory. Starting from a working combination lock and randomly adding some \"sequential\" blocks is highly likely to generate an incorrect result. Pipelining adds the conceptual problem that adding sequential blocks changes the semantics and that ALL the combinational blocks should have more or less the same frequency. Otherwise, the pipeline design is unbalanced<sup>4</sup>, and the slowest pipeline decides the overall frequency.</p> <p>The pipelining optimization concept is very different from software optimization.  In software, designers care about the average. If a function is slow and its execution requires half of the execution time, reducing by half should have a 25% performance improvement. The designer does not need to improve very infrequently used functions. In hardware, designers care about the worst case. If a pipeline stage is slow, improving it will result in a frequency improvement if it was the slowest. The benefit will be just the difference with the next slower pipeline stage, not optimizing the pipeline itself.</p> <p>The result is that hardware and software designers need to worry about different constraints like pipelining. Combined with the fact that hardware optimizations need to care about the worst case, not average, it is common for hardware designers to say that designing hardware is hard.</p>"},{"location":"pyrope/00-hwdesign/#hls-vs-hdl","title":"HLS vs. HDL","text":"<p>Hardware designers also use programming languages to specify their FPGA/ASIC design. In the past, designers \"drew\" the transistor/cells/gates and had a more visual layout to see/place where each combination and the sequential block was located. Although it is possible to do a design in such a way, it is not as productive as using some hardware design language.</p> <p>There are many popular software languages like C++, Java, Rust, swift... There are also several hardware design languages, but they tend to fall into two categories: HLS (High-Level Synthesis) or HDLs (Hardware Description Languages). HLS can be languages like a subset of C or Rust. The HDLs are languages like Verilog, CHISEL, or Pyrope.</p> <p>In a nutshell, HLS tries to leverage the larger Von Neumann community (languages and humans that know to program Von Neumann) and use compilers to transform to efficient hardware mappings that are not Von Neumann.</p> <p>HLS has to deal with constructs like loops and central memory. The typical solution for loops is to use heuristics and/or directives to split the loops into different pipeline stages. The standard solution for global memory is not to use it or put compiler directives to guide them. Other constructs like memory allocation and recursion are also avoided in HLS. When a C program is translated to hardware, if it has pointers and uses memory, it needs directives to indicate where the memory resides and mark potential overlap or pointer aliasing between pointers. Otherwise, the generated translation is likely to be inefficient.</p> <p>HDLs (Hardware Description Languages) do not have a Von Neumann model. The currently most popular HDL (Verilog) is a data flow language that does not have a global program counter like Von Neumann languages. Instead, the programmer specifies a hierarchy of modules<sup>5</sup>. In Verilog, the execution of each module has a complicated set of options, but from a high-level point of view, a group of statements is executed in each module. The module executes forever because it is a set of gates instantiated in the hardware design.</p>"},{"location":"pyrope/00-hwdesign/#hardware-artifacts","title":"Hardware artifacts","text":"<p>This section goes over several of the leading hardware artifacts that tend to be exist in most HDLs independent of the syntax.</p>"},{"location":"pyrope/00-hwdesign/#instantiation-vs-execution","title":"Instantiation vs execution","text":"<p>Hardware designers decide the gates are instantiated<sup>6</sup> in the design, while software designers focus on executing the instructions. If a set of gates is rarely used, the hardware still has to instantiate them, and their performance area impact is independent of the usage frequency. In software, a set of seldom executed instructions have no performance impact. This is not the case in hardware. As such, languages build around \"instantiation\" more than traditional instruction execution.</p> <p>Instantiation means that the designer explicitly indicates the gates or circuits mapped to hardware. In the vast majority of HDLs (Verilog, CHISEl, pyRTL, VHDL...), the designer specifies a top-level module. Each module can have a set of gates and more instantiated sub-modules.</p> <p>In \"software,\" languages have a \"main\" or starting execution point. The \"main\" executes or calls several functions depending on the data. Functions can iterate in loops, and the program finishes execution when the main finishes.</p> <p>In contrast, most HDLs differ from software languages in that they specify an instantiation tree hierarchy of modules, and then provide some syntax on how each module executes independently of the other modules.</p> <p>In HDLs, the execution never ends, and the modules run independently. It resembles a bit of an actor model. An actor is a module with individual execution, but there are many differences like the tree structure of instantiations and the incapacity to spawn new actors. Although it is possible to build an HLS around the actor model without spawning capability, popular HDLs do not.</p> <p>In most HDLs, the instantiated tree hierarchy is fixed. This makes sense from a hardware point of view<sup>7</sup>, but this means that a module can not be called inside a control flow statement. A common mistake from designers learning HDLs is to call a module conditionally. E.g., this code sequence is not what a software programmer may expect:</p> Problematic codePossible solution <pre><code>var result = 0\nif some_opcode {\n  result = do_division(a,b)\n}else{\n  result = do_multiplication(a,b)\n}\n</code></pre> <pre><code>var result = 0\nvar result1 = do_division(a,b)\nvar result2 = do_multiplication(a,b)\nif some_opcode {\n  result = result1\n}else{\n  result = result2\n}\n</code></pre> <p>A software programmer thinks about executing instructions. The previous syntax looks like execute or call <code>do_division</code> when <code>some_opcode</code> is true, but it is impossible to do in most HDLs because they are centered around instantiation, not execution. For synthesizable code, none of the most popular HDLs like Verilog, VHDL, CHISEL, pyRTL allow the instantiation of a module in a conditional. Some like Verilog have <code>functions</code> but those are inlined. When called, they can be seen as a macro preprocessor that inserts the function statements.</p> <p>HDLs force the designer to spicy the instantiation unconditionally, and then the <code>if</code> selects between the instantiations. Even though HDLs look like they execute instructions, they do not; it is all about cell instantiation and how to connect those instances. The <code>if</code> is not a branch instruction, it is a multiplexor instantiation. The <code>do_division</code> is not a function call; it instantiates a circuit or module.</p> <p>Artifact</p> <p>Function calls inside control flow statements are either not allowed or forced to be inlined.</p> <p>Artifact</p> <p>HDLs look like instruction execution, but they are about circuit instantiation.</p>"},{"location":"pyrope/00-hwdesign/#pipelining","title":"Pipelining","text":"<p>Pipelining is adding registers in combinational circuits to create smaller critical paths and hence higher frequency designs. It is essential to hardware design, and there is not much related to it in software design flows.</p> <p>To illustrate the problem, imagine a pipelined multiplier function (mult) that takes one cycle to produce the results, and the programmer has an assertion checking that it was a multiply. The result <code>c</code> is not the current cycle <code>a*b</code> but the last cycle <code>a</code> multiplied by the last cycle <code>b</code>. This is not what would be expected in a software API.</p> Problematic codeHLS possible solution <pre><code>var c = mul(a,b)\nassert c == a * b // assert fails!!\n</code></pre> <pre><code>var c = mul(a,b)\nassert c == a#[-1] * b#[-1] // read last cycle #[-1] a and b\n</code></pre> <p>If actors execution resembles concurrent module instantiation execution, async/await resembles pipelining. In async/await, the results of a function are not available at the function return. In HDLs, there is no await, and the results from previous cycles are output by the module instance.</p> <p>Pipelining is not restricted to just function or module instantiations. A the module itself can have a set of registers, and different variables/wires have the results from different cycles. It is up to the designer to manage it, and it is one of the primary complexity sources of hardware design and verification.</p> <p>Artifact</p> <p>Different variables or wires can have results from other cycles due to pipelining.</p>"},{"location":"pyrope/00-hwdesign/#simulation-vs-synthesis","title":"Simulation vs synthesis","text":"<p>Hardware designs tend to have extensive verification infrastructures. The reason is that once the chip is fabricated, it can not be easily be patched like software. It may need to trash millions of dollars and take months to get the next chip, even for just a line of code patch. This is not different from software; it is just that the cost of a bug could be potentially much higher.</p> <p>The difference from software is that the \"simulation\" results used for verification may be different from the hardware results generated during \"synthesis\".</p> <p>A mismatch between synthesis and simulation could happen due to script directives in the synthesis scripts, or due to use language features that only affect simulation.</p> Problematic codeHLS possible solution <pre><code>initial begin // initial code may not be used in synthesis\n   c = 3;\nend\n\n#3 d = 4; // delay simulation update, not synthesis update\ncase (x)  // synthesis: full_case, parallel_case\n...\n</code></pre> <pre><code>// Do allow simulation code to have side-effects on synthesis code\n// directives affect simulation AND synthesis (not one or the other)\n\nunique case(x) // do not use synthesis only directives\n...\n</code></pre> <p>Artifact</p> <p>Simulation and synthesis results can have different functionality</p>"},{"location":"pyrope/00-hwdesign/#reset","title":"Reset","text":"<p>Programmers are used to initializing their variables. Since the modules are called every cycle, the typical software syntax for initialization does not work. To make it worse, some languages like Verilog (and others) have two initializations: reset and simulation setup.</p> <p>Some differences between reset and software initialization:</p> <ul> <li>Reset can take many cycles</li> <li>Reset can be called many times</li> <li>Reset vs. variable initialization in some languages (Verilog)</li> </ul> Problematic codeHLS possible solution <pre><code>initial begin\n   d = 1;\nend\nalways @(posedge clk) begin\n   if (reset) begin\n     d = 2;\n...\n</code></pre> <pre><code>// Just use the reset flop values to initialize contents\n\nalways @(posedge clk) begin\n   if (reset) begin\n     d = 2;\n...\n</code></pre> <p>Artifact</p> <p>Reset is different from variable initialization</p>"},{"location":"pyrope/00-hwdesign/#non-blocking-assignments","title":"Non-blocking assignments","text":"<p>Many HDLs have what hardware designers call \"non-blocking assignments\". The idea is that in hardware, when assigning a variable, the designer could think about the \"result at the end of this cycle\" rather than \"update this mutable variable\".</p> <p>Technically, a nonblocking assignment is an assignment to a variable, but the variable will be updated only at the end of the cycle. To illustrate the concept, imagine a counter. The counter can be updated with a non-blocking assignment and following statements could still read the value before the scheduled update.</p> Problematic codeHLS possible solution <pre><code>counter &lt;- counter + 1  // non-blocking assignment\ntmp     &lt;- counter + 2  // non-blocking assignment\nassert tmp == (counter+1) // this may FAIL!\n</code></pre> <pre><code>// Do not use non-blocking\ncounter = counter + 1  // blocking assignment\ntmp     = counter + 2  // blocking assignment\nassert tmp == (counter+1) // this never fails\n</code></pre> <p>Artifact</p> <p>Some HDLs support non-blocking assignments which are not found in software.</p>"},{"location":"pyrope/00-hwdesign/#invalid-code","title":"Invalid code","text":"<p>HDLs can generate invalid code that can not be fabricated, or it is strongly recommended not to be fabricated. Examples are:</p> <ul> <li> <p>Combinational loops. Creating a loop with combinational logic is generally   considered a bug (only a few circuits could accept this). If the   combinational loop is inside a mux, it can be difficult to catch during   verification unless a good toggle coverage results.</p> </li> <li> <p>Implicit latches. Some HDLs like Verilog can generate code with implicit   latches. Since the module is executed each time, variables with a missing   initialization can remember results from the last cycles generating implicit   latches. Most ASIC tools do not accept this, and it is considered a bug.</p> </li> <li> <p>Bogus flow. Any compile (software or hardware) can have bugs, but because   hardware compilers tend to have a smaller user base; they have more bugs than   typical software compilers. Since this cost of fixing a bug is also higher,   the solution is to have an additional verification or logical equivalence   test.</p> </li> </ul> <p>If the compile generates an executable in software flows, it is considered a valid executable unless invalid assembly directives are used. In most HDLs, this is not the case due to constructs like combinational loops.</p> <p>Artifact</p> <p>HDLs can generate invalid synthesis and/or simulation code.</p>"},{"location":"pyrope/00-hwdesign/#multi-value-logic","title":"Multi value logic","text":"<p>Software designers are used to binary numbers with 0s and 1s. In many HDLs, there are more than two possible states for each bit. In languages like Verilog, there are four states: <code>0</code>, <code>1</code>, <code>?</code> or <code>z</code>. Where <code>?</code> is a \"quantum\" like state indicating that it is both zero and one simultaneously, and the <code>z</code> indicates that it is in high impedance, which means that nobody is writing the value. Some languages like VHDL have even more states.</p> <p>The challenge is that when running code, the result may be unexpected. There are many discussions on addressing what the community calls \"x-propagation.\" There is no agreement on the best solution. The reason for not removing <code>?</code> is that some large structures will not be initialized because of overheads. Although widely considered dangerous, some engineers like the <code>?</code> to allow more freedom to the synthesis tools<sup>8</sup></p> <p>There are three leading solutions categories:</p> <ul> <li>Allow <code>?</code> and run many simulations with different x-propagation rules.</li> <li>Allow <code>?</code> and randomly pick 0/1 for each <code>?</code> bit at simulation time.</li> <li>Do not allow <code>?</code>.</li> </ul> Problematic codeHLS possible solution <pre><code>x = 0b?   // a ? state\nif x {\n   puts \"x is never true\"\n}\nreg signed [3:0] a = -1;\n$display(\"%b\\n\", a[5:1]); // displays xx111\n</code></pre> <pre><code>// there is no agreement on the community, but possible solutions:\nx = 0b? // (1): compile error\nif x {  // (2): randomly pick 1 or 0\n}\nreg signed [3:0] a = -1;\n$display(\"%b\\n\", a[5:1]); // displays 11111 (sign extend)\n</code></pre> <p>Artifact</p> <p>HDLs can operate over non-binary logic</p>"},{"location":"pyrope/00-hwdesign/#simpler-hdl-constructs","title":"Simpler HDL constructs","text":"<p>Not everything is more challenging in HDLs when compared with software programming languages. These are some differences that can make the HDLs simpler:</p>"},{"location":"pyrope/00-hwdesign/#unlimited-precision","title":"Unlimited precision","text":"<p>High-performance software must adjust to the hardware, and as such, there are several integer types (int, short, long).  The result is that the programmer tends to be careful with overflows and type conversion. This is not a problem in hardware. If a 113 bits adder is needed, it can be synthesized. If only a 7 bits adder is required; the synthesis can create the smaller adder too.</p> <p>Some HLS may have different integer sizes, but it is either a \"strange\" design the decision or just as a type check so that no unnecessary hardware is generated.</p> <p>Overflow is a source of many subtle bugs. For example, experience programmers write <code>a + (b-a))/2</code> not the expected <code>(a+b)/2</code> because of integer overflow semantics. HDLs can handle this like unlimited precision scripting languages without the overhead.</p>"},{"location":"pyrope/00-hwdesign/#no-pointers","title":"No pointers","text":"<p>Memory and pointer management is big issue in most languages. Either garbage collection, manual, or alternative approaches.  Since there is no global memory, there is no memory to manage. Maybe even more important, there is no need for pointers. This avoids another set of problems like null dereferencing.</p>"},{"location":"pyrope/00-hwdesign/#no-destructors","title":"No destructors","text":"<p>Since there is no global memory, there is no need to have garbage collection or the associated object destruction. If a \"hardware resource\" is utilized, it can not be recycled. As a result, the destructor may not make sense in hardware.</p>"},{"location":"pyrope/00-hwdesign/#pass-by-value","title":"Pass by value","text":"<p>Most software languages support passing function arguments either by value or reference. This is done to avoid copying the object that may reside in memory. Again, HLS has no memory. Therefore it is not as problematic.</p> <p>Most HDLs only support passing by value. This is not a drawback but avoid another source of bugs without the cost overhead that it will represent in a Vonn Neumann machine.</p>"},{"location":"pyrope/00-hwdesign/#no-recursion","title":"No recursion","text":"<p>Most HDLs support recursion at compile-time but not at runtime. The reason is that there is no \"stack memory\". If the depth is bound, it can support run-time recursion, but the potentially sizeable combinational path would be \"strange\". Only manageable with retiming. As a result, most HDLs do not support runtime recursion.</p> <ol> <li> <p>Multi-threaded CPUs are just an array of Von Neumann machines.\u00a0\u21a9</p> </li> <li> <p>In this document, we call any logic gate, flop, or memory array.\u00a0\u21a9</p> </li> <li> <p>There are other constraints like power, but the same idea/problem could   be said for software design.\u00a0\u21a9</p> </li> <li> <p>Unbalance pipelines have higher overheads in power/area.\u00a0\u21a9</p> </li> <li> <p>Verilog modules could be seen as functions in a software language that   can be instantiated in one or more places. The instantiation point sets a   hierarchy of modules.\u00a0\u21a9</p> </li> <li> <p>Instantiation is the process of deciding which gates are fabricated or   mapped in a given hardware design. In an ASIC, it is the process of selecting   a set of gates that will be manufactured.\u00a0\u21a9</p> </li> <li> <p>Transistors can not be added at runtime.\u00a0\u21a9</p> </li> <li> <p>This is very controversial, and many companies coding styles do not allow   the use of <code>?</code> to improve synthesis results.\u00a0\u21a9</p> </li> </ol>"},{"location":"pyrope/01-introduction/","title":"Introduction","text":"<p>Warning</p> <p>This document explains the future Pyrope, some features are still not implemented. They are documented to guide the designers.</p> <p>Pyrope is a modern hardware description language, with these focus points:</p> <ul> <li>Fast parallel and incremental elaboration </li> <li>Modern and concise language</li> <li>Avoiding hardware specific artifacts<ul> <li>Allows optional hierarchical calls</li> <li>Supports instantiation optimization with typical software syntax</li> <li>Supports pipelining constructs</li> <li>No mismatch simulation vs synthesis</li> <li>Single reset mechanism</li> <li>Avoid non-blocking assignments</li> <li>Checks on invalid code</li> <li>Random on multi value logic when doing control flow</li> </ul> </li> <li>Zero cost abstraction</li> <li>Help hardware verification:<ul> <li>Powerful type system</li> <li>Hot-Reload support, powerful assertions</li> <li>Allows Pyrope 2 Verilog, edit Verilog, Verilog 2 Pyrope, edit Pyrope...</li> <li>Static checks as long as they not produce false positives</li> </ul> </li> </ul>"},{"location":"pyrope/01-introduction/#hello-world","title":"Hello world","text":"<p>Create a directory for the project: <pre><code>$ mkdir hello\n$ cd hello\n$ mkdir src\n</code></pre></p> <p>Populate the Pyrope code</p> <p><code>src/hello.prp</code> <pre><code>test \"my first test\" {\n  puts \"hello world\"\n}\n</code></pre></p> <p>Run <pre><code>$prp test\n</code></pre></p> <p>All the pyrope files reside in the <code>src</code> directory. The <code>prp</code> builder calls LiveHD to elaborate the pyrope files and run all the tests.</p>"},{"location":"pyrope/01-introduction/#trivial-gcd","title":"Trivial GCD","text":"<p>Populate the Pyrope code</p> PyropeCHISEL <p>src/gcd.prp: <pre><code>var gcd = proc (a:uint,b:uint)-&gt;(reg x:uint) {\n  x = a\n  reg y = b\n\n  while y!=0 #&gt; {\n    if x &gt; y { \n      x -= y \n    }else{ \n      y -= x \n    }\n  }\n}\n\nfor a in 1..=100 {\n  for b in 1..=100 {\n    test \"check.gcd({},{})\",a,b {\n      let z =#[..] gcd(a,b)\n\n      waitfor z?\n\n      assert z == __my_cpp_gcd(v1=a, v2=b)\n    }\n  }\n}\n</code></pre></p> <p>src/my_cpp_gcd.cpp <pre><code>void my_gcd_cpp(const Lbundle &amp;inp, Lbundle &amp;out) {\n  assert(inp.has_const(\"v1\") &amp;&amp; inp.has_const(\"v2\"));\n\n  auto x = inp.get_const(\"v1\");\n  auto y = inp.get_const(\"v2\");\n\n  while (y &gt; 0) {\n    if (x &gt; y) {\n      x -= y\n    }else{\n      y -= x\n    }\n  }\n\n  out.add_const(x);\n}\n</code></pre></p> <pre><code>import Chisel._\nimport firrtl_interpreter.InterpretiveTester\nimport org.scalatest.{Matchers, FlatSpec}\n\nobject GCDCalculator {\n  def computeGcd(a: Int, b: Int): (Int, Int) = {\n    var x = a\n    var y = b\n    while(y &gt; 0 ) {\n      if (x &gt; y) {\n        x -= y\n      }\n      else {\n        y -= x\n      }\n    }\n    x\n  }\n}\n\nclass GCD extends Module {\n  val io = new Bundle {\n    val a  = UInt(INPUT,  16)\n    val b  = UInt(INPUT,  16)\n    val e  = Bool(INPUT)\n    val z  = UInt(OUTPUT, 16)\n    val v  = Bool(OUTPUT)\n  }\n  val x  = Reg(UInt())\n  val y  = Reg(UInt())\n  when   (x &gt; y) { x := x - y }\n  unless (x &gt; y) { y := y - x }\n  when (io.e) { x := io.a; y := io.b }\n  io.z := x\n  io.v := y === UInt(0)\n}\n\nclass InterpreterUsageSpec extends FlatSpec with Matchers {\n\n  \"GCD\" should \"return correct values for a range of inputs\" in {\n    val s = Driver.emit(() =&gt; new GCD)\n\n    val tester = new InterpretiveTester(s)\n\n    for {\n      i &lt;- 1 to 100\n      j &lt;- 1 to 100\n    } {\n      tester.poke(\"io_a\", i)\n      tester.poke(\"io_b\", j)\n      tester.poke(\"io_e\", 1)\n      tester.step()\n      tester.poke(\"io_e\", 0)\n\n      while (tester.peek(\"io_v\") != BigInt(1)) {\n        tester.step()\n      }\n      tester.expect(\"io_z\", BigInt(GCDCalculator.computeGcd(i, j)._1))\n    }\n    tester.report()\n  }\n}\n</code></pre> <p>Run <pre><code>$prp test check.gcd\n</code></pre></p> <p>The <code>gcd.prp</code> includes the top-level module (<code>gcd</code>) and the unit test. </p> <ul> <li> <p>Some Pyrope features not common in other HDLs (CHISEL):</p> <ul> <li> <p>Pyrope is not a DSL. Most modern HDLs like CHISEL, pyMTL, pyRTL, C\u03bbaSH   are DSL cases. In these cases, there is a host language (SCALA, or Python,   or Haskell) that must be executed. The result of the execution is the hardware   description which can be Verilog or some internal IR like FIRRTL in CHISEL.    The advantage of the DSL is that it can leverage the existing language to   have a nice hardware generator. The disadvantage is that there are 2 languages   at once, the DSL and the host language, and that it is difficult to do   incremental because the generated executable from the host language must be   executed to generate the design.</p> </li> <li> <p>Global type inference. In the gcd example, the input/outputs are   inferred.</p> </li> <li> <p>Synthesizable object system with runtime polymorphism</p> </li> <li> <p>Immutable objects</p> </li> <li> <p>Language support for several hardware constructs</p> </li> </ul> </li> <li> <p>Some Pyrope features not common in other languages</p> <ul> <li> <p>No object references, only pass by value</p> </li> <li> <p>Pipelining support</p> </li> </ul> </li> </ul>"},{"location":"pyrope/02-basics/","title":"Basic syntax","text":""},{"location":"pyrope/02-basics/#comments","title":"Comments","text":"<p>Comments begin with <code>//</code>, there are no multi-line comments</p> <pre><code>// comment\na = 3 // another comment\n</code></pre>"},{"location":"pyrope/02-basics/#constants","title":"Constants","text":""},{"location":"pyrope/02-basics/#integers","title":"Integers","text":"<p>Pyrope has unlimited precision signed integers. Any literal starting with a digit is a likely integer constant.</p> <pre><code>0xF_a_0 // 4000 in hexa. Underscores have no meaning\n0b1100  // 12 in binary\n0sb1110 // -2 in binary (sb signed binary)\n33      // 33 in decimal\n0o111   // 73 in octal\n0111    // 111 in decimal (some languages use octal here)\n</code></pre> <p>Since powers of two are very common, Pyrope decimal integers can use the <code>K</code>, <code>M</code>, <code>G</code>, and <code>T</code> modifiers.</p> <pre><code>assert 1K == 1024\nassert 1M == 1024*1024\nassert 1G == 1024*1024*1024\nassert 1T == 1024*1024*1024*1024\n</code></pre> <p>Several hardware languages support unknown bits (<code>?</code>) or high-impedance (<code>z</code>). Pyrope aims at being compatible with synthesizable Verilog, as such <code>?</code> is also supported in the binary encoding.</p> <pre><code>0b?             // 0 or 1 in decimal\n0sb?            // 0 or -1 in decimal\n0b?0            // 0 or 2 in decimal\n0sb0?0          // 0 or 2 in decimal\n</code></pre> <p>The Verilog high impedance <code>z</code> is not supported. A <code>bus</code> construct must be used instead.</p> <p>Like in many HDLs, Pyrope has unknowns <code>?</code>. The x-propagation is a source of complexity in most hardware models. Pyrope has <code>x</code> or <code>?</code> to be compatible with Verilog existing designs. This means that inside the Pyrope compiler, the constant operations with unknowns are compatible with Verilog semantics. When the simulation is performed, the expectation is to randomly generate a 0 or 1 for each unknown (<code>?</code>) bit.</p> <p>The advice is not to use <code>?</code> besides <code>match</code> statement pattern matching. It is less error prone to use the default value (zero or empty string), but sometimes it is easier to use <code>nil</code> when converting Verilog code to Pyrope code. The <code>nil</code> means that the numeric value is invalid. If any operation is performed with <code>nil</code>, the result is an assertion failure. The only thing allowed to do with nil is to copy it. While the <code>nil</code> behaves like an invalid value, the <code>0sb?</code> behaves like an unknown value that still can be used in arithmetic operations. E.g: <code>0sb? | 1</code> is <code>1</code> but <code>nil | 1</code> is an assertion error.</p> <p>Notice that <code>nil</code> is a state in the integer basic type, it is not a new type by itself, it does not represent an invalid pointer, but rather an invalid integer. Also important is that the compiler will guarantee that all the <code>nil</code> are eliminated at compile time or a compile error is generated.</p>"},{"location":"pyrope/02-basics/#strings","title":"Strings","text":"<p>Pyrope accepts single line strings with a single quote (<code>'</code>) or double quote (<code>\"</code>).  Single quote does not have escape character, double quote supports escape sequences.</p> <pre><code>a = \"hello \\n newline\"\nb = 'simpler here'\n</code></pre> <ul> <li><code>\\n</code>: newline</li> <li><code>\\\\</code>: backslash</li> <li><code>\\\"</code>: double quote</li> <li><code>`</code>: backtick quote</li> <li><code>\\xNN</code>: hexadecimal 8 bit character (2 digits)</li> <li><code>\\uNNNN</code>: hexadecimal 16-bit Unicode character UTF-8 encoded (4 digits)</li> </ul> <p>Pyrope allows string interpolation only when double quote is used. Nevertheless, when string interpolation is used, the formatting guidelines are not allowed. The style is like C++ fmt::format which allows an identifier. When the identifier is provided the string is processed accordingly.</p> <pre><code>let num       = 2\nlet color     = \"blue\"\nlet extension = \"s\"\n\nlet txt1 = \"I have {num} {color} potato{extension}\"\nlet txt2 = format('I have {:d} {} potato{} ', num, color, extension)\ncassert txt1 == txt2 == \"I have 2 blue potatos\"\n\nlet txt3 = 'I have {num}'         // single quote does not do interpolation\ncassert txt3 == \"I have \\{num\\}\"  // \\{ escapes the interpolation\n</code></pre> <p>Integers and strings can be converted back and forth:</p> <pre><code>var a:string = \"127\"\nvar b:int    = a     // same as var b = int(a)\nvar c:string = b     // same as var c = string(b)\nassert a == c\nassert b == 0x7F\nassert a == b        // compile error, 'a' and 'b' have different types\n</code></pre>"},{"location":"pyrope/02-basics/#newlines-and-spaces","title":"Newlines and spaces","text":"<p>Spaces do not have meaning but new lines do. Several programming languages like Python use indentation level (spaces) to know the parsing meaning of expressions. In Pyrope, spaces do not have meaning, and newlines combined with the first token after newline is enough to decide the end of statement.</p> <p>By looking at the first character after a new line, it is possible to know if the rest of the line belongs to the previous statement or it is a new statement.</p> <p>If the line starts with an alphanumeric (<code>[a-z0-9]</code> that excludes operators like <code>or</code>, <code>and</code>) value or an open parenthesis (<code>(</code>), the rest of the line belongs to a new statement.</p> <pre><code>var (a,b,c,d) = _\na = 1\n  + 3           // 1st stmt\n(b,c) = (1,3)   // 2nd stmt\ncassert a == 4 and b == 1 and c == 3\n\nd = 1 +         // OK, but not formatted to style\n    3\n</code></pre> <p>This functionality allows parallelizing the parsing and elaboration in Pyrope. More important, it makes the code more readable, by looking at the beginning of the line, it is possible to know if it is a new statement or a continuation of the last one. It also helps to standardize the code format by allowing only one style.</p>"},{"location":"pyrope/02-basics/#identifiers","title":"Identifiers","text":"<p>An identifier is any non-reserved keyword that starts with an underscore or an alphabetic character. Since Pyrope is designer to support any synthesizable Verilog automatic translation, any sequence of characters between backticks (`) can form a valid identifier. The identifier uses the same escape sequence as strings. </p> <pre><code>`foo is . strange!\\nidentifier` = 4\n`for` = 3\ncassert `for`+1 == `foo is . strange!\\nidentifier`\n</code></pre> <p>Using the backtick, Pyrope can use any string as an identifier, even reserved keywords. Identifiers are case sensitive like Verilog, but the compiler issues errors for non ` escaped identifiers that do not follow these conditions in order:</p> <ul> <li>Identifiers with a single character followed by a number can be upper or lower case.</li> <li>An all upper case variable must be a compile time constant <code>comptime</code>.</li> <li>Types should either: (1) start the first character uppercase and everything   else lower case; (2) be all lower case and finish with <code>_t</code>.</li> <li>All the other identifiers that start with an alpha character <code>[a-z]</code> are   always lower case.</li> </ul>"},{"location":"pyrope/02-basics/#semicolons","title":"Semicolons","text":"<p>Semicolons are not needed to separate statements. In Pyrope, a semicolon (<code>;</code>) has the same meaning as a newline. Sometimes it is possible to add semicolons to separate statements. Since newlines affect the meaning of the program, a semicolon can do too.</p> <pre><code>a = 1 ; b = 2\n</code></pre>"},{"location":"pyrope/02-basics/#printing-and-debugging","title":"Printing and debugging","text":"<p>Printing messages is useful for debugging. <code>puts</code> prints a message and the string is formatted using the c++20 fmt format. There is an implicit newline printed. The same without a newline can be achieved with print.</p> <pre><code>a = 1\nputs \"Hello a is {}\", a\n</code></pre> <p>Pyrope does string interpolation, and it has attributes to access line of code and file name. Since tracing or debugging variables is quite common, the <code>dbg</code> statement behaves like puts and also prints the line of code and file name for easier tracing.</p> <pre><code>a = 1                                            // file foo line 3\nputs \"{}:{} a:{} tracing a\", a.[file], a.[loc], a\nputs \"{a.[file]}:{a.loc} a:{a} tracing a\"        // Same as previous\ndbg a, \"tracing a\"\n</code></pre> <p>The previous statements print \"foo:3 a:1 tracing a\" in the 3 cases. The line of code corresponds to the latest update of variable, not the <code>dbg</code> statement.</p> <p>Since many modules can print at the same cycle, it is possible to put a relative priority between puts (<code>priority</code>). If no relative priority is provided, a default 0 priority is provided. Messages are kept to the end of the cycle, and then printed in alphabetical order for a given priority. This is done to be deterministic. Higher priority (higher value) are printed after lower priority. Messages generated by assertions also get serialized like puts statements but have the highest priority.</p> <p>To avoid breaking down different <code>puts</code> inside the same method. All the <code>puts</code> in a given cycle are shown together.</p> <p>This example will print \"hello world\" even though there are 2 puts/prints in different files.</p> <pre><code>// src/file1.prp\nputs(priority=2, \" world\")\n\n// src/file2.prp\nprint(priority=1, \"hello\")\n</code></pre> <p>The available puts/print arguments: * <code>priority</code>: relative order to print in a given cycle. * <code>file</code>: file to send the message. E.g: <code>stdout</code>, <code>stderr</code>, <code>my_large.log</code>,...</p> <p>A related command to the puts is the <code>format</code> it behaves like <code>print</code> but returns a string.</p> <p><code>puts/print</code> are a bit special. In most languages, IO operations like <code>puts</code> are considered to have side-effects. In Pyrope, the <code>puts</code> can not modify the behavior of the synthesized code and it is considered a non-side-effect lambda call. This allows to have <code>puts</code> calls in <code>functions</code>.</p>"},{"location":"pyrope/02-basics/#functions-and-procedures","title":"Functions and procedures","text":"<p>Pyrope only supports anonymous lambdas. A lambda can be assigned to a variable, and it can be called as most programmers expect. Lambda section has more details on the allowed syntax.</p> <pre><code>var f = fun(a,b) { a + b }\n</code></pre> <p>Pyrope naming for consistency:</p> <ul> <li> <p><code>lambda</code> is any sequence of statements grouped in a code block that can be   assigned to a variable and called to execute later.</p> </li> <li> <p><code>function</code> is a lambda with only combination statements without non-Pyrope   calls.</p> </li> <li> <p><code>procedure</code> is a lambda that can have combination like function but also   non-combinational (register/memories). Procedures are a superset of functions.</p> </li> <li> <p><code>method</code> is a lambda (<code>function</code> or <code>procedure</code>) that updates another   variable.  The first argument is an explicit <code>self</code>.</p> </li> <li> <p><code>module</code> is a lambda that has a physical instance. Lambdas are either inlined   or modules.</p> </li> </ul> <p>lambda are not only restricted to Pyrope code. It is possible to interface with non-Pyrope (C++) code, but the calls should respect the same <code>procedure</code>/<code>function</code> definition. A C++ <code>function</code> can not update the C++ internal state or generate output because the simulation/compiler is allowed to call it multiple times. This is not the case for C++ <code>procedure</code>.</p>"},{"location":"pyrope/02-basics/#evaluation-order","title":"Evaluation order","text":"<p>Statements are evaluated one after another in program order. The main source of conflicts come from expressions.</p> <p>The expression evaluation order is important if the elements in the expression can have side effects. Pyrope constrains the expressions so that no matter the evaluation order, the synthesis result is the same. </p> <p>Languages like C++11 do not have a defined order of evaluation for all types of expressions. Calling <code>call1() + call2()</code> is not defined. Either <code>call1()</code> first or <code>call2()</code> first.</p> <p>In many languages, the evaluation order is defined for logical expressions. This is typically called the short-circuit evaluation. Some languages like Pascal, Rust, Kotlin have different <code>and/or</code> to express conditional evaluation. In Pascal, there is an <code>and/or</code> and <code>and_then/or_else</code> (conditional). In Rust <code>&amp;/|</code> and <code>&amp;&amp;/||</code> (conditional). In Kotlin <code>&amp;&amp;/||</code> and <code>and/or</code> (conditional). Pyrope uses has the <code>and/or</code> without short-circuit, and the <code>and_then/or_else</code> with explicit short-circuit.</p> <p>The programmer can explicitly set an evaluation order by using short-circuit expressions like <code>and_then</code>, <code>or_else</code>, or control expressions (<code>if/else</code>, <code>match</code>, <code>for</code>). An expression can have many <code>function</code> calls because those have no side-effects, and hence the evaluation order is not important.</p> <p>A <code>procedure</code> is an lambda that can update state internally. It can be through a C++ API call, or some synthesizable state. As such, only one <code>procedure</code> call can exist per expression.</p> <pre><code>var a = fcall() + 1               // OK\nvar x = pcall() + a               // OK, proc combined with variable read\nvar b = fcall(a) + 10 + pcall(a)  // OK\nvar d = t.pcall() + pcall2(b)     // compile error, multiple procedure calls\nvar y = t.pcall() + t.pcall()     // compile error, multiple procedure calls\n</code></pre> <p>Expressions also can have a code blocks (<code>{  }</code>) as long as there are no side-effects. In a way, expression code blocks can be seen as a type of <code>functions</code> that are called immedialy after definition.</p> <pre><code>var a = {var d=3 ; last d+1} + 100 // OK\nassert a == (3+1+100)\nassert a == {3+1+100}  // same, expression evaluated as 104 and returned\n</code></pre> <p>For most expressions, Pyrope is more restrictive than other languages because it wants to be a fully defined deterministic independent of implementation. To handle logging/messaging in <code>function</code> calls, Pyrope treats <code>puts</code> as a special instruction. Pyrope runtime delays the puts output until the end of the cycle. Section (Printing)[02-basics.md#printing] has more details.</p> <p>To illustrate the evaluation order, it is useful to see a Verilog example. The following Verilog sequence evaluates differently in VCS and Icarus Verilog. Pyrope treats <code>puts</code> and assertion messages in a special way. The reason why some methods may be called is dependent on the optimization (in this case, <code>testing(1)</code> got optimized away by vcs).</p> <pre><code>module test();\n\nfunction testing(input [0:3] a);\n  begin\n    $display(\"test called with %d\",a);\n    testing=1;\n  end\nendfunction\n\ninitial begin\n  if (0 &amp;&amp; testing(1)) begin\n    $display(\"test1\");\n  end\n\n  if (1 &amp;&amp; testing(2)) begin\n    $display(\"test2\");\n  end\n\n  if (0 || testing(3)) begin\n    $display(\"test3\");\n  end\n\n  if (1 || testing(4)) begin\n    $display(\"test4\");\n  end\nend\n</code></pre> Icarus outputVCS outputC++/short-circuit output <pre><code>test called with  1\ntest called with  2\ntest2\ntest called with  3\ntest3\ntest called with  4\ntest4\n</code></pre> <pre><code>test called with  2\ntest2\ntest called with  3\ntest3\ntest called with  4\ntest4\n</code></pre> <pre><code>test called with 2\ntest2\ntest called with 3\ntest3\ntest4\n</code></pre> <p>If an order is needed and a function call can have <code>debug</code> side-effects or synthesis side-effects, the statement must be broken down into several statements, or the <code>and_then</code> and <code>or_else</code> operations must be used.</p> Incorrect code with side-effectsAlternative 1Alternative 2 <pre><code>var r1 = pcall1() or  pcall2()  // compile error, non-deterministic\n\n\nvar r2 = pcall1() and pcall2()  // compile error, non-deterministic\n\n\nvar r3 = pcall1() +   pcall2()  // compile error\n// compile error only if pcall1/pcall2 can have side effects\n</code></pre> <pre><code>var r1 = fcall1()\nr1  = fcall2() unless r1\n\nvar r2 = fcall1()\nr2  = fcall2() when r2\n\nvar r3 = fcall1()\nr3 += fcall2()\n</code></pre> <pre><code>var r1 = fcall1() or_else fcall2()\n\n\nvar r2 = fcall1() and_then fcall2()\n\n\nvar r3 = fcall1()\nr3 += fcall2()\n</code></pre>"},{"location":"pyrope/02-basics/#basic-gates","title":"Basic gates","text":"<p>Pyrope allows a low level or structural direct basic gate instantiation. There are some basic gates to which to which the compiler translates Pyrope code to. These basic gates are also directly accesible:</p> <ul> <li><code>__sum</code> for addition and substraction gate.</li> <li><code>__mult</code> for multiplication gate.</li> <li><code>__div</code> for divisions gate.</li> <li><code>__and</code> for bitwise and gate</li> <li><code>__or</code> for bitwise or gate</li> <li><code>__xor</code> for bitwise xor gate</li> <li><code>__ror</code> for bitwise reduce-or gate</li> <li><code>__not</code> for bitwise not gate</li> <li><code>__get_mask</code> for extrating bits using a mask gate</li> <li><code>__set_mask</code> for replacing bits using a mask gate</li> <li><code>__sext</code> for sign-extension gate</li> <li><code>__lt</code> for less-than comparison gate</li> <li><code>__ge</code> for greater-equal comparison gate</li> <li><code>__eq</code> for equal comparison gate</li> <li><code>__shl</code> for shift left logical gate</li> <li><code>__sra</code> for shift right arithmetic gate</li> <li><code>__lut</code> for Look-Up-Table gate</li> <li><code>__mux</code> for a priority multiplexer</li> <li><code>__hotmux</code> for a one-hot excoded multiplexer</li> <li><code>__memory</code> for a memory gate</li> <li><code>__flop</code> for a flop gate</li> <li><code>__latch</code> for a latch gate</li> </ul> <p>Each of the basic gates operate always over signed integers like Pyrope, but their semantics vary. A more detailed explanation is available at LiveHD cell type section.</p>"},{"location":"pyrope/02-basics/#initialization","title":"Initialization","text":"<p>Each variable declaration (<code>var</code> or <code>let</code>) must have an assigned value. The type default value is <code>_</code> (<code>0</code> integer, <code>\"\"</code> string, <code>false</code> boolean, <code>nil</code> otherwise)</p> <pre><code>a  = 3        // compile error, no previous let or var\n\nvar b = 3\nb  = 5        // OK\nb += 1        // OK\n\nlet cu3 = if runtime { 3 }else{ 5 }\n\nlet d = \"hello\"  // OK\nd = \"bar\"        // compile error, 'd' is immutable\nvar d = \"bar\"    // compile error, 'd' already declared\n\nvar e = _        // OK, no type or default value, just scope declaration\ne:u32 = 33       // OK\n\nvar Foo = 33     // compiler error, 'let Foo = 33'\nFoo  = 33        // compiler error, `Foo` already declared as immutable\n</code></pre> <p>When the variable is a tuple or a range style, the default initialization is <code>nil</code>. <code>0sb?</code> can not be applied to ranges or tuples value because it is restricted for integers. <code>nil</code> should be used in those cases.</p> <pre><code>var tup = nil\n\nif cond::[comptime] {\n  tup = (a=1,b=2)\n}else{\n  tup = (a=1,b:u4=3,c=3)\n}\n\ncassert tup.a == 1\ncassert cond implies tup.b==2\ncassert !cond implies tup.b==3\n</code></pre> <p>Variables with first character upper case are <code>comptime</code>. This means that the contents must be known/fix at compilation time.</p> <pre><code>var A_xxx = something             // comptime\nvar A_yyy::[comptime] = something // also comptime, redundant but legal\n</code></pre>"},{"location":"pyrope/03-bundle/","title":"Tuples","text":"<p>Tuples are a basic construct in Pyrope. Tuples are defined as an \"ordered\" sequence fields that can be named. Arrays/memories are a subcategory of tuples by requiring all the entries to have the same type. Internally, there is not a difference between tuples and arrays, but it is possible to check that all the fields are the same (hence array) by using brackets instead of parenthesis.</p> <pre><code>var b = (f1=3,f2=4) // b is named and ordered\nvar c = (1,d=4)     // c is ordered and unnamed (some entries are not named)\n\nvar d = (1,2,3,4)     // array or tuple\nassert d == [1,2,3,4] // the [] also check that all the fields have same type\n\nassert (true,1) != [true,1]  // compile error, true is not the same type as 1\n</code></pre> <p>To access fields in a tuple we use the dot <code>.</code> or <code>[]</code> <pre><code>var a = (\n  ,r1 = (b=1,c=2)\n  ,r2 = (3,4)\n)\n// tuple position is from left to right\ncassert a.r1 == (1,2) and a.r2 == (3,4)\ncassert a.0  == (1,2) and a[1] == (3,4)\n\n// different ways to access the same field\ncassert a.r1.c    == 2\ncassert a['r1'].c == 2\ncassert a.r1.1    == 2\ncassert a.r1[1]   == 2\ncassert a[0][1]   == 2\ncassert a[0]['c'] == 2\ncassert a['r1.c'] == 2\ncassert a['r1.1'] == 2\ncassert a['0.c']  == 2\ncassert a['0.1']  == 2\ncassert a.0.c     == 2\ncassert a.0.1     == 2\n</code></pre></p> <p>The only main difference between <code>a.0</code> (dot) and <code>a[0]</code> (select) access is that dot access guarantees to be compile time index, while the select can have compile time or run-time index.</p> <p>There is introspection to check for an existing field with the <code>has</code> and <code>!has</code> operators.</p> <pre><code>var a = (foo = 3)\ncassert a has 'foo'\ncassert !(a has 'bar')\ncassert a !has 'bar' // \"has no\" is the opposite of \"has\"\ncassert a has 0\ncassert a !has 1\ncassert a !has 1\n</code></pre> <p>Tuple named fields can have a default type and or contents:</p> <pre><code>var val = 4\nvar x = (\n  ,field1=1           // field1 with implicit type and 1 value\n  ,field2:string = _  // field2 with explicit type and \"\" default value\n  ,field3:int = 3     // field3 with explicit type and 3 value\n  ,val                // unnamed field with value `val` (4)\n)\n</code></pre>"},{"location":"pyrope/03-bundle/#tuple-and-scope","title":"Tuple and scope","text":"<p>Since tuples can be named or unnamed, an entry like <code>xx=(foo)</code> creates a tuple <code>xx</code> and copies the current scope variable <code>foo</code> contents as the first entry. In many cases it is required to pass a sequence of strings or identifiers. A solution is to name all the fields or quote as strings:</p> <pre><code>var x=100\n\nvar tup1 = ('x',y=4)\nvar tup2 = (x,y=4)\n\ncassert tup1[0] == 'x'\ncassert tup2[0] == 100\n</code></pre> <p>Some constructs like enumerates and attributes typically pass identifiers without assigning a value. The problem is that the syntax becomes not so \"nice\".  To address these cases, Pyrope does not use a variable reference but a \"string\" in the enumerate (<code>enum(a,b=3)</code>) and attribute (<code>foo::[attr]</code>, <code>foo.[attr]</code>). In these constructs, a reference can be enforced with <code>...var</code></p> <pre><code>let aa = 3\nlet a = enum(aa,b=3)\ncassert a==b\n\ncassert x.[size] == x.['size']\n\nlet zz= \"size\"\ncassert x.[...zz] == x.[size]\n</code></pre>"},{"location":"pyrope/03-bundle/#everything-is-a-tuple","title":"Everything is a tuple","text":"<p>In Pyrope everything is a Tuple, and it has some implications that this section tries to clarify.</p> <p>A tuple starts with <code>(</code> and finishes with <code>)</code>. In most languages, the parentheses have two meanings, operation precedence and/or tuple/record. In Pyrope, since a single element is a tuple too, the parenthesis always means a tuple.</p> <p>A code like <code>(1+(2),4)</code> can be read as \"Create a tuple of two entries. The first entry is the result of the addition of <code>1</code> (which is a tuple of 1) and a tuple that has <code>2</code> as a unique entry. The second entry in the tuple is <code>4</code>\".</p> <p>The tuple entries are separated by comma (<code>,</code>). Extra commas do not add meaning.</p> <pre><code>var a = (1,2)   // tuple of 2 entries, 1 and 2\nvar b = (1)     // tuple of 1 entry, 1\nvar c = 1       // tuple of 1 entry, 1\nvar d = (,,1,,) // tuple of 1 entry, 1\ncassert a.0 == b.0 == c.0 == d.0\ncassert a!=b\ncassert b == c == d\n</code></pre> <p>A tuple with a single entry element is called a scalar.</p> <p>Tuples are used in many places:</p> <ul> <li>The arguments for a call function are a tuple. E.g: <code>fcall(1,2)</code></li> <li>The return of a function call is always a tuple. E.g: <code>foo = fcall()</code></li> <li>The index for a selector <code>[...]</code> is a tuple. As syntax sugar, the tuple parenthesis can be omitted. E.g: <code>foo@[0,2,3]</code></li> <li>The complex type declaration are a tuple. E.g: <code>let Xtype = (f=1,b:string)</code></li> </ul>"},{"location":"pyrope/03-bundle/#tuple-mutability","title":"Tuple mutability","text":"<p>The tuple entries can be mutable/immutable and named/unnamed. Tuple entries follow the variable mutability rules with the exception that <code>=</code> can be used to declare a mutable field. <code>(a=3)</code> is equivalent to <code>(var a=3)</code>.</p> <pre><code>var c=(x=1,let b = 2, var d=3)\nc.x   = 3  // OK\nx.foo = 2  // compile error, tuple 'x' does not have field 'foo'\nc.b   = 10 // compile error, 'c.b' is immutable\nc.d   = 30 // OK, d was already mutable type\n\nlet d=(x=1, let y=2, var z=3)\nd.x   = 2  // OK\nd.foo = 3  // compile error, tuple 'd' does not have field foo'\nd.z   = 4  // compile error, 'd' is immutable\n\nvar e:d = _\nassert e.x==1 and e.y==2 and e.z==3\ne.x = 30   // OK\ne.y = 30   // compile error, 'e.y' is immutable\ne.z = 30   // OK\n</code></pre> <p>Tuples are always ordered, but they can have unnamed entries. If needed a <code>_</code> can be used for name or default value during the tuple declaration.</p> <pre><code>var b = 100\nvar a = (b:u8, b, b:u8 = _, let c=4) // a.0 and a.1 are unnamed, a.2==a.b\na.b = 200\nassert a == (100, 100, 200, 4)\n\nvar f = (b=3, let e=5)\nf.b = 4                 // OK\nf.e = 10                // compile error, `f.e` is immutable\n\nlet x = (1,2)\nx[0] = 3                // compile error, 'x' is immutable\nvar y = (1, let _ = 3)  // 2nd field is unnamed (only let allows that)\ny[0] = 100              // OK\ny[1] = 101              // compile error, `y[1]` is immutable\n</code></pre> <p>While the tuple entries can be either mutable or immutable, the field name/types are immutable. It is possible to construct new tuples with the <code>++</code> (concatenate) and <code>...</code> (in-place operator):</p> <pre><code>var a=(a=1,b=2)\nlet b=(c=3)\n\nlet ccat1 = a ++ b\nassert ccat1 == (a=1,b=2,c=3)\nassert ccat1 == (1,2,3)\n\nvar ccat2 = a                // mutable tuple\na = a ++ (b=20)\nassert ccat2 == (a=1,b=(2,20),c=3)\nassert ccat2 == (1,(2,20),3)\n\nvar join1 = (...a,...b)\nassert join1 == (a=1,b=2,c=3)\nassert join1 == (1,2,3)\n\nvar join2 = (...a,...(b=20)) // compile error, 'b' already exists\n</code></pre> <p>The <code>a ++ b</code> concatenates two tuples. If the same field exists in both tuples, the resulting field will have a tuple with the entries of <code>a</code> and <code>b</code>.  The concat tries to match by field name, if the field names do not match or have no name a new entry is created. The algorithm starts with tuple <code>a</code> and starts from tuple field 0 upwards.</p> <pre><code>assert(((1,a=2,c=3) ++ (a=20,33,c=30,4)) == (1,a=(2,20),c=(3,30),33,4))\n</code></pre> <p>The <code>...</code> also concatenates, but it is an \"inline concatenate\". The difference is where the fields are concatenated and that it triggers a compile error if the same entry already exists.</p>"},{"location":"pyrope/03-bundle/#field-access","title":"Field access","text":"<p>Since everything is a tuple, any variable can do <code>variable.0.0.0.0</code> because it literaly means, return the tuple first entry for four times.</p> <p>Another useful shortcut is when a tuple has a single field or entry, the tuple contents can be accessed without requiring the individual position or field entry name. This is quite useful for function return tuples with a single entry.</p> <pre><code>let x = (first=(second=3))\n\nassert x.first.second == 3\nassert x.first        == 3\nassert x              == 3\nassert x.0.second     == 3\nassert x.first.0      == 3\nassert x.0            == 3\n</code></pre> <p>Tuples can also use structural binding to unpack a tuple multiple fields into separate variables.</p> <pre><code>let x = (f1=(f1a=1,f1b=3), f2=4)\n\nlet (y,z) = x\nassert y == (1,3) and z == 4\nassert y.f1a == 1 and y.f1b == 3\nassert y == (f1a=1,f1b=3)\n</code></pre>"},{"location":"pyrope/03-bundle/#tuples-vs-arrays","title":"Tuples vs arrays","text":"<p>Tuples are ordered, as such, it is possible to use them as arrays. Tuples and arrays share most behavior/operations, the key difference is that arrays are unnamed with the same type for all the entries.</p> <pre><code>var bund1 = (0,1,2,3,4) // ordered and can be used as an array\n\nvar array1 = [0,1,2,3,4]  // [] force array, so all the entries have same type\n\nvar bund2 = (bund1,bund1,((10,20),30))\nassert bund2[0][1] == 1\nassert bund2[1][1] == 1\nassert bund2[2][0] == (10,20)\nassert bund2[2][0][1] == 20\nassert bund2[2][1] == 30\n</code></pre> <p>Pyrope tries to be compatible with synthesizable Verilog. In Verilog, when an out of bounds, access is performed in a packed array (unpacked arrays are not synthesizable), or an index has unknown bits (<code>?</code>), a runtime warning can be generated and the result is an unknown (<code>0sb?</code>). Notice that this is a pessimistic assumption because maybe all the entries have the same value when the index has unknowns.</p> <p>The Pyrope compile will trigger compile errors for out-of-bound access. It is not possible to create an array index that may perform an out of bounds access.</p> <pre><code>var array = (0,1,2)       // size 3, not 4\nlet tmp = array[3]        // compile error, out of bounds access\nvar index = 2\nif runtime {\n  index = 4\n}\n// Index can be 2 or 4\n\nvar res1 = array[index]   // compile error, out of bounds access\n\nvar res2 = 0sb?           // Possible code to be compatible with Verilog\nif index&lt;3 {\n  res = array[index]      // OK\n}\n</code></pre> <p>Pyrope compiler will allow an index of an array/tuple with unknowns. If the index has unknown bits (<code>0sb?</code> or <code>0b1?0</code>) but the compiler can not know, the result will have unknowns (see internals for more details). Notice that the only way to have unknowns is that somewhere else a variable or a memory was explicitly initialized with unknowns. The default initialization in Pyrope is 0, not unknown like Verilog.</p>"},{"location":"pyrope/03-bundle/#concatenate-fields","title":"Concatenate fields","text":"<p>Each tuple field must be unique. Nevertheless, it is practical to have fields that add more subfields. This is the case for overloading. To append or concatenate in a given field the <code>++=</code> operator can be assigned.</p> <pre><code>var x = (\n  ,ff = 1\n  ,ff = 2 // compile error\n)\n\nvar y = (\n  ,ff = 1\n  ,ff ++= 2\n  ,zz ++= 3\n)\nassert y == (ff=(1,2),zz=3)\n</code></pre>"},{"location":"pyrope/03-bundle/#optional-tuple-parenthesis","title":"Optional tuple parenthesis","text":"<p>Parenthesis marks the beginning and the end of a tuple. Those parentheses can be avoided for an unnamed tuple in some cases:</p> <ul> <li>When doing a simple function call at the beginning of a line.</li> <li>When used inside a selector <code>[...]</code>.</li> <li>When used after an <code>in</code> operator followed by a <code>{</code> like in a <code>for</code> and <code>match</code> statements.</li> <li>For the inputs in a match statement.</li> <li>A single element lambda return value.</li> </ul> <pre><code>fcall 1,2         // same as: fcall(1,2)\nb = xx[1,2]       // same as: xx[(1,2)]\n\nfor a in 1,2,3 {  // same as: for a in (1,2,3) {\n  x = a\n}\ny = match z {\n  in 1,2 { 4 }    // same as: in (1,2) { 4 }\n  else { 5 }\n}\ny2 = match var one=1 ; one ++ z {  // same as: y2 = match (1,z) {\n  == (1,2) { 4 }\n}\n\nlet addb = fun(a,b:u32)-&gt; a:u32 { // same as: letaddb = fun(a,b:u32)-&gt;(a:u32)\n  a = a + b\n}\n</code></pre> <p>A named tuple parenthesis can be omitted on the left-hand side of an assignment. This is to mutate or declare multiple variables at once.  It is not allowed to avoid the parenthesis at the right-hand-side of the statement. The reason is that it is a bit confusing.</p> <pre><code>var a,b = (2,3)    // compile error, left-hand-side must be a tuple (a,b)\nvar (a,b) = 2,3    // compile error, right-hand-side must be a tuple (2,3)\nvar (a,b) = (2,3)\nassert a==2 and b==3\n\nvar (c,d) = 1..=2  // compile error, range is a single entry assignment\nvar c = 1..=2      // OK\nvar (c,d) = 1      // compile error, 2 entry tuple in lhs, same in rhs\nvar (c,d) = (1,2)  // OK\nassert c == 1 and d == 2\n</code></pre> <p>One thing to remember is that the <code>=</code> separates the statement in two parts (left and right), this is not the case with type or attributes that always apply to the immediatly declared variable or item.</p> <pre><code>let c = 4\nlet (x,b) = (true, c:u3) // assign x=true, b=4 AND check that c is type u3\n\ncassert x == true\ncassert b == 4\n</code></pre>"},{"location":"pyrope/03-bundle/#enumerate-enum","title":"Enumerate (<code>enum</code>)","text":"<p>Enumerates, or enums for short, use the familiar tuple structure, but there is a significant difference in initialization. Enums require named tuples, but in most cases the named tupled should not have a set value. Enums automatically assigns values, tuples need explicit value initialization.</p> <pre><code>let b = \"foo\"\nlet c = 1\nlet test1     = enum(a=c,b)    // OK\nlet something = (b)            // OK\ncassert something == \"foo\"\ncassert test1.a != test1.b\ncassert test1.a==1 and test1.b==2\n</code></pre> <p>The <code>enum</code> keyword does not reference scope variables unless the reference is on the right-hand-side.</p> <p>If an external variable wants to be used as a field, there has to be an explicit expression with a string type or a named tuple.</p> <pre><code>let a = \"field\"\nlet c = (foo=4)\nlet my_other_enum = enum(...a,b=3,...c)\ncassert my_other_enum.field != my_other_enum.b\ncassert my_other_enum.b   == 3\ncassert my_other_enum.foo == 4\ncassert my_other_enum.foo != my_other_enum.b\n</code></pre> <p>The enum default values are NOT like typical non-hardware languages. The enum auto-created values use a one-hot encoding. The first entry has the first bit set, the 2nd the 2nd bit set. If an entry has a value, the next entry uses the next free bit. If any field is set, then the enumerate behaves like a traditional enumerate sequence.</p> <pre><code>let V3 = enum(\n   ,a\n   ,b\n   ,c\n)\ncassert V3.a == 1\ncassert V3.b == 2\ncassert V3.c == 4\n\nlet V4 = enum(\n   ,a\n   ,b=5\n   ,c\n)\ncassert V4.a == 0\ncassert V4.b == 5\ncassert V4.c == 6\n</code></pre>"},{"location":"pyrope/03-bundle/#hierarchical-enumerates","title":"Hierarchical enumerates","text":"<p>Enum can accept hierarchical tuples. Each enum level follows the same algorithm. Each entry tries to find a new bit. In the case of the hierarchy, the lower hierarchy level bits are kept.</p> <pre><code>let Animal = enum(\n  ,bird  =(eagle, parrot)\n  ,mammal=(rat  , human )\n)\n\ncassert Animal.bird.eagle != Animal.mammal\ncassert Animal.bird != Animal.mammal.human\ncassert Animal.bird == Animal.bird.parrot\n\ncassert int(Animal.bird        ) == 0b000001\ncassert int(Animal.bird.eagle  ) == 0b000011\ncassert int(Animal.bird.parrot ) == 0b000101\ncassert int(Animal.mammal      ) == 0b001000\ncassert int(Animal.mammal.rat  ) == 0b011000\ncassert int(Animal.mammal.human) == 0b101000\n</code></pre> <p>In general, for each leaf enum, the number of bits is equivalent to the number of entries in the leaf tuple.</p> <p>It is possible to use a sequence that is more consistent with traditional programming languages, but this only works with non-hierarchical enumerates when an integer type (<code>:int</code>, <code>:u32</code>, <code>:i4</code> ...) is used.</p> <pre><code>let V5=enum(\n   ,a\n   ,b=5\n   ,c\n)\ncassert int(V5.a) == 0\ncassert int(V5.b) == 5\ncassert int(V5.c) == 6\n</code></pre> <p>The same syntax is used for enums to different objects. The hierarchy is not allowed when an ordered numbering is requested.</p> <p>Enumerates of the same type can perform bitwise binary operations (and/or/xor/nand/xnor/xnor) and set operators (in/!in).</p> <pre><code>let human_rat = Animal.mammal.rat | Animal.mammal.human  // union op\n\nassert Animal.mammal      in human_rat\nassert Animal.mammal.rat  in human_rat\nassert Animal.bird       !in human_rat\n</code></pre>"},{"location":"pyrope/03-bundle/#enumerate-typecast","title":"Enumerate typecast","text":"<p>To convert a string back and forth to an enumerate, explicit typecast is needed but possible.</p> <pre><code>let E3=enum(\n  ,l1=(\n    ,l1a\n    ,l1b\n    )\n  ,l2\n  )\ncassert string(E3.l1.l1a) == \"E3.l1.l1a\"\ncassert string(E3.l1) == \"E3.l1\"\ncassert E3(\"l1.l2\") == E3.l1.l2\n</code></pre>"},{"location":"pyrope/04-variables/","title":"Variables and types","text":"<p>A variable is an instance of a given type. The type may be inferred from use. The basic types are Boolean, lambda, Integer, Range, and String. All those types can be combined with tuples.</p>"},{"location":"pyrope/04-variables/#variable-scope","title":"Variable scope","text":"<p>Scope constrains variables visibility. There are three types of scope delimitation in Pyrope: code block scope, lambda scope, and tuple scope. Each has a different set of rules constraining the variable visibility. Overall, the variable/field is visible from declaration until the end of scope.</p> <p>Pyrope uses <code>var</code> or <code>let</code> to declare a variable, but all the declarations must have a value. <code>_</code> is used to specify the default value (<code>false</code> for boolean, <code>0</code> for integer, <code>\"\"</code> for string, undefined lambda for lambda, and <code>0..=0</code> for range).</p> <p>In all the cases, variable declaration is either: * <code>let variable [:type] = expression</code> * <code>var variable [:type] = expression</code></p> <p>In a tuple scope, <code>variable [:type] = expression</code> is equivalent to <code>var variable [:type] = expression</code>. This is to avoid the most common case where tuple fields are frequently declared <code>var</code> not <code>let</code>. This is different from lambda captures that declare a new variable but they are always immutable (<code>let</code>).</p> Code Block scopeLambda scopeTuple scope <pre><code>assert a == 3        // compile error, undefined variable 'a'\nvar a = 3\n{\n  assert a == 3\n  a = 33             // OK. assign 33\n  a:int = 33         // OK, assign 33 and check that 'a' has type int\n  let b = 4\n  let a = 3333       // compile error, variable shadowing\n  var a = 33         // compile error, variable shadowing\n}\nassert b == 3        // compile error, undefined variable 'b'\n</code></pre> <pre><code>assert a == 3        // compile error, undefined variable 'a'\nvar a = 3\nvar x = 10\nlet f1 = fun[a,x=a+1]() {\n  assert a == 3\n  a = 33             // compile error, capture/inputs are immutable\n  x = 300            // compile error, capture/inputs are immutable\n  let b = 4\n  let a = 3333       // compile error, variable shadowing\n  var a = 33         // compile error, variable shadowing\n  return b+3\n}\nassert f1() == 7\nassert x == 10\nassert b == 3        // compile error, undefined variable 'b'\n\nlet f2 = fun() {     // restrict scope\n  assert a == 3      // compile error, undefined variable 'a'\n}\nlet f3 = fun[ff=a]() { // restrict scope\n  assert ff == 3     // OK\n  ff = 3             // compile error, immutable variable\n}\n</code></pre> <pre><code>var a = 3\nlet r1 = (\n  ,a = a+1           // same as var a = a+1\n  ,c = {assert a == 3 and self.a==4; 50}\n)\nr1.a = 33            // compile error, 'r1' is immutable variable\n\nvar r2 = (a=100, let c=(a=a+1, e=self.a+30))\nassert r2 == (a=100,c=(a=101, e=131))  // checks values not mutability\nr2.a = 33            // OK\nr2.c.a = 33          // compile error, 'r2.c' is immutable variable\n</code></pre> <ul> <li> <p>Shadowing is not allowed in lambdas or code blocks. Tuples can redefine   (shadow) the same variable but to use inside the tuple, the <code>self</code> keyword   must be used always to access tuple scoped variables.</p> </li> <li> <p>Lambdas and tuples upper scope variables are always immutable.</p> </li> <li> <p>Lambdas can restrict upper scope visibility with <code>[]</code>.</p> </li> <li> <p>A variable is visible from definition until the end of scope in program order.</p> </li> </ul> <p>Since the captures and lambda inputs are always immutable, it is not allowed to declare them as <code>var</code> and redundant to declare them as <code>let</code>.</p> <pre><code>let f3 = fun(var x) { x + 1 }    // compile error, inputs are immutable\nlet f2 = fun[var x](z) { x + z } // compile error, captures are immutable\n</code></pre> <p>Tuple scope is also useful for declaring function default values:</p> <pre><code>fun example(a:int, b:int=self.a+5) -&gt; (_:int) {\n  return a+b\n}\nassert example(a=3) == (a+a+5)\nassert example(6,7) == (6+7)\nassert example(6) == (6+6+5)\nassert example(b=3) !=0         // compile error: undefined `a` argument\n</code></pre>"},{"location":"pyrope/04-variables/#basic-types","title":"Basic types","text":"<p>Pyrope has 8 basic types:</p> <ul> <li><code>boolean</code>: either <code>true</code> or <code>false</code></li> <li><code>enum</code>: enumerated</li> <li><code>fun</code>: A function or pure combinational lambda</li> <li><code>int</code>: which is signed integer of unlimited precision</li> <li><code>proc</code>: A procedure or lambda with state/clock or side-effects</li> <li><code>range</code>: A one hot encoding of values <code>1..=3 == 0b1110</code></li> <li><code>string</code>: which is a sequence of characters</li> <li><code>variant</code>: An union without typecast</li> </ul> <p>All the types except the function can be converted back and forth to an integer.</p>"},{"location":"pyrope/04-variables/#integer-or-int","title":"Integer or <code>int</code>","text":"<p>Integers have unlimited precision and they are always signed. Unlike most other languages, there is only one type for integer (unlimited), but the type system allows to add constrains to be checked when assigning the variable contents. Notice that the type is the same (<code>u32</code> is the same type as <code>i3</code>, they just have different constraints):</p> <ul> <li><code>int</code>: an unlimited precision integer number.</li> <li><code>unsigned</code>: An integer basic type constrained to be a natural number.</li> <li><code>u&lt;num&gt;</code>: An integer basic type constrained to be a natural number with a maximum value of \\(2^{\\texttt{num}}\\). E.g: <code>u10</code> can go from zero to 1024.</li> <li><code>i&lt;num&gt;</code>: an integer 2s complement number with a maximum value of \\(2^{\\texttt{num}-1}-1\\) and a minimum of \\(-2^{\\texttt{num}}\\).</li> <li><code>int(a..&lt;b)</code>: integer basic type constrained to be between <code>a</code> and <code>b</code>.</li> </ul> <pre><code>var a:int         = _ // any value, no constrain\nvar b:unsigned    = _ // only positive values\nvar c:u13         = _ // only from 0 to 1&lt;&lt;13\nvar d:int(20..=30)= _ // only values from 20 to 30 (both included)\nvar d:int(-5..&lt;6) = _ // only values from -5 to 6 (6 not included)\nvar e:int(-1,0)   = _ // 1 bit integer: -1 or 0\n</code></pre> <p>Integers can have 3 value (<code>0</code>,<code>1</code>,<code>?</code>) expression or a <code>nil</code>. Section Integers has more details, but those values can not be part of the type requirement.</p> <p>Integer typecast accepts strings as input. The string must be a valid formatted Pryope number or an assertion is raised.</p>"},{"location":"pyrope/04-variables/#boolean","title":"Boolean","text":"<p>A boolean is either <code>true</code> or <code>false</code>. Booleans can not mix with integers in expressions unless there is an explicit typecast (<code>int(false)==0</code> and <code>int(true)==-1</code>) or the integer is a 1 bit signed integer (0 and -1). Unlike integers, booleans do not support undefined value. A typecast from integer to boolean will raise an assertion when the integer has undefined bits (<code>?</code>) or <code>nil</code>.</p> <pre><code>let b = true\nlet c = 3\n\nif c    { call(x) }  // compile error, 'c' is not a boolean expression\nif c!=0 { call(x) }  // OK\n\nvar d = b or false   // OK\nvar e = c or false   // compile error, 'c' is not a boolean\n\nlet e = 0xfeed\nif e@[3] {           // OK, bit extraction for single bit returns a boolean\n  call(x)\n}\n\nassert 0 == (int(true)  + 1)  // explicity typecast\nassert 1 == (int(false) + 1)  // explicity typecast\nassert boolean(33) or false   // explicity typecast\n</code></pre> <p>String input typecase is valid, but anything different than (\"0\", \"1\", \"-1\", \"true\", \"TRUE\", \"t\", \"false\", \"FALSE\", \"f\") raises an assertion failure.</p> <p>Logical and arithmetic operations can not be mixed.</p> <pre><code>let x = a and b\nlet y = x + 1    // compile error: 'x' is a boolean, '1' is integer\n</code></pre>"},{"location":"pyrope/04-variables/#lambda-funproc","title":"Lambda (<code>fun</code>/<code>proc</code>)","text":"<p>Lambdas have several options (see Functions), but from a high level they provide a sequence of statements and they have a tuple for input and a tuple for output. Lambdas also can capture values from declaration. Like strings, lambdas are always immutable objects but they can be assigned to mutable variables.</p>"},{"location":"pyrope/04-variables/#range","title":"Range","text":"<p>Ranges are very useful in hardware description languages to select bits. They are 3 ways to specify a closed range:</p> <ul> <li><code>first..=last</code>: Range from first to the last element, both included</li> <li><code>first..&lt;last</code>: Range from first to last, but the last element is not included</li> <li><code>first..+size</code>: Range from first to <code>first+size</code>. Since there is <code>size</code>   elements, it is equivalent to write <code>first..&lt;(first+last)</code>.</li> </ul> <p>When used inside selectors (<code>[range]</code>) the ranges can be open (no first/last specified) or use negative numbers. Ranges only work with positive numbers, a negative number is to specify the distance from last.</p> <ul> <li><code>[first..&lt;-val]</code> is the same as <code>[first..&lt;(last-val+1)]</code>. The advantage is that the <code>last</code> or size in the tuple can be unknown.</li> <li><code>[first..]</code> is the same as <code>[first..=-1]</code>.</li> </ul> <pre><code>let a = (1,2,3)\nassert a[0..] == (1,2,3)\nassert a[1..] == (2,3)\nassert a[..=1] == (1,2)\nassert a[..&lt;2] == (1,2)\nassert a[1..&lt;10] == (2,3)\n\nlet b = 0b0110_1001\nassert b@[1..]        == 0b0110_100\nassert b@[1..=-1]     == 0b0110_100\nassert b@[1..=-2]     == 0b0110_100  // unsigned result from bit selector\nassert b@sext[1..=-2] == 0sb110_100\nassert b@[1..=-3]     == 0sb10_100\nassert b@[1..&lt;-3]     == 0b0_100\nassert b@[0]          == false\n</code></pre> <p>A range is a separate tuple. As such it can not directly compare with tupes. It requires an explicit conversion. If the range does not contain negative values, it can be converted to an integer back and forth which corresponds to a one-hot encoding.</p> <p>Range type cast from integers use the same one-hot encoding. It is not possible to type cast from tuple to range, but it is possible from range to tuple.</p> <pre><code>let c = 1..=3\nassert int(c) == 0b1110\nassert range(0b01_1100) == 2..=4\n\nassert range(1,2,3)            // compile error, typecast not allowed\nassert (1,2,3) == tuple(1..=3)\n</code></pre> <p>In most cases, the range can be used in contructs like <code>for</code> for positive and negative numbers. The <code>tuple</code> typecast is not needed, but if placed the semantic is the same. The same <code>tuple</code> typecast is also optional when doing a comparison. Both ranges a <code>step</code> to change the step.</p> <pre><code>assert   int(0..=10 step  2) == 0b101_0101_0101\nassert tuple(0..=10 step  2) == ( 0,2,4,6,8,10)\nassert tuple(10..=0 step -2) == (10,8,6,4,2, 0)\nassert      (10..=0 step -2) == (10,8,6,4,2, 0)\n\nassert -1..=2 == (-1,0,1,2)\nlet x = -1..=2\n\nassert (i for i in 0..=10 step 2) == (0,2,4,6,8,10)\n</code></pre> <p>Since the range is an integer, a decreasing range should have the same meaning that an increasing range (<code>1..=3 == 3..=1</code>) but to avoid mistakes/confusions, Pyrope generates a compile error in decreasing ranges.</p> <pre><code>assert 5..=0                           // compile error, 5 + 1 never reaches 0\nassert 5..=0 step -1 == (5,4,3,2,1,0)\n</code></pre> <p>A closed range can be converted to a single integer or a tuple. A range encoded as an integer is a set of one-hot encodings. As such, there is no order, but in Pyrope, ranges always have the order from smallest to largest. The <code>step expr</code> can be added to indicate a step or step function. This is only possible when both begin and end of the range are fully specified.</p> <pre><code>assert((0..&lt;30 step 10) == (0,10,20)) // ranges and tuples can combined\nassert((1..=3) ++ 4 == (1,2,3,4))   // tuple and range ops become a tuple\nassert 1..=3 == (1,2,3)\nassert((1..=3)@[..] == 0b1110)      // convert range to integer with @[..]\n</code></pre>"},{"location":"pyrope/04-variables/#string","title":"String","text":"<p>Strings are a basic type, but they can be typecasted to integers using the ASCII sequence. The string encoding assigns the lower bits to the first characters in the string, each character has 8 bits associated.</p> <pre><code>a = 'cad'              // c is 0x63, a is 0x61, and d is 0x64\nb = 0x64_61_63\nassert a == string(b)  // typecast number to string\nassert int(a) == b     // typecast string to number\nassert a@[..] == b     // typecast string to number\n</code></pre> <p>Like ranges, strings can also be seen as a tuple, and when tuple operations are performed they are converted to a tuple.</p> <pre><code>assert \"hello\" == ('h','e','l','l','o')\nassert \"h\" ++ \"ell\" == ('h','e','l','l') == \"hell\"\n</code></pre>"},{"location":"pyrope/04-variables/#type-declarations","title":"Type declarations","text":"<p>Each variable has a type, either implicit or explicit, and as such, it can be used to declare a new type.</p> <p>Pyrope does not have a <code>type</code> keyword. Instead it leverages the tuples for type creation. The difference is that a type should be an immutable variable, and therefore it is recommended to start with Uppercase.</p> <pre><code>var bund1 = (color:string, value:s33)\nx:bund1        = _      // OK, declare x of type bund1 with default values\nbund1.color    = \"red\"  // OK\nbund1.is_green = fun(self) { self.color == \"green\" }\nx.color        = \"blue\" // OK\n\nlet typ = (color:string, value:s33, is_green:fun(self) = _)\ny:typ        = _        // OK\ntyp.color    = \"red\"    // compile error\ntyp.is_green = fun(self) { self.color == \"green\" }\ny.color      = \"red\"    // OK\n\nlet bund3 = (color:string, value:s33)\nz:bund3        = _                 // OK\nbund3.color    = \"red\"             // compile error\nbund3.is_green = fun(self) { ... } // compile error\nz.color        = \"blue\"            // OK\n\nassert x equals typ  // same type structure\nassert z equals typ  // same type structure\nassert x equals z    // same type structure\n\nassert y is typ\nassert typ is typ\nassert z !is bund3\nassert z !is typ\nassert z !is bund1\n</code></pre> <p>Adding a method to a tuple with <code>tup.fn = fun...</code> is the same as <code>tup = tup ++ (fn=fun...)</code>.</p>"},{"location":"pyrope/04-variables/#type-checks","title":"Type checks","text":"<p>When a type is used in the left-hand-side of a declaration statement, the type is set for the whole existence of the variable. It is possible to also use type checks outside the variable declaration. Those are to check that the variable <code>does</code> comply with the type specified.</p> <pre><code>var a = true  // infer a is a boolean\n\nfoo = a:bool or false // checks that 'a' is a boolean\n</code></pre>"},{"location":"pyrope/04-variables/#attributes","title":"Attributes","text":"<p>Attributes is the mechanism that the programmer specifies some special checks/functionality that the compiler should perform. Attributes are associates to variables either setting an attribute or checking the value. Some example of check is to mark statements compile time constant, or read the number of bits in an assertion, or placement hints, or even interact with the synthesis flow to read timing delays.</p> <p>A key difference from attribute and tuple fields is that attributes are always compile time and the compiler flow has special meaning functionality for them.</p> <p>Pyrope does not specify all the attributes, the compiler flow specifies them. There are some built-in required attributes like checking the number of bits.</p> <p>Reading attributes should not affect a logical equivalence check. Writing attributes can have a side-effect because it can change bits use for wrap/saturate or change pins like reset/clock in registers. Additionally, attributes can affect assertions, so they can stop/abort the compilation.</p> <p>The are three operations that can be done with attributes: set, check, read.</p> <ul> <li> <p>Set: when associated to a variable type in the left-hand-side of an   assignment or directly accessed. If a variable definition, this binds the   attribute with all the use cases of the variable. If the variable just   changes attribute value, a direct assignment is possible E.g: <code>foo::[max=300]   = 4</code> or <code>baz.[attr] = 10</code></p> </li> <li> <p>Check: when associated to a type property in the right-hand-side of an   assignment. The attribute is a comma separated list of boolean expression   that must evaluate true only at this statement. E.g: <code>var tmp =   yy::[comptime, attr2&gt;0] + xx</code></p> </li> <li> <p>Read: a direct read of an attribute value is possible with <code>variable.field.[attribute]</code></p> </li> </ul> <p>The attribute set, writes a value to the attribute. If no value is given a boolean <code>true</code> is set. The attribute checks are expressions that must evaluate true.</p> <p>Since conditional code can depend on an attribute, which results in executing a different code sequence that can lead to the change of the attribute. This can create a iterative process. It is up to the compiler to handle this, but the most logical is to trigger a compile error if there is no fast convergence.</p> <pre><code>// attribute set\nvar foo:u32:[comptime=true] = xx   // enforce that foo is comptime true always\nvar bar::[comptime] = xx           // same as previous statement\nyyy = xx                           // yyy does not check comptime\nyyy::[comptime=true] = xx          // now, checks that 'yyy` is comptime\n\n// attribute check\nif bar == 3 {\n  tmp = bar::[comptime == true]    // check that this use of bar is comptime\n  tmp = bar::[comptime]            // same as previous statement\n  tmp = bar ; assert bar.[comptime] // same as previous statements\n}\n                                   // bar/foo may not be comptime\n\n// attribute read\nassert tmp.[bits] &lt; 30 and !tmp.[comptime]\n</code></pre> <p>The attribute check is like a type check, both can be converted to assertions, but the syntax is cleaner.</p> Attribute CheckAssertion Equivalent Check <pre><code>let x = y::[cond,bar==3] + 1\n\nread_state = fun(x) {\n  let f:u32:[comptime] = x // f is compile time or a error is generated\n  return f                 // f should be compile time constant\n}\n\nvar foo = read_state(zz) // foo will be compile time constant\n</code></pre> <pre><code>let x = y + 1\ncassert y.[cond]\ncassert y.[bar]==3\n\nread_state = fun(x) {\n  let f = x\n  cassert f does u32\n  cassert f.[comptime]\n  f\n}\n\nvar foo = read_state(zz) // foo will be compile time constant\n</code></pre> <p>Pyrope allows to assign the attribute to a variable or a function call. Not to statements because it is confusing if applied to the condition or all the sub-statements.</p> <pre><code>if cond::[comptime] {    // cond is checked to be compile time constant\n  x::[comptime] = a +1   // x is set to be compile time constant\n}else{\n  x::[comptime] = b      // x is set to be compile time constant\n}\n\n\nif cond.[comptime] {  // checks if cond is compute at comptime\n  let v = cond\n  if cond {\n    puts \"cond is compile time and true\"\n  }\n}\n</code></pre> <p>The programmer could create custom attributes but then a LiveHD compiler pass to deal with the new attribute is needed to handle based on their specific semantic. To understand the potential Pyrope syntax, this is a hypothetical <code>::[poison]</code> attribute that marks tuple.</p> <pre><code>let bad = (a=3,b::[poison]=4)\n\nlet b = bad.b\n\nassert b.[poison] and b==4\n</code></pre> <p>Attributes control fields like the default reset and clock signal. This allows to change the control inside procedures. Notice that this means that attributes are passed by reference. This is not a value copy, but a pass by reference. This is needed because when connecting things like a reset, we want to connect to the reset wire, not the current reset value.</p> <pre><code>let counter = proc(en, width) {\n  reg value:uint:[bits=width] = 0\n  value = value + 1\n  value\n}\n\nlet counter2::[clock_pin=clk1]=counter\nlet counter3::[reset_pin=rst2]=counter\n\nvar ctr2 =#[..] counter2(my_enable)\nvar ctr3 =#[..] counter3(my_enable)\n</code></pre> <p>In the long term, the goal is to have any synthesis directive that can affect the correctness of the result to be part of the design specification so that it can be checked during simulation/verification.</p> <p>There are 3 main classes of a attributes that all the Pyrope compilers should always implement: Bitwidth, comptime, debug.</p>"},{"location":"pyrope/04-variables/#variable-attribute-list","title":"Variable attribute list","text":"<p>In the future, the compiler may implement some of the following attributes, as such, these attribute names are reserved and not allowed for custom attribute passes:</p> <ul> <li><code>clock</code>: indicate a signal/input is a clock wire</li> <li><code>critical</code>: synthesis time criticality</li> <li><code>debug</code> (sticky): variable use for debug only, not synthesis allowed</li> <li><code>delay</code>: synthesis time delay</li> <li><code>defer</code>: for reads, it means last value written. For assigns, it means defer write</li> <li><code>deprecated</code>: to generate special warnigns about usage</li> <li><code>donttouch</code>: do not touch/optimize away</li> <li><code>file</code>: to print the file where the variable was declared</li> <li><code>inline</code>, <code>noinline</code>: to indicate if a module is inlined</li> <li><code>inp_delay</code>, <code>out_delay</code>: synthesis optimizations hints</li> <li><code>keep</code>: same as donttouch but shorter</li> <li><code>key</code>: variable/entry key name</li> <li><code>left_of</code>, <code>right_of</code>, <code>top_of</code>, <code>bottom_of</code>, <code>align_with</code>: placement hints</li> <li><code>let</code> and <code>var</code>: is the variable declared as <code>let</code> and/or <code>var</code></li> <li><code>loc</code>: line of code information</li> <li><code>max_delay</code>, <code>min_delay</code>: synthesis optimizations checked at simulation</li> <li><code>max_load</code>, <code>max_fanout</code>, <code>max_cap</code>: synthesis optimization hints</li> <li><code>multicycle</code>: number of cycles for optimizations checked at simulation</li> <li><code>pipeline</code>: pipeline related information</li> <li><code>private</code>: variable/field not visible to import/regref</li> <li><code>rand</code> and <code>crand</code>: simulation and compile time random number generation</li> <li><code>reset</code>: indicate a signal/input is a reset wire</li> <li><code>size</code>: Number of entries in tuple or array</li> <li><code>typename</code>: type name at variable declaration</li> <li><code>valid</code>, <code>retry</code>: for elastic pipelines</li> <li><code>warn</code>: is a boolean what when set to false disables compile warnings for associated variable</li> </ul>"},{"location":"pyrope/04-variables/#registers-and-pipestage-attribute-list","title":"Registers and pipestage attribute list","text":"<p>Registers have the following attributes:</p> <ul> <li><code>async</code>: false by default, selects an asynchronous reset</li> <li><code>initial</code>: reset value when reset is high</li> <li><code>clock</code>: connected to <code>clock</code> by default</li> <li><code>reset</code>: connected to <code>reset</code> by default</li> <li><code>negreset</code>: active low reset signal</li> <li><code>posclk</code>: true by default, selects a posedge or negnedge flop</li> <li><code>retime</code>: allow to retime across the register</li> </ul> <p>Pipestage accept the same register attributes but also two more:</p> <ul> <li><code>lat</code>: latency for the pipestage</li> <li><code>num</code>: Number of unitsi used when the pipestage is not fully pipelined.</li> </ul>"},{"location":"pyrope/04-variables/#memories-attribute-list","title":"Memories attribute list","text":"<p>Memories are arrays with persistence like registers. As such, some of the attributes are similar to registers, but unlike registers they can have multiple clocks.</p> <ul> <li><code>addr</code>: Tuple of address ports for the memory.</li> <li><code>bits</code>: The number of bits for each memory entry</li> <li><code>size</code>: The number of entries. Total size in bits is \\(size x bits\\).</li> <li><code>clock</code>: Optional clock pin, <code>clock</code> by default. A tuple is possible to specify the clock for each address port.</li> <li><code>din</code>: Tuple for memory data in port. The read ports must be hardwired to <code>0</code>.</li> <li><code>enable</code>: Tuple for each memory port. Write or read enable (read ports can have enable too).</li> <li><code>fwd</code>: Forwarding guaranteed (true/false). If fwd is false, there is no guarantee, it can have fwd or not.</li> <li><code>latency</code>: Number of cycles (<code>0</code> or <code>1</code>) when the read is performed</li> <li><code>wensize</code>: Write enable size allows to have a write mask. The default value   is 1, a wensize of 2 means that there are 2 bits in the <code>enable</code> for each   port. a wensize 2 with 2 ports has a total of 2+2+2 enable bits. Bit 0 of the   enable controls the lower bits of the memory entry selected.</li> <li><code>rdport</code>: Indicates which of the ports are read and which are written ports.</li> <li><code>posclk</code>: Positive edge clock memory for all the memory clocks. The default is <code>true</code> but it can be set to <code>false</code>.</li> </ul>"},{"location":"pyrope/04-variables/#lambda-attribute-list","title":"Lambda attribute list","text":"<p>Lambda attributes allow Introspection which requires some attributes.</p> <ul> <li><code>inputs</code>: returns the input tuple from the lambda</li> <li><code>outputs</code>: returns the input tuple from the lambda</li> <li><code>where</code>: returns the lambda used in the <code>where</code> clause</li> </ul>"},{"location":"pyrope/04-variables/#bitwidth-attribute-list","title":"Bitwidth attribute list","text":"<p>To set constrains on integer, boolean, and range basic types, the compiler has a set of bitwidth related attributes:</p> <ul> <li><code>max</code>: the maximum value allowed</li> <li><code>min</code>: the minimum value allowed</li> <li><code>ubits</code>: Maximum number of bits to represent the unsigned value. The number must be positive or zero</li> <li><code>sbits</code>: Maximum number of bits, and the number can be negative</li> <li><code>wrap</code>: allows to drop bits that do not fit on the left-hand side. It performs sign   extension if needed.</li> <li><code>saturate</code> keeps the maximum or minimum (negative integer) that fits on the   left-hand side.</li> </ul> <p>The integer type constructor allows to use a range to set max/min, but it is syntax sugar for direct attribute set.</p> <pre><code>opt1:uint(300) = 0\nopt2:int:[min=0,max=300] = 0  // same\nopt3::[min=0,max=300] = 0     // same\nopt4:int(0..=300) = 0         // same\n\nassert opt1.[ubits] == 0    // opt1 initialized to 0, so 0 bits\nopt1 = 200\nassert opt1.[ubits] == 8    // last assignment needs 9 sbits or 8 ubits\ntmp  = opt1::[ubits==8] + 1   // expression AND assert opt1.[ubits]==8 check\n</code></pre> <p>The wrap/saturate are attributes that only make sense for attribute set. There is not much to check/read besides checking that it was set before.</p> <pre><code>a:u32 = 100\nb:u10 = 0\nc:u5  = 0\nd:u5  = 0\nw:u5:[wrap] = 0     // attribute set for all the 'w' uses\n\nb = a               // OK, o precision lost\nc::[wrap] = a       // OK, same as c = a@[0..&lt;5] (Since 100 is 0b1100100, c==4)\nc = a               // compile error, 100 overflows the maximum value of 'c'\nw = a               // OK, 'w' has a wrap set at declaration\n\nc::[saturate] = a   // OK, c == 31\nc = 31\nd = c + 1           // compile error, '32' overflows the maximum value of 'd'\n\nd::[wrap] = c + 1   // OK d == 0\nd::[saturate] = c+1 // OK, d==31\nd::[saturate] = c+1 // OK, d==31\n\nx::[saturate] boolean = c // compile error, saturate only allowed in integers\n</code></pre>"},{"location":"pyrope/04-variables/#comptime-attribute","title":"comptime attribute","text":"<p>Pyrope borrows the <code>comptime</code> functionality from Zig. Any variable can set/check/read the compile time status. This means that the value must be constant at compile time or a compile error is generated.</p> <pre><code>let a::[comptime] = 1     // obviously comptime\nb::[comptime] = a + 2     // OK too\nlet c::[comptime] = rand  // compile error, 'c' is not compile time constant\n</code></pre> <p>To avoid too frequent comptime directives, Pyrope treats all the variables that start with uppercase as compile time constants.</p> <pre><code>var Xconst1 = 1      // obvious comptime\nvar Xvar2   = rand   // compile error, 'Xvar2' is not compile time constant\n</code></pre>"},{"location":"pyrope/04-variables/#debug-attribute","title":"debug attribute","text":"<p>In software and more commonly in hardware, it is common to have extra statements and state to debug the code. These debug functionality can be more than plain assertions, they can also include code.</p> <p>The <code>debug</code> attribute marks a mutable or immutable variable. At synthesis, all the statements that use a <code>debug</code> can be removed. <code>debug</code> variables can read from non debug variables, but non-debug variables can not read from <code>debug</code>. This guarantees that <code>debug</code> variables, or statements, do not have any side-effects beyond debug statements.</p> <pre><code>var a = (b::[debug]=2, c = 3) // a.b is a debug variable\nlet c::[debug] = 3\n</code></pre> <p>Assignments to debug variables also bypass protection access. This means that private variables in tuples can be accessed (read-only). Since <code>assert</code> marks all the results as debug, it allows to read any public/private variable/field.</p> <pre><code>x:(_priv=3, zz=4) = _\n\nlet tmp = x._priv         // compile error\nlet tmp::[debug] = x.priv // OK\n\nassert x._priv == 3    // OK, assert is a debug statement\n</code></pre>"},{"location":"pyrope/04-variables/#register","title":"Register","text":"<p>Both mutable and immutable variables are created every cycle. To have persistence across cycles the <code>reg</code> type must be used.</p> <pre><code>reg counter:u32   = 10\nvar not_a_reg:u32 = 20\n</code></pre> <p>In <code>reg</code>, the right-hand side of the initialization (<code>10</code> in the counterexample) is called only during reset. In non-register variables, the right-hand side is called every cycle. Most of the cases <code>reg</code> is mutable but it can be declared as immutable.</p>"},{"location":"pyrope/04-variables/#public-vs-private","title":"Public vs private","text":"<p>All variables are public by default. To declare a variable private within the tuple or file the <code>private</code> attribute must be set.</p> <p>The private has different meaning depending on when it is applied:</p> <ul> <li> <p>When applied to a tuple entry (<code>(field::[private] = 3)</code>), it means that the   entry can not be accessed outside the tuple.</p> </li> <li> <p>When applied to a <code>pipestage</code> variable (<code>var foo::[private] = 3</code>), it means that the   variable is not pipelined to the next type stage. Section   pipestage has more details.</p> </li> <li> <p>When is applied to a pyrope file upper scope variable (<code>reg top_reg:[private]   = 0</code>), it means that an <code>import</code> command or register reference can not access   it across files. Section typesystem has more details.</p> </li> </ul>"},{"location":"pyrope/04-variables/#operators","title":"Operators","text":"<p>There are the typical basic operators found in most common languages except exponent operations. The reason is that those are very hardware intensive and a library code should be used instead.</p> <p>All the operators work over signed integers.</p>"},{"location":"pyrope/04-variables/#unary-operators","title":"Unary operators","text":"<ul> <li><code>!a</code> or <code>not a</code> logical negation</li> <li><code>~a</code> bitwise negation</li> <li><code>-a</code> arithmetic negation</li> </ul>"},{"location":"pyrope/04-variables/#binary-integer-operators","title":"Binary integer operators","text":"<ul> <li><code>a + b</code> addition</li> <li><code>a - b</code> substraction</li> <li><code>a * b</code> multiplication</li> <li><code>a / b</code> division</li> <li><code>a &amp; b</code> bitwise and</li> <li><code>a | b</code> bitwise or</li> <li><code>a ^ b</code> bitwise xor</li> <li><code>a ~&amp; b</code> bitwise nand</li> <li><code>a ~| b</code> bitwise nor</li> <li><code>a ~^ b</code> bitwise xnor</li> <li><code>a &gt;&gt; b</code> arithmetic right shift</li> <li><code>a@[..] &gt;&gt; b</code> logical right shift</li> <li><code>a &lt;&lt; b</code> left shift</li> </ul> <p>In the previous operations, <code>a</code> and <code>b</code> need to be integers. The exception is <code>a &lt;&lt; b</code> where <code>b</code> can be a tuple. The <code>&lt;&lt;</code> allows having multiple values provided by a tuple on the right-hand side or amount. This is useful to create one-hot encodings.</p> <pre><code>cassert 1&lt;&lt;(1,4,3) == 0b01_1010\n</code></pre>"},{"location":"pyrope/04-variables/#binary-boolean-operators","title":"Binary boolean operators","text":"<ul> <li><code>a and b</code> logical and</li> <li><code>a or b</code> logical or</li> <li><code>a implies b</code> logical implication</li> <li><code>a !and b</code> logical nand</li> <li><code>a !or b</code> logical nor</li> <li><code>a !implies b</code> logical not implication</li> </ul>"},{"location":"pyrope/04-variables/#tupleset-operators","title":"Tuple/Set operators","text":"<ul> <li><code>a in b</code> is element <code>a</code> in tuple <code>b</code></li> <li><code>a !in b</code> true when element <code>a</code> is not in tuple <code>b</code></li> <li><code>tuple(a)</code> converts <code>a</code> to tuple, <code>a</code> can be a boolean, range, integer,   string, or already a tuple</li> </ul> <p>Most operations behave as expected when applied to signed unlimited precision integers.</p> <p>The <code>a in b</code> checks if values of <code>a</code> are in <code>b</code>. Notice that both can be tuples. If <code>a</code> is a named tuple, the entries in <code>b</code> match by name, and then contents. If <code>a</code> is unnamed, it matches only contents by position.</p> <pre><code>cassert (1,2) in (0,1,3,2,4)\ncassert (1,2) in (a=0,b=1,c=3,2,e=4)\ncassert (a=2) !in (1,2,3)\ncassert (a=2) in (1,a=2,c=3)\ncassert (a=1,2) in (3,2,4,a=1)\ncassert (a=1,2) !in (1,2,4,a=4)\ncassert (a=1) !in (a=(1,2))\n</code></pre> <p>The <code>a in b</code> has to deal with undefined values (<code>nil</code>, <code>0sb?</code>). The LHS with an undefined will be true if the RHS has the same named entry either defined or undefined.</p> <pre><code>cassert (x=nil,c=3) in (x=3,c=3)\ncassert (x=nil,c=3) in (x=nil,c=3,d=4)\ncassert (c=3)      !in (c=nil,d=4)\n</code></pre> <ul> <li><code>a ++ b</code> concatenate two tuples. If field appears in both, concatenate field. The a field is defined in one tupe and undefined in the other, the undefined value is not concatenated.</li> </ul> <pre><code>cassert ((a=1,c=3) ++ (a=1,b=2,c=nil)) == (a=(1,1), c=3, b=2)\ncassert ((1,2) ++ (a=2,nil,5)) == (1,2,a=2,5)\ncassert ((x=1) ++ (a=2,nil,5)) == (x=1,a=2,nil,5)\n\ncassert ((x=1,b=2) ++ (x=0sb?,3)) == (x=1,b=2,3)\n</code></pre> <ul> <li><code>(,...b)</code> in-place insert <code>b</code>. Behaves like <code>a ++ b</code> but it triggers a   compile error if both have the same defined named field.</li> </ul> <pre><code>cassert (1,b=2,...(3,c=3),6) == (1,b=2,3,c=3,6)\ncassert (1,b=2,...(nil,c=3),0sb?,6) == (1,b=2,nil,c=3,0sb?,6)\n</code></pre>"},{"location":"pyrope/04-variables/#type-operators","title":"Type operators","text":"<ul> <li><code>a has b</code> checks if <code>a</code> tuple has the <code>b</code> field where <code>b</code> is a string or   integer (position).</li> </ul> <pre><code>cassert((a=1,b=2) has \"a\")\n</code></pre> <ul> <li><code>a does b</code> is the tuple structure of <code>a</code> a subset of <code>b</code></li> <li><code>a equals b</code> same as <code>(a does b) and (b does a)</code></li> <li><code>a case b</code> same as <code>cassert a does b</code> and for each <code>b</code> field with a defined value,   the value matches <code>a</code> (<code>nil</code>, <code>0sb?</code> are undefined values)</li> <li><code>a is b</code> is a nominal type check. Equivalent to <code>a::[typename] == b::[typename]</code></li> </ul> <p>Each type operator also has the negated <code>(a !does b) == !(a does b)</code>, <code>(a !equals b) == !(a equals b)</code>, <code>a !case b == !(a case b)</code></p> <p>The <code>does</code> performs just name matching when the LHS is a named tuple. It reverts to name and position matching when some of the LHS entries are unnamed.</p> <pre><code>cassert (a=1,b=3) does (b=100,a=333,e=40,5)\ncassert (a=1,3) does (a=100,300,b=333,e=40,5)\ncassert (a=1,3) !does (b=100,300,a=333,e=40,5)\n</code></pre> <p>A <code>a case b</code> is equivalent to <code>cassert b does a</code> and for each defined value in <code>b</code> there has to be the same value in <code>a</code>. This can be used in any expression but it is quite useful for <code>match ... case</code> patterns.</p> <pre><code>match (a=1,b=3) {\n  case (a=1) { cassert true }\n  else { cassert false }\n}\n\nmatch let t=(a=1,b=3); t {\n  case (a=1  ,c=4) { cassert false }\n  case (b=nil,a=1) { cassert t.b==3 and t.a==1 }\n  else { cassert false }\n}\n</code></pre> <p>An <code>x = a case b</code> can be translated to:</p> <pre><code>cassert b does a\nx = b in a\n</code></pre>"},{"location":"pyrope/04-variables/#reduce-and-bit-selection-operators","title":"Reduce and bit selection operators","text":"<p>The reduce operators and bit selection share a common syntax <code>variable@op[sel]</code> where:</p> <ul> <li> <p><code>variable</code> is a tuple where all the tuple fields and subfields must have a   explicit type size unless the tuple has 1 entry.</p> </li> <li> <p><code>op</code> is the operation to perform</p> <ul> <li><code>|</code>: or-reduce.</li> <li><code>&amp;</code>: and-reduce.</li> <li><code>^</code>: xor-reduce or parity check.</li> <li><code>+</code>: pop-count.</li> <li><code>sext</code>: Sign extends selected bits.</li> <li><code>zext</code>: Zero sign extends selected bits (default option)</li> </ul> </li> <li> <p><code>sel</code> can be a close-range like <code>1..&lt;=4</code> or <code>(1,4,6)</code> or an open range like   <code>3..</code>. Internally, the open range is converted to a close-range based on the   variable size.</p> </li> </ul> <p>The or/and/xor reduce have a single bit signed result (not boolean). This means that the result can be 0 (<code>0sb0</code>) or -1 (<code>0sb1</code>). pop-count and <code>zext</code> have always positive results. <code>sext</code> is a sign-extended, so it can be positive or negative.</p> <p>If no operator is provided, a <code>zext</code> is used by default. The bit selection without operator can also be used on the left-hand side to update a set of bits.</p> <p>The or-reduce and and-reduce are always size insensitive. This means that to perform the reduction it is not needed to know the number of bits. It could pick more or fewer bits and the result is the same. E.g: 0sb111 or 0sb111111 have the same and/or reduce. This is the reason why both can work with open and close ranges.</p> <p>This is not the case for the xor-reduce and pop-count. These two operations are size insensitive for positive numbers but sensitive for negative numbers. E.g: pop-count of 0sb111 is different than 0sb111111. When the variable is negative a close range must be used. Alternatively, a <code>zext</code> must be used to select bits accordingly. E.g: <code>variable@[0..=3]@+[..]</code> does a <code>zext</code> and the positive result is passed to the pop-count. The compiler could infer the size and compute, but it is considered non-intuitive for programmers.</p> <pre><code>x = 0b1_0110   // positive\ny = 0s1_0110   // negative\nassert x@[0,2] == 0b10\nassert y@[100,200]       == 0b11   and x@[100,200]       == 0\nassert y@sext[0,100,200] == 0sb110 and x@sext[1,100,200] == 0b001\nassert x@|[..] == -1\nassert x@&amp;[0,1] == 0\nassert x@+[0..=5] == x@+[0..&lt;100] == 3\nassert y@+[0..=5]  // compile error, 'y' can be negative\nassert y@[..]@+[..] == 3\nassert y@[0..=5]@+[..] == 3\nassert y@[0..=6]@+[..] == 4\n\nvar z     = 0b0110\nz@[0] = 1\nassert z == 0b0111\nz@[0] = 0b11 // compile error, '0b11` overflows the maximum allowed value of `z@[0]`\n</code></pre> <p>Note</p> <p>It is important to remember that in Pyrope all the operations use signed numbers. This means that an and-reduce over any positive number is always going to be zero because the most significant bit is zero, E.g: <code>0xFF@&amp;[..] == 0</code>. In some cases, a close-range will be needed if the intention is to ignore the sign. E.g: <code>0xFF@&amp;[0..&lt;8] == -1</code>.</p> <p>The bit selection operator only works with ranges, boolean, and integers. It does not work with tuples or strings. For converting in these object a <code>union:</code> must be used.</p> <p>Another important characteristic of the bit selection is that the order of the bits on the selection does not affect the result. Internally, it is a bitmask that has no order. For the <code>zext</code> and <code>sext</code>, the same order as the input variable is respected. This means that <code>var@[1,2] == var@[2,1]</code>. As a result, the bit selection can not be used to transpose bits. A tuple must be used for such an operation.</p> <pre><code>var v = 0b10\nassert v@[0,1] == v@[1,2] == v@[..] == v@[0..=1] == v@[..=1] == 0b10\n\nvar trans = 0\n\ntrans@[0] = v@[1]\ntrans@[1] = v@[0]\nassert trans == 0b01\n</code></pre>"},{"location":"pyrope/04-variables/#precedence","title":"Precedence","text":"<p>Pyrope has very shallow precedence, unlike most other languages the programmer should explicitly indicate the precedence. The exception is for widely expected precedence.</p> <ul> <li>Unary operators (not,!,~,?) bind stronger than binary operators (+,++,-,*...)</li> <li>Comparators can be chained (a&lt;=c&lt;=d) same as (a&lt;=c and c&lt;=d)</li> <li>mult/div precedence is only against +,- operators.</li> <li>Parenthesis can be avoided when a expression left-to-right has the same   result as right-to-left.</li> </ul> Priority Category Main operators in category 1 unary not ! ~ ? 2 mult/div *, / 3 other binary ..,^, &amp;, -,+, ++, &lt;&lt;, &gt;&gt;, in, does, has, case, equals, to 4 comparators &lt;, &lt;=, ==, !=, &gt;=, &gt; 5 logical and, or, implies <pre><code>assert((x or !y) == (x or (!y)) == (x or not y))\nassert((3*5+5) == ((3*5) + 5) == 3*5 + 5)\n\na = x1 or x2==x3 // same as b = x1 or (x2==x3)\nb = 3 &amp; 4 * 4    // compile error: use parenthesis for explicit precedence\nc = 3\n  &amp; 4 * 4\n  &amp; 5 + 3        // compile error: use parenthesis for explicit precedence\nc2 = 3\n  &amp; (4 * 4)\n  &amp; (5 + 3)      // OK\n\nd = 3 + 3 - 5    // OK, same result right-left\n\ne = 1\n  | 5\n  &amp; 6           // compile error: use parenthesis for explicit precedence\n\nf = (1 &amp; 4)\n  | (1 + 5)\n  | 1\n\ng = 1 + 3\n  * 1 + 2\n  + 5           // OK, but not nice\n\ng1= 1 + (3 * 1)\n  + 2\n  + 5           // OK\n\ng2= (1 + 3)\n  * (1 + 2)\n  + 5           // OK\n\nh = x or y and z// compile error: use parenthesis for explicit precedence\n\ni = a == 3 &lt;= b == d\nassert i == (a==3 and 3&lt;=b and b == d)\n</code></pre> <p>Comparators can be chained, but only when they follow the same type or the direction is the same.</p> <pre><code>assert a &lt;= b &lt;= c  // same as a&lt;=b and b&lt;=c\nassert a &lt;  b &lt;= c  // same as a&lt; b and b&lt;=c\nassert a == b &lt;= c  // compile error, chained only allowed with same comparator\nassert a &lt;= b &gt;  c  // compile error, not same direction\n</code></pre>"},{"location":"pyrope/04-variables/#optional","title":"Optional","text":"<p>The <code>?</code> is used by several languages to handle optional or null pointer references. In non-hardware languages, <code>?</code> is used to check if there is valid data or a null pointer. This is the same as checking the <code>::[valid]</code> attribute with a more friendly syntax.</p> <p>Pyrope does not have null pointers or memory associated management. Pyrope uses <code>?</code> to handle <code>::[valid]</code> data. Instead, the data is left to behave without the optional, but there is a new \"valid\" field associated with each tuple entry. Notice that it is not for each tuple level but each tuple entry.</p> <p>There are 4 explicitly interact with valids:</p> <ul> <li> <p><code>tup.f1?</code> reads the valid for field <code>f1</code> from tuple <code>tup</code></p> </li> <li> <p><code>tup?.f1.f2</code> returns <code>0bs0</code> if tuple fields <code>f1</code> or <code>f2</code> are invalid</p> </li> <li> <p><code>tup.f1? = cond</code> explicitly sets the field <code>f1</code> valid to <code>cond</code></p> </li> <li> <p><code>a = b op c</code> variable <code>a</code> will be valid if <code>b</code> AND <code>c</code> are valid</p> </li> </ul> <p>The optional or valid attached to each variable and tuple field is implicitly computed as follows:</p> <ul> <li> <p>Non-register variables are initialized with valid unless <code>_</code> is used in the   initialization which explicitly clears the valid attribute.</p> </li> <li> <p>Registers set the valid after reset, but if the reset clears the valid, there   is not guaranteed on attribute <code>[valid]</code> during reset. If the register does   not have a reset signal, the register is always valid unless explicitly   cleared.</p> </li> <li> <p>Left-hand side variables <code>valids</code> are set to the and-gate of all the variable   valids used in the expression</p> </li> <li> <p>memory/arrays do not tend to have reset signals. As such they are always   valid unless the memory has explicit reset code. In which case the valid   behaves like in flops.</p> </li> <li> <p>Writing to a register updates the register valid based on the din valid, or   when the attribute <code>[valid]</code> is explicitly managed.</p> </li> <li> <p>conditionals (<code>if</code>) update valids independently for each path</p> </li> <li> <p>A tuple field has the valid set to false if any of the tuple fields is   invalid</p> </li> <li> <p>The valid computation can be overwritten with the <code>[valid]</code> attribute. This   is possible even during reset.</p> </li> </ul> <p>Observation</p> <p>The variable valid calculation is similar to the Elastic 'output_written' from Liam but it is not an elastic update because it does not consider the abort or retry.</p> <p>The previous rules will clear a valid only if an expression has no valid, but the only way to have a non-valid is if the inputs to the lambda are invalid or if the valid is explicitly clear. The rules are designed to have no overhead when valid are not used. The compiler should detect that the valid is true all the time, and the associated logic is removed.</p> <p>Most statements evaluate independent of the valid expression. Expressions will evaluate the same if any of the inputs is valid or invalid. The valid attribute is computed in parallel to avoid being in the critical path. The exception are the verification statements like asserts and printing statatements like <code>puts</code>. These statements are gated or not performed if any of the inputs is invalid. To ignore the valid check, the <code>always</code> command can be appended before and as a result the statments will evaluate every cycle independent of the reset/valid status.</p> <pre><code>var v1:u32 = _                 // v1 is zero every cycle AND not valid\nassert v1.[valid] == false\nvar v2:u32 = 0                 // v2 is zero every cycle AND     valid\nassert v2.[valid] == true\n\ncassert v1?\ncassert not v2?\n\nassert v1 == 0 and v2 == 3     // data still same as usual\n\nv1 = 0sb?                      // OK, poison data\nv2 = 0sb?                      // OK, poison data, and update valid\nassert v2?                     // valid even though data is not\n\nassert v1 != 0                 // usual verilog x logic\nassert v2 != 0                 // usual verilog x logic\n\nlet res1 = v1 + 0              // valid with just unknown 0sb? data\nlet res2 = v2 + 0              // valid with just unknown 0sb? data\n\nassert res1?\nassert res2?\n\nreg counter:u32 = 0\n\nalways assert counter.reset implies !counter?\n</code></pre> <p><code>valid</code> can be overwritten by the setter method:</p> <pre><code>let custom = (\n  ,data:i16 = _\n  ,setter = proc(ref self, v) {\n    self.data = v\n    self.[valid] = v != 33\n  }\n)\n\nvar x:custom = _\n\ncassert x?\nx.data = 33\ncassert not x?\nx.data = 100\ncassert x?\n</code></pre> <p>The contents of the tuple field do not affect the field valid bit. It is data-independent. Tuples also can have an optional type, which behaves like adding optional to each of the tuple fields.</p> <pre><code>let complex = (\n  ,reg v1:string = \"foo\"\n  ,v2:string = _\n\n  ,setter = proc(ref self,v) {\n     self.v1 = v\n     self.v2 = v\n  }\n)\n\nvar x1:complex = _\nvar x2:complex:[valid=false] = 0  // toggle valid, and set zero\nvar x3:complex = 0\nx3.[valid] = false                // set invalid\n\nassert x1.v1 == \"\" and x1.v2 == \"\"\nassert not x2? and not x2.v1? and not v2.v2?\nassert x2.v1 == \"\" and x2.v2 == \"\"\n\nassert x2?.v1 == \"\" and x2?.v1 != \"\"  // any comparison is false\n\n// When x2? is false, any x2?.foo returns 0sb? with the associated x rules\n\nx2.v2 = \"hello\" // direct access still OK\n\nassert not x2? and x2.v1 == \"\" and x2.v2 == \"hello\"\n\nx2 = \"world\"\n\nassert x2? and x2?.v1 == \"world\" and x2.v1 == \"world\"\n</code></pre>"},{"location":"pyrope/04-variables/#variable-initialization","title":"Variable initialization","text":"<p>Variable initialization indicates the default value set every cycle and the optional (<code>::[valid]</code> attribute).</p> <p>The <code>let</code> and <code>var</code> statements require an initialization value for each cycle. Pyrope only has undefined values unless explicitly indicated. A variable has an undefined value if and only if the value is set to <code>nil</code> or all the bits are unknown (<code>0sb?</code>). Undefined variables always have invalid optional (<code>.[valid]==false</code>), and defined can have valid or invalid optional.</p> <p>On any assignment (<code>v = _</code>) where the rhs is a single underscore <code>_</code>, the variable optional is set to false, and it is assigned the default value:</p> <ul> <li><code>0</code> for integer</li> <li><code>false</code> for boolean</li> <li><code>\"\"</code> for string</li> <li><code>nil</code> otherwise</li> </ul> <pre><code>var a:int = _\ncassert a==0 and a.[valid] == false and not a?\n\nvar b:int = 0\ncassert b==0 and b.[valid] and b?\nb = nil\ncassert b==nil and b.[valid] == false and not b?\n\nvar c:fun(a1) = _\ncassert c == nil and c.[valid]==false\nc = fun(a1) { cassert true }\ncassert c!= nil and c.[valid]\n\nvar d:[] = _               // empty tuple\ncassert d != nil and d.[valid]\ncassert d[0] == nil and !d[0].[valid]\n\nvar e:int = nil\ncassert e==nil and !e.[valid] and not e?\ne = 0\ncassert e==0 and e.[valid] and e?\n</code></pre> <p>The same rules apply when a tuple or a type is declared.</p> <pre><code>let a = \"foo\"\n\nvar at1 = (\n  ,a:string\n)\ncassert at1[0] == \"foo\"\ncassert at1 !has \"a\"    // at1.a undefined\n\nvar at2 = (\n  ,a:string = _\n)\ncassert at2.a == \"\"  and at2.a.[valid]==false\nat2.a = \"torrellas\"\ncassert at2.a == \"torrellas\" and at2[0] == \"torrellas\"\n\nvar at3:at2 = _\ncassert at3.a == \"\"  and at3.a.[valid]==false\n\nvar at4:at2 = (a=\"josep\")\ncassert at4.a == \"josep\"  and at4.a.[valid] and at4.[valid]\n</code></pre> <p>Conditional path affect variable initialization and values. If all the conditional passes assign a value, the valid will be true. If only one path assigns a value, the valid will be set only on that path, but the data may always have the path.</p> <pre><code>var x=_\nvar y=2\nvar z=_\nif rand {\n  x = 3\n  y = 4\n  z = 5\n}else{\n  z = 6\n}\nassert x==3  // Simplified due to the _ initialization\nassert rand      implies x.[valid]\nassert x.[valid] implies rand\n\nassert y.[valid]\nassert  rand implies y == 4\nassert !rand implies y == 2\n\nassert z.[valid]\nassert  rand implies z == 5\nassert !rand implies z == 6\n</code></pre> <p>It is also possible to assign to an underscore, it behaves like a sink. It may be useful when dealing with structured bindings ()multiple return values) with unused arguments.</p> <pre><code>let weird_pick_bits = fun(b:u32) -&gt; (x:u1, _:u4) {\n  return (x=n@[2..&lt;3], b@[5])\n}\n\nfun fcall_returns_2_values()-&gt;(xx,yy) {\n  xx = 3\n  yy = 7\n}\n\nlet (a,_) = fcall_returns_2_values()\nassert a == 3\n\nvar b:u8 = 3\n_ = a           // legal, but it is just a \"read\" to 'a'\n_ = 3\nb = _\nassert b == 0 and not b.[valid]\n</code></pre>"},{"location":"pyrope/05-assert/","title":"Verification","text":"<p>Verification covers the language constructs and special support to ease design verification.</p>"},{"location":"pyrope/05-assert/#assertions","title":"Assertions","text":"<p>Assertions are considered debug statements. This means that they can not have side effects on non-debug statements.</p> <p>Pyrope supports a syntax close to Verilog for assertions. The language is designed to have 3 levels of assertion checking: compilation time, simulation runtime, and formal verification time.</p> <p>There are 5 main verification statements:</p> <ul> <li> <p><code>assert</code> and <code>cassert</code> are used to specify conditions that should hold true.   <code>cassert</code> is required to hold true at compile time and <code>assert</code> can be   checked either at compile or runtime if too slow to check. If the condition   doesn't hold, an error is raised.</p> </li> <li> <p><code>optimize</code> is exactly like <code>assert</code>, but it also allows the tool to simplify   code based on the given conditions. This can lead to more efficient code   generation. While unproven <code>assert</code> can be enabled/disabled during   simulation, <code>optimize</code> can not be disabled because it can lead to incorrect   simulation state.</p> </li> <li> <p><code>requires</code> is statement that can be placed in lambdas. The clause specifies   pre conditions to be true when the lambda is called. <code>requires</code> allows for code   optimizations like <code>optimize</code> statement.</p> </li> <li> <p><code>ensures</code> is a statement similar to <code>requires</code> but the clause specifies a   post condition. <code>ensures</code> allows code optimizations like <code>optimize</code> statement.</p> </li> </ul> <p>Hardware setups always have an extensive CI/verification setup. This means that run-time assertion failures are OK, better compile time to reduce design time, but OK at simulation time. This means that in things like type check, if it may be OK but not possible to prove, the compiler can decide to insert an assert instead of forcing a code structure change. To enforce that an assertion is checked only at compile time a <code>cassert</code> must be used. <code>assert</code>, <code>requires</code>, <code>ensures</code> can be checked at runtime if not possible to check at compile time.</p> <pre><code>a = 3\nassert a == 3          // checked at runtime (or compile time)\ncassert a == 3         // checked at compile time\n\noptimize b &gt; 3         // may optimize and perform a runtime check\n\nlet max_not_zero = fun(a,b) -&gt; (res)\n  requires a&gt;0\n  requires b&gt;0\n  ensures res==a or res==b {\n\n  res = if a&gt;b {a} else {b}\n}\n</code></pre> <p>A whole statement is conditionally executed using the <code>when</code>/<code>unless</code> gate expression. This is useful to gate verification statements (<code>assert</code>, <code>optimize</code>) that can have spurious error messages under some conditions.</p> <pre><code>a = 0\nif cond {\n  a = 3\n}\nassert cond implies a == 3, \"the branch was taken, so it must be 3??\"\nassert a == 3, \"the same error\" when   cond\nassert a == 0, \"the same error\" unless cond\n</code></pre> <p>The recommendation is to write as many <code>assert</code> and <code>optimize</code> as possible. If something can not happen, writing the <code>optimize</code> has the advantage of allowing the synthesis tool to generate more efficient code.</p> <p>The <code>optimize</code> will allow code optimizations, the <code>cassert</code> should also result in code optimizations. The reason why <code>assert</code> does not trigger optimizations is because they can be enabled/disabled at simulation time.</p> <p>In a way, most type checks have equivalent <code>cassert</code> checks.</p>"},{"location":"pyrope/05-assert/#lec","title":"LEC","text":"<p>The <code>lec</code> command is a formal verification step that checks that all the arguments are logically equivalent. <code>lec</code> only works for combinational logic, so does not need to worry about state or reset signals. The first argument is the gold model, the rest are implementation. This matters because the gold model unknown output bit checks against any value for the equivalent implementation bit.</p> <p>Note</p> <p>The recommendation is to use <code>optimize</code> and <code>assert</code> frequently, but clearly to check preconditions and postconditions of methods. The 1949 Turing quote of how to write assertions and programs is still valid \"the programmer should make a number of definite assertions which can be checked individually, and from which the correctness of the whole program easily follows.\"</p> <pre><code>let fun1 = fun(a,b) { a | b}\nlet fun2 = fun(a,b) { ~(~a | ~b) }\nlec fun1, fun2\n</code></pre> <p>In addition, there is the <code>lec_valid</code> command. It is similar to <code>lec</code> but it checks the optional or valid (<code>::[valid]</code>) from the output. It can take several cycles to show the same result.</p> <pre><code>let mul2 = proc(a,b) -&gt; (reg out) {\n  reg pipe1 = _\n\n  out = pipe1\n\n  pipe1 = a*b\n}\n\nlet mul0 = fun(a,b)-&gt;(out) { out = a*b }\n\nlec_valid mul0, mult2\n</code></pre>"},{"location":"pyrope/05-assert/#coverage","title":"Coverage","text":"<p>A bit connected with the assertion is coverage. The goal of an assertion is to be true all the time. The goal of a coverage point is to be true at least once during testing.</p> <p>There are two directives <code>cover</code> and <code>covercase</code>. The names are similar to the System Verilog <code>coverpoint</code> and <code>covergroup</code> but the meaning is not the same.</p> <ul> <li> <p><code>cover cond [, message]</code> the boolean expression <code>cond</code> must evaluate true   sometime during the verification or the tool can prove that it is true at   compile time.</p> </li> <li> <p><code>covercase grp, cond [,message]</code> is very similar to cover but it has a <code>grp</code>   group. There can be one or more covers for a given group. The extra check is   that one of the <code>cond</code> in the cover case must be true each time.</p> </li> </ul> <pre><code>// coverage case NUM group states that random should be odd or even\ncovecase NUM,   random&amp;1 , \"odd number\"\ncovecase NUM, !(random&amp;1), \"even number\"\n\ncovercase COND1, reset, \"in reset\"\ncovercase COND1, val&gt;3, \"bigger than 3\"\n\nassert((!reset and val&gt;3) or reset)  // less checks than COND1\n\ncover a==3, \"at least a is 3 once in a while\"\n</code></pre> <p>The <code>covercase</code> is similar to writing the assertions, but it checks that all the conditions happen through time or a low coverage is reported. In the <code>COND1</code> case, the assertion does not check that sometimes reset is set, and others the value is bigger than 3.  The assertion will succeed if reset is always set, but the covercase will fail because the \"bigger than 3\" case will not be tested.</p> <p>The <code>cover</code> allows to not be true a given cycle. To allow the same in a <code>covercase</code>, the designer can add <code>coverase GRP, true</code>. This is a true always cover point for the indicated cover group.</p>"},{"location":"pyrope/05-assert/#reset-optional-and-verification","title":"Reset, optional, and verification","text":"<p>In hardware is common to have an undefined state during the reset period. To avoid unnecessary assertion failures, if any of the inputs depends on a register directly or indirectly, the assertion is not checked when the reset is high for the given registers. In Pyrope, the registers and memory contents outputs are \"invalid\" (<code>::[valid]</code> attribute). <code>assert</code> and <code>optimize</code> will not check when any of the signals are invalid. This is useful to avoid unnecessary assert checks during reset or when the lambda is called with invalid data.</p> <p>Adding the <code>always</code> modifier before the assert/coverage keywords guarantees that the check is performed every cycle independent of the valid attribute.</p> <p>To provide assert/optimize during reset, Pyrope provides a <code>always assert</code>, <code>always cassert</code>, <code>always optimize</code>, <code>always covercase</code>, and <code>always cover</code>.</p> <pre><code>reg memory:[]u33 = (1,2,3) // may take cycles to load this contents\n\nassert memory[0] == 1 // not checked during reset\n\nalways assert memory[1] == 2 // may fail during reset\nalways assert memory[1] == 2 unless memory.reset  // should not fail\n</code></pre>"},{"location":"pyrope/05-assert/#random","title":"Random","text":"<p>Random number generation are quite useful for verification. Pyrope provides easy interfaces to generate \"compile time\" (<code>::[crand]</code>) and \"simulation time\" random number (<code>::[rand]</code>) generation.</p> <pre><code>let x:u8 = _\n\nfor i in 1..&lt;100 {\n  cassert 0 &lt;= x.[crand]  &lt;= 255\n}\n\nlet get_rand_0_2556 = fun(a:u8) {\n  a.[rand]\n}\n</code></pre> <p>Both rand and crand look at the set type max/min value and create a randon value between them. rand picks randomly in boolean and enumerate types, but it triggers a compile error for string, range, and lambda types.</p> <p>When applied to a tuple, it randomly picks an entry from the tuple.</p> <pre><code>let a = (1,2,3,b=4)\nlet x = a.[rand]\n\ncassert x==1 or x==2 or x==3 or x==4\ncassert x.b==4 when x==4\n</code></pre> <p>The simulation random number is considered a <code>::[debug]</code> statement, this means that it can not have an impact on synthesis or a compile error is generated.</p>"},{"location":"pyrope/05-assert/#test","title":"Test","text":"<p>Pyrope has the <code>test [message [,args]+] ( [stmts+] }</code>.</p> Many parallel testsSingle large test <pre><code>let add = fun(a,b) { a+b }\n\nfor a in 0..=20 {\n  for b in 0..=20 {\n    test \"checking add({},{})\", a,b {\n       cassert a+b == add(a,b)\n    }\n  }\n}\n</code></pre> <pre><code>let add = fun(a,b) { a+b }\n\ntest \"checking add\" {\n  for a in 0..=20 {\n    for b in 0..=20 {\n       cassert a+b == add(a,b)\n    }\n  }\n}\n</code></pre> <p>The <code>test</code> code block also accepts the keyword <code>step</code> that advances one clock cycle, and the test continues from that given point. This is useful for when a lambda is instantiated and we want to check/update the inputs/outputs.</p> <pre><code>let counter = proc(update)-&gt;(value) {\n  reg count:u8:[wrap] = 0\n\n  value = count\n\n  count += 1 when update\n}\n\ntest \"counter through several cycles\" {\n\n  var inp = true\n  let x = counter(inp.[defer])  // inp contents at the end of each cycle\n\n  assert x == 0 // x.value == 0\n  assert inp == true\n\n  step\n\n  assert x == 1\n  inp = false\n\n  step\n\n  assert x == 1\n  assert inp == false\n  inp = true\n\n  assert inp == true\n  assert x == 1\n\n  step\n\n  assert inp == true\n  assert x == 2\n}\n</code></pre> <p>During <code>test</code> simulation, all the assertions are checked but the test does not stop with a failure until the end. Sometimes it is useful to write tests to check that assertions fail. Assertion failures will be printed but the test will continue and fail only if the <code>assert.[failed]</code> is true. The <code>test</code> code block also accepts to read and/or clear failed attribute.</p> <pre><code>test \"assert should fail\" {\n\n let n = assert.[failed]\n assert n == false\n\n assert false // FAILS\n\n assert assert.[false]\n\n assert.[failed] = false // disable test failures done when it finishes\n}\n</code></pre>"},{"location":"pyrope/05-assert/#monitor","title":"Monitor","text":"<p>TODO</p>"},{"location":"pyrope/05b-statements/","title":"Statements","text":""},{"location":"pyrope/05b-statements/#conditional-ifelifelse","title":"Conditional (<code>if</code>/<code>elif</code>/<code>else</code>)","text":"<p>Pyrope uses a typical <code>if</code>, <code>elif</code>, <code>else</code> sequence found in most languages. Before the if starts, there is an optional keyword <code>unique</code> that enforces that a single condition is true in the if/elif chain. This is useful for synthesis which allows a parallel mux. The <code>unique</code> is a cleaner way to write an <code>optimize</code> statement.</p> <p>The <code>if</code> sequence can be used in expressions too.</p> <pre><code>a = unique if x1 == 1 {\n    300\n  }elif x2 == 2 {\n    400\n  }else{\n    500\n  }\n\nvar x = _\nif a { x = 3 } else { x = 4 }\n</code></pre> <p>The equivalent code with an explicit <code>optimize</code>, but unlike the <code>optimize</code>, the <code>unique</code> will guarantee to generate the <code>hotmux</code> statement.</p> <pre><code>optimize !(x1==1 and x2==2)\na = if x1 == 1 {\n    300\n  }elif x2 == 2 {\n    400\n  }else{\n    500\n  }\n</code></pre> <p>Like several modern programming languages, there can be a list of expressions in the evaluation condition. If variables are declared, they are restricted to the remaining if/else statement blocks.</p> <pre><code>var tmp = x+1\n\nif var x1=x+1; x1 == tmp {\n   puts \"x1:{} is the same as tmp:{}\", x1, tmp\n}elif var x2=x+2; x2 == tmp {\n   puts \"x1:{} != x2:{} == tmp:{}\", x1, x2, tmp\n}\n</code></pre>"},{"location":"pyrope/05b-statements/#unique-parallel-conditional-match","title":"Unique parallel conditional (<code>match</code>)","text":"<p>The <code>match</code> statement is similar to a chain of unique if/elif, like the <code>unique if/elif</code> sequence, one of the options in the match must be true. The difference is that one of the entries must be truth or an error is generated. This makes the <code>match</code> statement a replacement for the common \"unique parallel case\" Verilog directive. The <code>match</code> statement behaves like also having an <code>optimize</code> statement which allows for more efficient code generation than a sequence of <code>if/else</code>.</p> <p>In addition to functionality, the syntax is different to avoid redundancy. <code>match</code> joins the match expression with the beginning of the matching entry must form a valid expression.</p> <pre><code>x = 1\nmatch x {\n  == 1            { puts \"always true\" }\n  in 2,3          { puts \"never\"       }\n}\n// It is equivalent to:\nunique if x == 1  { puts \"always true\" }\nelif x in (2,3)   { puts \"never\"       }\nelse              { assert false       }\n</code></pre> <p>Like the <code>if</code>, it can also be used as an expression.</p> <pre><code>var hot = match x {\n    == 0sb001 { a }\n    == 0sb010 { b }\n    == 0sb100 { c }\n  }\n\n// Equivalent\noptimize (x==0sb001 or x==0sb010 or x==0sb100)\nvar hot2 = __hotmux(x, a, b, c)\n\nassert hot==hot2\n</code></pre> <p>Like the <code>if</code> statement, a sequence of statements and declarations are possible in the match statement.</p> <pre><code>match let one=1 ; one ++ (2) {\n  == (1,2) { puts \"one:{}\", one }      // should always hit\n}\n</code></pre> <p>Since the <code>==</code> is the most common condition in the <code>match</code> statement, it can be omitted.</p> <pre><code>for x in 1..=5 {\n  let v1 = match x {\n    3 { \"three\" }\n    4 { \"four\" }\n    else { \"neither\"}\n  }\n\n  let v2 = match x {\n    == 3 { \"three\" }\n    == 4 { \"four\" }\n    else { \"neither\"}\n  }\n  cassert v1 == v2\n}\n</code></pre>"},{"location":"pyrope/05b-statements/#gate-statements-whenunless","title":"Gate statements (<code>when</code>/<code>unless</code>)","text":"<p>A simple statement like assignments, variable declarations, and function calls and returns can be gated or not executed with a <code>when</code> or <code>unless</code> statement. This is similar to an <code>if</code> statement, but the difference is that the statement is in the current scope, not creating a new scope. This allows cleaner more compact syntax.</p> <pre><code>var a = 3\na += 1 when false             // never executes \nassert a == 3\nassert a == 1000 when a &gt; 10  // assert never executed either\n\nreg my = 3 when some_condition  // no register declared otherwise\n\nreturn \"fail\" unless success_condition\n</code></pre> <p>Complex assignments like <code>a |&gt; b(1) |&gt; c</code> can not be gated because it is not clear if the gated applies to the last call or the whole pipeline sequence. Similarly, gating ifs/match statements do not make much sense. As a result, <code>when</code>/<code>unless</code> can only be applied to assignments, function calls, and code block control statements (<code>return</code>, <code>break</code>, <code>continue</code>).</p>"},{"location":"pyrope/05b-statements/#code-block","title":"Code block","text":"<p>A code block is a sequence of statements delimited by <code>{</code> and <code>}</code>. The functionality is the same as in other languages. Variables declared within a code block are not visible outside the code block. In other words, code block variables have scope from definition until the end of the code block.</p> <p>Code blocks are different from lambdas. A lambda consists of a code block but it has several differences. In lambdas, (1) variables defined in upper scopes are accessed inside as immutable copies only when captured by scope; (2) inputs and outputs could be constrained, and (3) the <code>return</code> statement finishes a lambda not a code block.</p> <p>The main features of code blocks:</p> <ul> <li> <p>Code blocks define a new scope. New variable declarations inside are not visible outside it. </p> </li> <li> <p>Code blocks do not allow variable declaration shadowing.</p> </li> <li> <p>Expressions can have multiple code blocks but they are not allowed to have   side-effects for variables outside the code block. The evaluation   order provides more details on expressions   evaluation order.</p> </li> <li> <p>When used in an expression or lambda, the last statement in the lambda code   block can be an expression. It is not needed to add the <code>return</code> keyword in   this case.</p> </li> </ul> <pre><code>{\n  var x=1\n  var z=_\n  {\n    z = 10\n    var x=_           // compiler error, 'x' is a shawdow variable\n  }\n  assert z == 10 \n}\nlet zz = x            // compile error, `x` is out of scope\n\nvar yy = {let x=3 ; 33/3} + 1\nassert yy == 12\nlet xx = {yy=1 ; 33}  // compile error, 'yy' has side effects\n\nif {let a=1+yy; 13&lt;a} {\n  // a is not visible in this scope\n  some_code()\n}\n\nlet doit = fun(f,a) {\n  let x = f(a)\n  assert x == 7\n  return 3\n}\n\nlet z3 = doit(fun(a) { \n  assert a!=0\n  return 7             // exist the current lambda\n  100                  // never reached statement\n}, 33)\ncassert z3 == 3\n</code></pre>"},{"location":"pyrope/05b-statements/#loop-for","title":"Loop (<code>for</code>)","text":"<p>The <code>for</code> iterates over the first-level elements in a tuple or the values in a range.  In all the cases, the number of loop iterations must be known at compile time. The loop exit condition can not be run-time data-dependent.</p> <p>The loop can have an early exit when calling <code>break</code> and skip of the current iteration with the <code>continue</code> keyword.</p> <pre><code>for i in 0..&lt;100 {\n some_code(i)\n}\n\nvar bund = (1,2,3,4)\nfor (index,i) in bund.enumerate() {\n  assert bund[j] == i\n}\n</code></pre> <pre><code>let b = (a=1,b=3,c=5,7,11)\nassert b.keys() == ('a', 'b', 'c', '', '')\nassert b.enumerate() == ((0,1), (1,3), (2,5), (3,7), (4,11))\nlet xx= zip(b.keys(), b.enumerate()) \ncassert xx == (('a',0,a=1), ('b',1,b=3), ('c',2,c=5), ('',3,7), ('',4,11))\n\nfor (key,index,i) in zip(keys(b),b.enumerate()) {\n  assert i==1  implies (index==0 and key == 'a')\n  assert i==3  implies (index==1 and key == 'b')\n  assert i==5  implies (index==2 and key == 'c')\n  assert i==7  implies (index==3 and key == '' )\n  assert i==11 implies (index==4 and key == '' )\n}\n\nlet c = ((1,a=3), b=4, c=(x=1,y=6))\nassert c.enumerate() == ((0,(1,a=3)), (1,b=4), (2,c=(x=1,y=6)))\n</code></pre> <p>The <code>for</code> can also be used in an expression that allows building comprehensions to initialize arrays. Pyrope uses a comprehension similar to Julia or Python.</p> <pre><code>var c = for i in 1..&lt;5 { var xx = i }  // compile error, no expression\nvar d = i for i in 0..&lt;5 \nvar e = i for i in 0..&lt;5 if i\nassert (0,1,2,3,4) == d\nassert e == (1,2,3,4)\n</code></pre> <p>The iterating element is copied by value, if the intention is to iterate over a vector or array to modify the contents, a <code>ref</code> must be used. Only the element is mutable. When a <code>ref</code> is used, it must be a variable reference, not a function call return (value). The mutable for can not be used in comprehensions.</p> <pre><code>b = (1,2,3,4,5)\n\nfor x in ref b {\n  x += 1\n}\nassert b == (2,3,4,5,6)\n</code></pre>"},{"location":"pyrope/05b-statements/#code-block-control","title":"Code block control","text":"<p>Code block control statements allow changing the control flow for <code>lambdas</code> and loop statements (<code>for</code>, <code>loop</code>, and <code>while</code>). <code>return</code> can have a value.</p> <ul> <li> <p><code>return</code> exits or terminates the current lambda. The current output variables   are provided as the <code>lambda</code> output. If a tuple is provided, the tuple is the   returned value, the output variables are not used.</p> </li> <li> <p><code>break</code> terminates the closest inner loop (<code>for</code>/<code>while</code>/<code>loop</code>). If none is   found, a compile error is generated.</p> </li> <li> <p><code>continue</code> looks for the closest inner loop (<code>for</code>/<code>while</code>/<code>loop</code>) code   block. The <code>continue</code> will perform the next loop iteration. If no inner loop   is found, a compile error is generated.</p> </li> </ul> <pre><code>var total:[] = _\nfor a in 1..=10 {\n  continue when a == 2\n  total ++= a\n  break when a == 3    // exit for scope\n}\nassert total == (1,3)\n\nif true {\n  code(x)\n  continue             // compile error, no upper loop scope\n}\n\na = 3\nvar total2:[] = _\nwhile a&gt;0 {\n  total2 ++= a\n  break when a == 2    // exit if scope\n  a = a - 1\n  continue\n  assert false         // never executed\n}\nassert total2 == (3,2)\n\ntotal = i+10 for i in 1..=9 if i&lt;3\nassert total == (11, 12)\n</code></pre>"},{"location":"pyrope/05b-statements/#whileloop","title":"while/loop","text":"<p><code>while cond { [stmts]+ }</code> is a typical while loop found in most programming languages. The only difference is that like with loops, the while must be fully unrolled at compilation time. The <code>loop { [stmts]+ }</code> is equivalent to a <code>while true { [stmts]+ }</code>.</p> <p>Like <code>if</code>/<code>match</code>, the <code>while</code> condition can have a sequence of statements with variable declarations visible only inside the while statements.</p> <pre><code>// a do while contruct does not exist, but a loop is quite clean/close\n\nvar a = 0\nloop {\n  puts \"a:{}\",a\n\n  a += 1\n\n  break unless a &lt; 10 \n} // do{ ... }while(a&lt;10)\n</code></pre>"},{"location":"pyrope/05b-statements/#defer","title":"defer","text":"<p>A <code>defer</code> attribute can be applied to variables. When used to read a variable, it returns the last values written to the variable the end of the current cycle. This is needed if we need to have any loop in connecting blocks. The <code>defer</code> applied to a write, delays the write update to the end of the cycle. The delayed writes happen before the delayed reads. This is also for delaying assertion checks to the end of the cycle like post condition checks.</p> <pre><code>var c = 10\nassert b.[defer] == 33    // behaves like a postcondition\nb = c.[defer]\nassert b == 33\nc += 20\nc += 3\n</code></pre> <p>To connect the <code>ring</code> function calls in a loop. <pre><code>f1 = ring(a, f4.[defer])\nf2 = ring(b, f1)\nf3 = ring(c, f2)\nf4 = ring(d, f3)\n</code></pre></p> <p>If the intention is to read the result after being a flop, there is no need to use the <code>defer</code>, a normal register access could do it. If the read variables are registers, the <code>flop#[0]</code> is not the same as <code>defer</code>. The <code>flop#[0]</code> reads the value before any update, the <code>defer</code> read, gets values after updates.</p> <pre><code>reg counter:u32 = _\n\nlet counter_m1 = counter#[1]  // compile error, #[1] only allowed for debug\nlet counter_0  = counter#[0]  // current cycle \nlet counter_1  = counter#[-1] // last cycle\nlet counter_2  = counter#[-2] // last last cycle cycle \n\nvar deferred = counter.[defer]\n\nif counter &lt; 100 {\n  counter += 1\n}else{\n  counter = 0\n}\n\nif counter == 10 {\n  assert deferred   == 10\n  assert counter_0  ==  9\n  assert counter_1  ==  8\n  assert counter_2  ==  7\n}\n</code></pre> <p>The <code>defer</code> can also be applied to write/updates to the end of the cycle but uses/reads the current value. In a way, the assignment is delayed to the end of the current cycle. If there are many defers to the same variable, they are ordered in program order.</p> <pre><code>var a = 1\nassert a == 1 and a.[defer] == 200\n\na::[defer] = 100\nassert a == 1 and a.[defer] == 200\n\na::[defer] = 200\nassert a == 1 and a.[defer] == 200\n</code></pre> <p>If there are <code>defer</code> reads and <code>defer</code> assignments, the defered writes are performed before the defered reads.</p> <pre><code>var a = 1\nvar x = 100\nx::[defer] = a\na = 200\n\ncassert x == 100\nassert x.[defer] == 1\n</code></pre>"},{"location":"pyrope/05b-statements/#testing-test","title":"Testing (<code>test</code>)","text":"<p>The test statement requires a text identifier to notify when the test fails. The <code>test</code> is similar to a <code>puts</code> statement followed by a scope (<code>test &lt;str&gt; [,args] { stmts+ }</code>). The statements inside the code block can not have any effect outside. </p> <pre><code>test \"my test {}\", 1 {\n  assert true\n}\n</code></pre> <p>Each <code>test</code> can run in parallel, to increase the throughput, putting the randomization outside the test statement increases the number of tests:</p> Parallel testsSingle test <pre><code>let add = fun(a,b) { a+b }\n\nfor i in 0..&lt;10 { // 10 tests\n  let a = (-30..&lt;100).rand\n  let b = (-30..&lt;100).rand\n\n  test \"test {}+{}\",a,b {\n    assert add(a,b) == (a+b)\n  }\n}\n</code></pre> <pre><code>let add = fun(a,b) { a+b }\n\ntest \"test 10 additions\" {\n  for i in 0..&lt;10 { // 10 tests\n    let a = (-30..&lt;100).rand\n    let b = (-30..&lt;100).rand\n\n    assert add(a,b) == (a+b)\n  }\n}\n</code></pre>"},{"location":"pyrope/05b-statements/#test-only-statements","title":"Test only statements","text":"<p><code>test</code> code blocks are allowed to use special statements not available outside testing blocks:</p> <ul> <li> <p><code>step [ncycles]</code> advances the simulation for several cycles. The local variables will preserve the value, the inputs may change value.</p> </li> <li> <p><code>waitfor condition</code> is a syntax sugar to wait for a condition to be true.</p> </li> </ul> <code>step</code>synthesizable equivalent <pre><code>test \"wait 1 cycle\" {\n  let a = 1 + input\n  puts \"printed every cycle input={}\", a\n  step 1\n  puts \"also every cycle a={}\",a  // printed on cycle later\n}\n</code></pre> <pre><code>test \"wait 1 cycle\" {\n  {\n    let a = 1 + input\n    puts \"printed every cycle input={}\", a\n  } #&gt; {\n    puts \"also every cycle a={}\",a  // printed on cycle later\n  }\n}\n</code></pre> <p>The <code>waitfor</code> command is equivalent to a <code>while</code> with a <code>step</code>.</p> <code>waitfor</code>equivalent Pyrope <pre><code>total = 3\n\nwaitfor a_cond  // wait until a_cond is true\n\nassert total == 3 and a_cond\n</code></pre> <pre><code>total = 3\n\nwhile !a_cond {\n  step\n}\n\nassert total == 3 and a_cond\n</code></pre> <p>The main reason for using the <code>step</code> is that the \"equivalent\" <code>#&gt;[1]</code> is a more structured construct. The <code>step</code> behaves more like a \"yield\" in that the next call or cycle it will continue from there. The <code>#&gt;[1]</code> directive adds a pipeline structure which means that it can be started each cycle. Calling a lambda that has called a <code>step</code> and still has not finished should result in a simulation assertion failure.</p> <ul> <li> <p><code>peek</code> allows to read any flop, and lambda input or output</p> </li> <li> <p><code>poke</code> is similar to <code>peek</code> but allows to set a value on any flop and lambda   input/output.</p> </li> </ul>"},{"location":"pyrope/06-functions/","title":"Lambdas","text":"<p>A <code>lambda</code> consists of a sequence of statements that can be bound to a variable. The variable can be copied and called as needed. Unlike most languages, Pyrope only supports anonymous lambdas. The reason is that without it lambdas would be assigned to a namespace. Supporting namespaces would avoid aliases across libraries, but Pyrope allows different versions of the same library at different parts of the project. This will effectively create a namespace alias. The solution is to not have namespaces but relies upon variable scope to decide which lambda to call.</p> <p>Observation</p> <p>Allowing multiple version of the same library/code is supported by Pyrope. It looks like a strange feature from a software point of view, but it is common in hardware to have different blocks designed/verified at different times. The team may not want to open and modernize a block. In hardware, it is also common to have different blocks to be compiled with different compiler versions. These are features that Pyrope enables.</p> <p>Pyrope divides the lambdas into two categories: <code>functions</code> and <code>procedures</code>. Functions operate only over combinational logic. They can not have any synthesis side-effect. This means the function outputs are only a function of the function inputs. Any external call can only affect <code>debug</code> statements not the synthesizable code. <code>functions</code> resemble <code>pure functions</code> in normal programming languages. In pure functions, the function results depend only on the input parameters. In Pyrope, they are allowed to have side effects on debug code (non-synthesizable).</p> <p>Non-function lambdas are called <code>procedures</code> or <code>methods</code>. The only difference between <code>procedures</code> and <code>methods</code> is that a <code>method</code> has <code>self</code> as the first argument in the output which allows to mutable the called tuple.</p> <p>Lambdas also can be divided into <code>modules</code> and non-<code>modules</code>. A <code>module</code> is a lambda visible at synthesis call hierarchy. A <code>non-module</code> is an inlined or flattened <code>lambda</code>.</p> <p><code>functions</code> are combinational logic, but <code>procedures</code> can have inputs and/or outputs registerd or just be a generic pure combinational function.</p> Combinational (fun)Combinational (proc)Inputs Registerd (proc)Outputs Registerd (proc) <pre><code>let add=fun(a,b)-&gt;(res) {\n  res = a+b\n}\n\nfun add(a,b)-&gt;(res) {  // Same as let add=fun(a,b)-&gt;(res)\n  res = a+b\n}\n</code></pre> <pre><code>let add=proc(a,b)-&gt;(res) {  // nicer to use fun, but proc works\n  res = a+b\n}\n\nproc add(a,b)-&gt;(res) {  // same\n  res = a+b\n}\n</code></pre> <pre><code>let add=proc(reg a, reg b)-&gt;(res) {\n  res = a+b\n}\n\nproc add(reg a, reg b)-&gt;(res) { // same\n  res = a+b\n}\n</code></pre> <pre><code>let add=proc(a, b)-&gt;(reg res) {\n  res = a+b\n}\n\nproc add(a, b)-&gt;(reg res) { // same\n  res = a+b\n}\n</code></pre>"},{"location":"pyrope/06-functions/#declaration","title":"Declaration","text":"<p>Only anonymous lambdas are supported, this means that there is no global scope for functions, procedures, or modules. The only way for a file to access a lambda is to have access to a local variable with a definition or to \"import\" a variable from another file. The more familiar <code>fun name</code> or <code>proc name</code> declaration is also valid, but it is syntax sugar and equivalent to <code>let name = fun</code>.</p> <pre><code>let a_3   = {   3 }      // just scope, not a lambda. Scope is evaluate now\nlet a_fun = fun() { 4 }  // when a_fun is called 4 is returned\n\nlet fun3 = fun(){ 5 }    // public lambda that can be imported by other files\n\nlet x = a_3()            // compile error, explicit call not posible in scope\nlet x = a_fun()          // OK, explicit call needed when no arguments\n\nassert a_3() equals 3\nassert a_fun equals _:fun()\nassert a_fun() equals 4\nassert a_fun() == 4      // calls to eval the function\n</code></pre> <p>The lambda definition has the following fields:</p> <pre><code>[GENERIC] [CAPTURE] [INPUT] [-&gt; OUTPUT] [where COND] |\n</code></pre> <ul> <li> <p><code>GENERIC</code> is an optional comma separated list of names between <code>&lt;</code> and <code>&gt;</code> to   use as generic types in the lambda.</p> </li> <li> <p><code>CAPTURE</code> has the list of capture variables for the lambda. If no capture is   provided, no local variable can be captured by value which is equivalent to   an empty list (<code>[]</code>), The captures are by value only, no capture by reference   is allowed. Unlike most languages, capture must be comptime. Section   Closures has more details.</p> </li> <li> <p><code>INPUT</code> has a list of inputs allowed with optional types. <code>()</code> indicates no   inputs. <code>(...args)</code> allow to accept a variable number of arguments.</p> </li> <li> <p><code>OUTPUT</code> has a list of outputs allowed with optional types. <code>()</code> indicates no   outputs.</p> </li> <li> <p><code>COND</code> is the condition under which this statement is valid. The <code>COND</code> can   use the inputs, outputs, and <code>self</code> to evaluate. If the outputs are used in   the <code>COND</code>, the lambda must be immutable (<code>fun</code>). This means that the method   is called when the condition could evaluate true depending on its execution,   but being immutable there are no side effects. Section   overload has more details.</p> </li> </ul> <pre><code>var add:fun(...x) = _\nadd = fun(...x) { x.0+x.1+x.2 }      // no IO specified\nadd = fun(a,b,c){ a+b+c }            // constrain inputs to a,b,c\nadd = fun(a,b,c){ a+b+c }            // same\nadd = fun(a:u32,b:s3,c){ a+b+c }     // constrain some input types\nadd = fun(a,b,c) -&gt; (x:u32){ a+b+c } // constrain result to u32\nadd = fun(a,b,c) -&gt; (res){ a+b+c }   // constrain result to be named res\nadd = fun(a,b:a,c:a){ a+b+c }        // constrain inputs to have same type\nadd = fun&lt;T&gt;(a:T,b:T,c:T){ a+b+c }   // same\n\nx = 2\nvar add2:fun2(a) = _\nadd2 = fun       (a){   x + a }    // compile error, undefined 'x'\nadd2 = fun[     ](a){   x + a }    // compile error, undefined 'x'\nadd2 = fun[x    ](a){   x + a }    // explicit capture x\nadd2 = fun[foo=x](a){ foo + a }    // capture x but rename to something else\n\nvar y = (\n  ,val:u32 = 1\n  ,inc1 = fun (ref self) { self.val = u32(self.val + 1) }\n)\n\nlet my_log::[debug] = fun (...inp) {\n  print \"loging:\"\n  for i in inp {\n    print \" {}\", i\n  }\n  puts\n}\n\nlet f = fun&lt;X&gt;(a:X,b:X){ a+b }   // enforces a and b with same type\nassert f(33:u22,100:u22)\n\nmy_log a, false, x+1\n</code></pre>"},{"location":"pyrope/06-functions/#argument-naming","title":"Argument naming","text":"<p>Input arguments must be named. E.g: <code>fcall(a=2,b=3)</code> There are the following exceptions that avoid naming arguments:</p> <ul> <li> <p>If the type system can distinguish between unnamed arguments (no ambiguity)</p> </li> <li> <p>If there is an argument/call match. The calling variable name has the same as an argument</p> </li> <li> <p>If the argument is a single letter, and there is no name match, only position is used</p> </li> <li> <p><code>self</code> does not need to be named (first argument position)</p> </li> </ul> <p>There are several rules on how to handle arguments.</p> <ul> <li> <p>Calls use the Uniform Function Call Syntax (UFCS) but only when <code>self</code> is defined as   first argument. <code>(a,b).f(x,y) == f((a,b),x,y)</code></p> </li> <li> <p>Pipe <code>|&gt;</code> concatenated inputs: <code>(a,b) |&gt; f(x,y) == f(x,y,a,b)</code></p> </li> <li> <p>Function calls with arguments do not need parenthesis after newline or a   variable assignment: <code>a = f(x,y)</code> is the same as <code>a = f x,y</code></p> </li> <li> <p>Functions without arguments, need explicit parenthesis in function call.</p> </li> </ul> <p>Pyrope uses a Uniform Function Call Syntax (UFCS) when the first argument is <code>self</code>. It resembles Nim or D UFCS but it can be different from the order in other languages. Notice the different order in UFCS vs pipe, and also that in the pipe the argument tuple is concatenated.</p> <pre><code>let div  = fun (self,b) { self / b }  // named input tuple\nlet div2 = fun (...x){ x.0 / x.1 }    // unnamed input tuple\n\nlet noarg = fun () { 33 }         // explicit no args\n\nassert 33 == noarg()              // () needed to call\n\nassert noarg // compile error, `noarg()` needed for calls without arguments\n\na=div(3  , 4  , 3)       // compile error, div has 2 inputs\nb=div(self=8, b=4)       // OK, 2\nc=div self=8, b=4        // compile error, parenthesis needed for complex call\nd=(self=8).div(b=2)      // OK, 4\nd=(8).div(b=2)           // OK, 4 . self does not need to be named\nd=8.div(2)               // OK, single character inputs no need to be named\ne=(self=8).div b=2       // compile error, parenthesis needed for complex call\n\nh=div2(8, 4, 3)          // OK, 2 (3rd arg is not used)\ni=8.div2(4,3)            // compile error, no self in div2\n\nj=(8,4)  |&gt; div2         // OK, 2, same as div2(8,4)\nj=(8,4)  |&gt; div2()       // OK, 2, same as div2(8,4)\nk=(4)    |&gt; div2(8)      // OK, 2, same as div2(8,4)\nl=(4,33) |&gt; div2(8)      // OK, 2, same as div2(8,4,33)\nm=4      |&gt; div2 8       // compile error, parenthesis needed for complex call\n\nn=div((8,4), 3)          // compile error: (8,4)/3 is undefined\no=(8,4).div2(1)          // compile error: (8,4)/1 is undefined\n</code></pre> <p>The UFCS allows to have <code>lambdas</code> to call any tuple, but if the called tuple has a lambda defined with the same name a compile error is generated. Like with variables, Pyrope does not allow <code>lambda</code> call shadowing. Polymorphism is allowed but only explicit one as explained later.</p> <pre><code>var tup = (\n  ,let f1 = fun(self) { 1 }\n)\n\nlet f1 = fun (self){ 2 }   // compile error, f1 shadows tup.f1\nlet f1 = fun (){ 3 }       // OK, no\n\nassert f1()         != 0  // compile error, missing argument\nassert f1(tup)      != 0  // compile error, f1 shadowing (tup.f1 and f1)\nassert 4.f1()       != 0  // compile error, f1 can be called for tup, so shadow\nassert tup.f1()     != 0  // compile error, f1 is shadowing\n\nlet xx = fun[tup] { tup.f1() } // OK, function restricted scope for f1\nassert xx()\n\nassert (4:tup).f1() == 1\nassert 4.f2()       == 3  // UFCS call\nassert tup.f1()     == 1\n</code></pre> <p>The keyword <code>self</code> is used to indicate that the function is accessing a tuple. <code>self</code> is required to be the first argument. If the procedure modifies the tuple contents, a <code>ref self</code> must be passed as input.</p> <pre><code>var tup2 = (\n  ,val:u8 = _\n  ,upd = proc(ref self) { self.val::[saturate] += 1 }\n  ,calc = fun(self) { self.val}\n)\n</code></pre> <p>A lambda call uses parenthesis (<code>foo() or foo(1,2)</code>). The parenthesis can be avoid in tree conditions: (1) arguments are passed in a simple function call statement; (2) after a pipeline directive; (3) the variable has a getter method (<code>get</code>).</p> <pre><code>no_arg_fun()     // must use explicit parenthesis/called\narg_fun 1,2      // parenthesis are optional\narg_fun(1,2)     // OK too\n(1,2) |&gt; arg_fun // OK too, it is after |&gt;\n\nvar intercepted:(\n ,field:u32\n ,getter=fun(self) { self.field + 1 }\n ,setter=fun(ref self,v ) { self.field = v }\n) = 0\n\ncassert intercepted == 1  // will call get method without explicit call\ncassert intercepted.field == 0\n</code></pre>"},{"location":"pyrope/06-functions/#pass-by-reference","title":"Pass by reference","text":"<p>Pyrope is an HDL, and as such, there are not memory allocation issues. This means that all the arguments are pass by value and the language has value semantics. In other words, there is not need to worry about ownership or move/forward semantics like in C++/Rust. All the arguments are always by value. Nevertheless, sometimes is useful to pass a reference to an array/register so that it can be updated/accessed on different lambdas.</p> <p>Pyrope arguments are by value, unless the <code>ref</code> keyword is used. Pass by reference is needed to avoid the copy by value of the function call. Unlike non-hardware languages, there is no performance overhead in passing by value. The reason for passing as reference is to allow the lambda to operate over the passed argument. If modified, it behaves like if it were an implicit output. This is quite useful for large objects like memories to avoid the copy.</p> <p>The pass by reference behaves like if the calling lambda were inlined in the caller lambda while still respecting the lambda scope. The <code>ref</code> keyword must be explicit in the lambda input definition but also in the lambda call. The lambda outputs can not have a <code>ref</code> modifier.</p> <p>No logical or arithmetic operation can be done with a <code>ref</code>. As a result, it is only useful for lambda input arguments.</p> <pre><code>let inc1 = fun(ref a) { a += 1 }\n\nlet x = 3\ninc1(ref x)       // compile error, `x` is immutable but modified inside inc1\n\nvar y = 3\ninc1(ref y)\nassert y == 4\n\nlet banner = fun() { puts \"hello\"  }\nlet execute_method = fun(fn:fun()-&gt;()) {  // example with explicit type for fn\n  fn() // prints hello when banner passed as argument\n}\n\nexecute_method(banner)     // OK\n</code></pre> <p>In Pyrope, to call a method, parenthesis are needed only when the method has arguments. This is needed to distinguish for higher order functions that need to distinguish between a function call and a pass of the lambda.</p>"},{"location":"pyrope/06-functions/#output-tuple","title":"Output tuple","text":"<p>Pyrope everything is a tuple, even the output or return from a lambda. When a single element is returned, it can be an unnamed tuple by omiting parenthesis.</p> <pre><code>let ret1 = fun()-&gt;(a:int) { // named\n  a = 1\n}\n\nlet ret2 = fun()-&gt;a:int {   // unnamed\n  a = 2\n}\n\nlet ret3 = fun()-&gt;(a,b) {   // named\n  a = 3\n  b = 4\n}\n\nlet a1 = ret1()\nassert a1.a == 1 // NOT a1 == 1\n\nlet a2 = ret2()\nassert a2 == 1   // NOT a2.a == 1\n\nlet a3 = ret3()\nassert a3.a == 3 and a2.b == 4\n\nlet (x1,x2) = ret3()\nassert x1   == 3 and x2   == 4\n</code></pre>"},{"location":"pyrope/06-functions/#attributes","title":"Attributes","text":"<p>Variables can have attributes, but <code>procedures</code> can also have them. Procedure attributes have only one direction from inside the method to outside/caller. They can be used to signal out of band information about the procedude. Attributes can only be <code>integer</code>, <code>bool</code>, or <code>string</code>. Depending on the type, they are initialized to <code>0</code>, <code>false</code>, or <code>\"\"</code>.</p> <p>The procedure attribute is stored in the variable that keeps the lambda. This means that it can be checked before or after the lambda call, and that different variables can point to the same procedure but keep different attributes.</p> <pre><code>let p1 = proc(a)-&gt;(res) {\n  self.[my_zero_found] or= (a == 0)\n\n  res = a + 1\n}\n\nlet p2 = p1      // copy\nlet p3 = ref p1  // reference\n\ntest \"testing p1\" {\n  assert p1.[my_zero_found] == false\n  assert p2.[my_zero_found] == false\n\n  cassert p1(3) == 4\n  assert p1.[my_zero_found] == false\n\n  cassert p1(0) == 1\n  assert p1.my_zero_found == true\n\n  cassert p1(50) == 51\n  assert p1.[my_zero_found] == true\n  assert p2.[my_zero_found] == false\n  assert p3.[my_zero_found] == true\n}\n</code></pre>"},{"location":"pyrope/06-functions/#methods","title":"Methods","text":"<p>Pyrope arguments are by value, unless the <code>ref</code> keyword is used. <code>ref</code> is needed when a method intends to update the tuple contents. In this case, <code>ref self</code> argument behaves like a pass by reference in non-hardware languages. This means that the tuple fields are updated as the method executes, it does not wait until the method finishes execution. A method without the <code>ref</code> keyword is a pass by value call. Since all the inputs are immutable by default (<code>let</code>), any <code>self</code> updates should generate a compile error.</p> <pre><code>let Nested_call = (\n  ,var x = 1\n  ,let outter= proc(ref self) {  self.x = 100 ; self.inner(); self.x = 5 }\n  ,let inner = fun(self) { assert self.x == 100 }\n  ,let faulty = proc(self) { self.x = 55 } // compile error, immutable self\n  ,proc okcall(ref self) { self.x = 55 }   // equivalent to let okcall=proc\n)\n</code></pre> <p><code>self</code> can also be returned but this behaves like a normal copy by value variable return.</p> <pre><code>var a_1 = (\n  ,x:u10\n  ,let f1 = fun(ref self,x)-&gt;(self) { // BOTH ref self and return self is OK\n    self.x = x\n    self\n  }\n)\n\na_1.f1(3)\nvar a_2 = a_1.f1(4)  // a_2 is updated, not a_1\nassert a_1.x == 3 and a_2.x == 4\n\n// Same behavior as in a function with UFCS\nfun2 = fun (ref self, x) { self.x = x }\n\na_1.fun2(10)\nvar a_3 = a_1.fun2(20)\nassert a_1 == 10 and a_3 == 20\n</code></pre> <p>Since UFCS does not allow shadowing, a wrapper must be built or a compile error is generated.</p> <pre><code>var counter = (\n  ,var val:i32 = 0\n  ,let inc = fun (ref self, v){ self.var += v }\n)\n\nassert counter.val == 0\ncounter.inc(3)\nassert counter.val == 3\n\nlet inc = fun (ref self, v) { self.var *= v } // NOT INC but multiply\ncounter.inc(2)             // compile error, multiple inc options\nassert 44.inc(2) == 8\n\ncounter.val = 5\nlet mul = inc\ncounter.mul(2)             // call the new mul method with UFCS\nassert counter.val == 10\n\nmul(counter, 2)            // also legal\nassert counter.val == 20\n</code></pre> <p>It is possible to add new methods after the type declaration. In some languages, this is called extension functions.</p> <pre><code>let t1 = (a:u32)\n\nvar x:t1 = (a=3)\n\nt1.double = proc(ref self) { self.a *= 2 }  // extension function\n// previous is exactly the same as:\n// t1 = t1 ++ (double = proc(ref self) { self.a *= 2 })\n\nvar y:t1 = (a=3)\nx.double             // compile error, double method does not exit\ny.double             // OK\nassert y.a == 6\n</code></pre>"},{"location":"pyrope/06-functions/#constraining-arguments","title":"Constraining arguments","text":"<p>Arguments can constrain the inputs and input types. Unconstrained input types allow for more freedom and a potentially variable number of arguments generics, but it can be error-prone.</p> unconstrained declarationconstrained declaration <pre><code>foo = fun (self) { puts \"fun.foo\" }\na = (\n  ,foo = fun () {\n     bar = fun() { puts \"bar\" }\n     puts \"mem.foo\"\n     return (bar=bar)\n  }\n)\nb = 3\nc = \"string\"\n\nb.foo         // prints \"fun.foo\"\nb.foo()       // prints \"fun.foo\"\nx = a.foo     // prints \"mem.foo\"\ny = a.foo()   // prints \"mem.foo\"\nx()           // prints \"bar\"\n\na.foo.bar()   // prints \"mem.foo\" and then \"bar\"\na.foo().bar() // prints \"mem.foo\" and then \"bar\"\na.foo().bar   // prints \"mem.foo\" and then \"bar\"\n\nc.foo         // prints \"fun.foo\"\n</code></pre> <pre><code>foo = fun (self:int) { puts \"fun.foo\" }\na = (\n  ,foo = fun () {\n     bar = fun() { puts \"bar\" }\n     puts \"mem.foo\"\n     return (bar=bar)\n  }\n)\nb = 3\nc = \"string\"\n\nb.foo         // prints \"fun.foo\"\nb.foo()       // prints \"fun.foo\"\nx = a.foo     // prints \"mem.foo\"\ny = a.foo()   // prints \"mem.foo\"\nx()           // prints \"bar\"\n\na.foo.bar()   // prints \"mem.foo\" and then \"bar\"\na.foo().bar() // prints \"mem.foo\" and then \"bar\"\na.foo().bar   // prints \"mem.foo\" and then \"bar\"\n\nc.foo         // compile error, undefined 'foo' field/call\n</code></pre> <p>The <code>where</code> statement also allows to constrain arguments. This is a sample of fibonnaci implementation with and without <code>where</code> clauses. Section overload has more details on the method overloading.</p> <pre><code>let fib1 = fun(n) where n==0 {0}\n        ++ fun(n) where n==1 {1}\n        ++ fun(n)            { fib1(n-1) + fib1(n-2) }\n\nassert fib1(10) == 55\n\nlet fib2 = fun(n) {\n  return match n {\n    == 0 {0}\n    == 1 {1}\n    else {fib2(n-1) + fib2(n-2)}\n  }\n}\n\nassert fib2(10) == 55\n</code></pre>"},{"location":"pyrope/06b-instantiation/","title":"Instantiation","text":"<p>Instantiation is the process of translating from Pyrope to an equivalent set of gates. The gates could be simplified or further optimized by later compiler passes or optimization steps. This section provides an overview of how the major Pyrope syntax constructs translate to gates.</p>"},{"location":"pyrope/06b-instantiation/#conditionals","title":"Conditionals","text":"<p>Conditional statements like <code>if/else</code> and <code>match</code> translate to multiplexers (muxes).</p> <p>A trivial <code>if/else</code> with all the options covered is a simple mux.</p> <pre><code>var res:s4 = _\nif cond {\n  res = a\n}else{\n  res = b\n}\n\n// RTL equivalent (bus of 4 bits in a,b,res2)\nvar res2:s4 = __mux(cond,b,a)\n\nlec res, res2\n</code></pre> <p>An expression <code>if/else</code> is also a mux.</p> <pre><code>var res = if cond { a }else{ b }\n\n// RTL equivalent\nvar res2 = __mux(cond,b,a)\n\nlec res, res2\n</code></pre> <p>The <code>when/unless</code> is also a mux.</p> <pre><code>var res = a\nres = b unless cond\n\n// RTL equivalent\nvar res2 = __mux(cond,b,a)\n\nlec res, res2\n</code></pre> <p>Chaining <code>if</code>/<code>elif</code> creates a chain of muxes. If not all the inputs are covered, the value from before the <code>if</code> is used. If the variable did not exist, a compile error is generated.</p> <pre><code>var res = a\nif cond1 {\n  res = b\n}elif cond2 {\n  res = c\n}else{\n  assert true // no res\n}\n\n// RTL equivalent\nvar tmp = __mux(cond2, a, c)\nvar res2= __mux(cond1, tmp, b)\n\nlec res, res2\n</code></pre> <p><code>unique if</code>/<code>elif</code> is similar but avoids mux nesting using a one-hot encoded mux. </p> <pre><code>var res = a\nunique if cond1 {\n  res = b\n}elif cond2 {\n  res = c\n} // no res in else\n\n// RTL equivalent\nvar sel = (!cond1 and !cond2, cond1, cond2)@[..]  // one hot encode\nvar res2= __hotmux(sel, a, b, c)\noptimize !(cond1 and cond2)                       // one hot check\n\nlec res, res2\n</code></pre> <p>The <code>match</code> is similar to the <code>unique if</code> but also checks that one of the options is enabled, which allows further optimizations. From a Verilog designer point of view, the <code>match</code> is a \"full parallel\" and the <code>unique if</code> is a \"parallel\". Both are checked at verification and optimized at synthesis.</p> <pre><code>var res = a\nmatch x {\n  == c1 { res = b }\n  == c2 { res = c }\n  == c3 { res = d }\n}\n\n// RTL equivalent\nlet cond1 = x == c1\nlet cond2 = x == c2\nlet cond3 = x == c3\nvar sel = (cond1, cond2, !cond1 and !cond2)@[..]  // one hot encode (no cond3)\nvar res2= __hotmux(sel, b, c, d)\noptimize ( cond1 and !cond2 and !cond3)\n      or (!cond1 and  cond2 and !cond3)\n      or (!cond1 and !cond2 and  cond3)    // one hot check (no else allowed)\n\nlec res, res2\n</code></pre>"},{"location":"pyrope/06b-instantiation/#optional-expression","title":"Optional expression","text":"<p>Valid or optionals are computed for each assignment and passed to every lambda call. Each variable has an associated valid bit, but it is removed if never read, and it is always true unless the variables are assigned in conditionals or non-short-circuit (<code>and_then</code>/<code>or_else</code>) expressions.</p> Short-circuit expressionUsual expressionConditionalsLambda call (inlined) <pre><code>var lhs = v1 or_else v2\n\n// RTL equivalent\nlet lhs2  = __or(v1, v2)\nlet lhs2_v = __or(__and(v1?, v1), __and(v2?, v2))\n\nlec lhs , lhs2\nlec lhs?, lhs2_v\n</code></pre> <pre><code>var lhs = v1 + v2\n\n// RTL equivalent\nlet lhs2   = __sum(A=(v1, v2))\nlet lhs2_v = __and(v1?, v2?)\n\nlec lhs , lhs2\nlec lhs?, lhs2_v\n</code></pre> <pre><code>lhs = v0\nif cond1 {\n  lhs = v1\n}elif cond2 {\n  lhs = v2\n} // no else\n\n// RTL equivalent\nlet tmp = __mux(cond2, v0, v2)\nlet lhs2= __mux(cond1, tmp, v1)\n\nlet tmp_v = __mux(cond2, v0?, v2?)\nlet lhs2_v= __mux(cond1, tmp_v, v1?)\n\nlec lhs , lhs2\nlec lhs?, lhs2_v\n</code></pre> <pre><code>let f = fun(a,b) { if a == 0 { 3 }else{ b } }\n\nvar lhs = c\nif cond {\n   lhs = f(a,b)\n}\n\n// RTL equivalent\nlet a_cond = __not(__ror(a))             // a == 0\nlet tmp    = __mux(a_cond, b, 3)         // if a_cond { 3 }else{ b }\nvar lhs2   = c\nlhs2       = __mux(cond, x, tmp)\n\nlet tmp_v  = __mux(a_cond, a?, __and(a?,b?)) // a? or (a==0 and b?)\n\nlet lhs2_v = __mux(cond, c?, tmp_v)\n\nlec lhs , lhs2\nlec lhs?, lhs2_v\n</code></pre>"},{"location":"pyrope/06b-instantiation/#lambda-calls","title":"Lambda calls","text":"<p>Lambda calls are either inlined or become a specific instance (module). When the instance is located in a conditional path, the instance is moved to the main scope toggling the inputs valid attribute <code>::[valid=false]</code>. The instance has the assigned variable name. If the instance is a <code>var</code>, the variable name can be the SSA name.</p> Lambda callInstance <pre><code>let sub = proc(a,b)-&gt;(x) {\n  let tmp = sum(a,b)       // instance tmp,sum\n\n  x = sum(tmp,3)           // instance x,sum\n}\n\nlet top = proc(a,b,c)-&gt;(x) {\n\n x = sub(a,b).x\n if c {\n   let tmp=3\n   x += sub(b,tmp).x\n }\n}\n</code></pre> <pre><code>let sub = proc(a,b)-&gt;(x) {\n  let tmp = sum(a,b)       // instance tmp\n\n  x = sum(tmp,3)           // instance x\n}\n\nlet top = proc(a,b,c)-&gt;(x) {\n\n x = sub(a,b).x           // instance x\n\n let x_0 = _\n let sub_arg_0 = _\n let sub_arg_1 = _\n if c {\n   let tmp=3\n   sub_arg_0 = b\n   sub_arg_1 = tmp\n   x += x_0.[defer]       // use defer (instance after conditional code)\n }\n x_0 = sub(sub_arg_0,sub_arg_1).x   // instance x_0 (SSA)\n}\n</code></pre>"},{"location":"pyrope/06b-instantiation/#optional-lambdas","title":"Optional lambdas","text":"<p>HDLs use typical software constructs that look like function calls to represent instances in design. As previously explained, hardware languages are about instantiation, and software languages are about instruction execution. A lambda called unconditionally is likely to result in <code>module</code> unless the compiler decides to be small and it is inlined.</p> <p>In Pyrope, the semantics are that when a lambda is conditionally called, it should behave like if the lambda were inlined in the conditional place. Since functions have no side effects, it is also equivalent to call the lambda before the conditional path, and assign the return value inside the conditional path only. Special care must be handled for the <code>puts</code> which is allowed in functions. The <code>puts</code> is not called if the function is conditionally called and the condition is false.</p> Conditional proc callPyrope inline equivalent <pre><code>let case_1_counter = proc(runtime)-&gt;(res) {\n\n  let r = (\n    ,reg total:u16 = _          // r is reg, everything is reg\n    ,increase = fun(a) {\n      puts \"hello\"\n\n      let res = self.total\n      self.total::[wrap] = res+a\n\n      res\n    }\n  )\n\n  if runtime == 2 {\n    res = r.increase(3)\n  }elif runtime == 4 {\n    res = r.increase(9)\n  }\n}\n</code></pre> <pre><code>let case_1_counter = proc(runtime)-&gt;(res) {\n\n  let r = (\n    ,reg total:u16 = _\n    ,increase = fun(a) {\n      puts \"hello\"\n\n      let res = self.total\n      self.total::[wrap] = res+a\n\n      res\n    }\n  )\n\n  if runtime == 2 {\n    puts \"hello\"\n\n    let res = r.total\n    r.total::[wrap] = res+3\n    res = res\n  }elif runtime == 4 {\n    puts \"hello\"\n\n    let res = r.total\n    r.total::[wrap]= res+9\n    res = res\n  }\n}\n</code></pre> <p>The result of conditionally calling procedures is that most of the code may be inlined. This can change the expected equivalent Verilog generated modules.</p> <p>Calling a procedure with the inputs set invalid has a different behavior. For once C++ calls will still happen, and updates to registers with not valid data is allowed to reset the valid bit.</p>"},{"location":"pyrope/06b-instantiation/#expressions","title":"Expressions","text":"<p>Pyrope expressions are guaranteed to have the same result independent of the order of evaluation. Only <code>and_then</code>, <code>or_else</code> or complex constructs like <code>if/else</code>, <code>match</code>, <code>for</code> have evaluation order.</p>"},{"location":"pyrope/06b-instantiation/#setup-vs-reset-vs-execution","title":"Setup vs reset vs execution","text":"<p>In a normal programming language, the Von Neumann PC specifies clear semantics on when the code is executed. The language could also have a macro or template system executed at compile-time, the rest of the code is called explicitly when the function is called. As mentioned, a key difference is that HDLs focus on instantiation of gates/logic/registers, not instruction execution. HDLs tend to have 3 code sections:</p> <ul> <li> <p>Setup: This is code executed to set up the hierarchies, parameters, read   configuration setups... It is usually executed at compile time. In Verilog,   these are the preprocessor directives and the generate statements.  In   CHISEL, the scala is the setup code.</p> </li> <li> <p>Reset: Hardware starts in an undefined/inconsistent state. Usually, a reset   signal is enabled several cycles and the associated reset logic configures   the system to a given state.</p> </li> <li> <p>Execution: This is the code executed every cycle after reset. The reset   logic activation can happen at any time, and parts of the machine may be in   reset mode while others are not.</p> </li> </ul> <p>In addition, some languages like Verilog have \"initialization\" code that is executed before reset. This is usually done for debugging, and it is not synthesizable. Although not always synthesizable, we consider this setup code.</p> <p>Pyrope aims to have the setup, reset, and execution specified.</p>"},{"location":"pyrope/06b-instantiation/#setup-code","title":"Setup code","text":"<p>Compiling a Pyrope program requires specifying a \"top file\" file and a \"top variable\" in the top file. The top file is executed only once. The top file may \"import\" other files. Each of the imports is executed only once too. The imported files are executed before the current file is executed. This is applied recursively but no loops are supported in import dependence chains.</p> <p>The \"setup\" code is the statements executed once for each imported file. Those statements can not be \"imported\" by other files. Only the resulting public variables can be imported.</p> <p>During setup, each file can have a list of public variables. Those are variables that can be used by importing modules.  The \"top variable\" is selected for simulation/synthesis.</p> <p>It is important to point that <code>comptime</code> may be used during setup but also in non-setup code. <code>comptime</code> just means that the associated variables are known at compile time. This is quite useful during reset and execution too or just to guaranteed that a computation is solved at compile time.</p>"},{"location":"pyrope/06b-instantiation/#reset-code","title":"Reset code","text":"<p>The reset logic is associated with registers and memories. The assignment to register declaration is the reset code. It will be called for as many cycles are the reset is held active.  The <code>reg</code> assignment can be a constant or a call to <code>conf</code> that can provide a runtime file with the values to start the simulation/synthesis.</p> <pre><code>reg r:u16 = 3 // reset sets r to 3\nr = 2             // non-reset assignment\n\nreg array:[]u16 = (1,2,3,4)  // reset values\n\nreg r2:u128 = conf.get(\"my_data.for.r2\")\n\nreg array:[] = conf.get(\"some.conf.hex.dump\")\n</code></pre> <p>The assignment during declaration to a register is always the reset value. If the assignment is a method, the method is called every cycle during reset.</p> <pre><code>reg array:[1024]tag:[clock_pin=my_clock] = proc(ref self) {\n  reg reset_iter:u10:[reset_pin=false] = 0sb? // no reset flop\n\n  self[reset_iter].state = I\n\n  reset_iter::[wrap] = reset_iter + 1\n}\n</code></pre> <p>Since the reset can be high many cycles, it may be practical/necessary to have a reset inside the reset procedure. To guarantee determinism, any register inside the reset procedure can be either asynchrnous reset or a register without reset signal.</p> <pre><code>reg my_flop:[8]u32 = proc(ref self) {\n  reg reset_counter:u3:[async=true] = _ // async is only posedge reset\n\n  self[reset_counter] = reset_counter\n  reset_counter::[wrap] += 1\n}\n</code></pre> <p>A related functionality and constrains happen when a tuple have some register fields and some non-register fields. The same reset procedure is called every cycle Similarly a tuple can have a reset when assigned to a register.</p> Mixed tuple reset with constantsMixed tuple reset with method <pre><code>let Mix_tup = (\n  ,reg flag:bool = false\n  ,state: u2\n)\n\nvar x:Mux_tup = (false,1)  // 0 used at reset, 1 used every cycle\n\nassert x.flag implies x.state == 2\n\nx.state = 0\nif x.flag {\n  x.state = 2\n}\nx.flag = true\n</code></pre> <pre><code>let Mix_tup = (\n  ,reg flag:bool = false\n  ,state:u2\n)\n\nvar x:Mux_tup = proc(ref self) {\n  self.flag  = proc(ref self) { self = false }  // reset code\n  self.state = 2                                // every cycle code\n}\n\nassert x.flag implies x.state == 2\n\nx.state = 0\nif x.flag {\n  x.state = 2\n}\n</code></pre> <p>A sample of asynchronous reset with different reset and clock signal</p> <pre><code>reg my_asyn_other_reg:u8:[\n  ,async = true\n  ,clock = ref clk2    // ref to connect, not read clk2 value\n  ,reset = ref reset33 // ref to connect, not read current reset33 value\n] = 33 // initialized to 33 at reset\n\n\nif my_async_other_reg == 33 {\n  my_async_other_reg = 4\n}\n\nassert my_async_other_reg in (4,33)\n</code></pre>"},{"location":"pyrope/06b-instantiation/#retime","title":"retime","text":"<p>Values stored in registers (flop or latches) and memories (synchronous or asynchronous) can not be used in compiler optimization passes. The reason is that a scan chain is allowed to replace the values.</p> <p>The <code>retime</code> attribute indicates that the register/memory can be replicated and used for optimization. Copy values can propagate through <code>retime</code> register/memories.</p> <p>A register or memory without explicit <code>:[retime]</code> attribute can only be optimized away if there is no read AND no write to the register. Even just having writes the register is preserved because it can be used to read values with the scan-chain.</p>"},{"location":"pyrope/06b-instantiation/#execution-code","title":"Execution code","text":"<p>HDLs specify a tree-like structure of modules. The top module could instantiate several sub-modules. Pyrope Setup phase is to create such hierarchical structures. The call order follows a program order from the top point every cycle, even when reset is set.</p> <p>The following Verilog hierarchy can be encoded with the equivalent Pyrope:</p> VerilogPyrope equivalentPyrope alternative IPyrope alternative II <pre><code>module inner(input z, input y, output a, output h);\n  assign a =   y &amp; z;\n  assign h = !(y &amp; z);\n\nendmodule\n\nmodule top2(input a, input b, output c, output d);\n\ninner foo(.y(a),.z(b),.a(c),.h(d));\n\nendmodule\n</code></pre> <pre><code>let inner = fun(z,y)-&gt;(a,h) {\n  a =   y &amp; z\n  h = !(y &amp; z)\n}\n\nlet top2 = fun(a,b)-&gt;(c,d) {\n  let x= inner(y=a,z=b)\n  c = x.a\n  d = x.h\n}\n</code></pre> <pre><code>let Inner_t = (\n  ,setter = proc(ref self, z,y) {\n    self.a =   y &amp; z\n    self.h = !(y &amp; z)\n  }\n)\n\nlet Top2_t = (\n  ,setter = proc(ref self,a,b) {\n    let foo:Inner_t = (y=a,z=b)\n\n    self.c = foo.a\n    self.d = foo.h\n  }\n)\n\nlet top:Top2_t = (a,b)\n</code></pre> <pre><code>let Inner_t = (\n  ,setter = proc(ref self, z,y) {\n    self.a =   y &amp; z\n    self.h = !(y &amp; z)\n  }\n)\n\nlet Top2_t = (\n  ,foo:Inner_t = _\n  ,setter = proc(ref self,a,b) {\n    (self.c, self.d) = self.foo(y=a,z=b)\n  }\n)\n\nlet top:Top2_t = (a,b)\n</code></pre> <p>The top-level module <code>top2</code> must be a module, but as the alternative Pyrope syntax shows, the inner modules may be in tuples or direct module calls. The are advantages to each approach but the code quality should be the same.</p>"},{"location":"pyrope/06b-instantiation/#registers","title":"Registers","text":"<pre><code>reg a:u4 = 3\na::[saturate] = a+1\n\nreg b = 4\nif cond {\n  reg c = _           // weird as reg, but legal syntax\n  c = b + 1\n  b = 5\n}\n\n// RTL equivalent\na_qpin = __flop(reset=ref reset, clk=ref clk, initial=3, din=a.[defer])\ntmp    = __sum(A=(a_qpin, 1))\na      = __mux(tmp[4], tmp@[0..=3], 0xF)    // saturate, not wrap\n\nb_qpin = __flop(reset=ref reset, clk=ref clk, initial=4, din=b.[defer])\nb      = __mux(cond, b_qpin, 5)\n\nc_cond_qpin = __flop(reset=ref reset, clk=ref clk, initial=0, din=c_cond.[defer])\nc_cond      = __sum(A=(b, 1))\n</code></pre>"},{"location":"pyrope/06c-pipelining/","title":"Pipelining","text":""},{"location":"pyrope/06c-pipelining/#registers","title":"Registers","text":"<p>Together with memories, flip-flops or latches are the basic constructs used by hardware to store information and to build pipeline stages. Pyrope's goal is to be a zero-cost overhead, and as such it allows to handle flops directly.</p> <p>To create an individual flop, a direct RTL instantiation of a <code>__flop</code> can be used. Flops and latches have several pins <code>din</code>, <code>q</code>, <code>clock</code>, <code>enable</code> and configuration options <code>posclk</code>, <code>initial</code>, and <code>async</code>. </p> <p>A more programmer-friendly is to use to declare a register. The compiler does not provide guarantees that the register will not be split into multiple registers.</p> <p>The explicit connection likely requires constructs like <code>.[defer]</code> to connect the flop <code>q</code> pin.</p> Structural flop stylePyrope style <pre><code>var counter_next:u8:[wrap] = _\n\nlet counter_q = __flop(din=counter_next.[defer] // defer to get last update\n                   ,reset_pin=my_rst, clock_pin=my_clk\n                   ,enable=my_enable            // enable control\n                   ,posclk=true\n                   ,initial=3                   // reset value\n                   ,async=false)\n\ncounter_next = counter_q + 1\n</code></pre> <pre><code>reg counter:u8:[reset_pin=my_rst, clock_pin=my_clk, posclk=true]= 3\nassert counter == counter#[0]  // counter still has the q value\n\nif my_enable {\n  counter::[wrap] = counter + 1\n}\n</code></pre> <p>Flops have <code>din</code> and a <code>q</code> pin. At the beginning of the cycle both <code>din</code> and <code>q</code> have the same value, but as the <code>din</code> is updated with \"next cycle\" <code>q</code> value their contents may be different. Different HDLs have different syntax to distinguish between <code>din</code> and <code>q</code> pin. In Verilog, it is common to have a coding style guideline that gives a different name to the din variables than to the q variables (E.g: <code>counter_q</code> vs <code>counter_next</code>). The structural flop style is a legal Pyrope code using these type of names.</p> <p>In a more friendly Pyrope style, a register like <code>counter</code> starts with the <code>q</code> pin each cycle. The last value written to <code>counter</code> connects to the <code>din</code>. It is always possible to access the <code>q</code> pin/value directly with  pipeline directives <code>something#[0]</code>.</p> <p>If the register is accessed with the <code>-1</code> cycle (<code>#something#[-1]</code>), the flop will insert an additional pipeline to access 1 cycle before flop contents.</p> <p>It is also possible to use positive values (<code>variable#[3]</code>) which means the value in the future 3 cycles, but this is only allowed in debug statements like <code>assert</code> or <code>puts</code>.</p> <p>Latches are possible but with the direct RTL instantiation. Latches have  a <code>din</code> and <code>enable</code> pin like a flop, but just one option <code>posclk</code>.</p> <pre><code>var my_latch_q = __latch(din=my_din, enable=my_enable, posclk=true)\n</code></pre>"},{"location":"pyrope/06c-pipelining/#pipestage","title":"Pipestage","text":"<p>Pyrope has a <code>pipestage</code> statement (<code>#&gt;identifier[fsm_configuration]</code>) that helps to create simple pipeline stages. The <code>identifier[fsm_configuration]</code> is optional and the default meaning is a fully pipelined 1 pipeline stage depth. It is the same as saying <code>_[lat=1]</code>. The identifier can be accessed as an attribute to count the pipestage utilization.</p> <p>The fsm configuration can have <code>lat</code> (number of pipeline stages or latency) or <code>num</code> (number of units). The number of units is only needed when the code is not fully pipelined in combination with loop constructs like <code>while</code>, <code>for</code>, and <code>loop</code>.</p> <p>The <code>num</code> sets the number of units. The hardware will not back pressure, but an assertion will fail during simulation if the number of units is overflowed. <code>num</code> only makes sense when the latency (<code>lat</code>) is more than 1.</p> <pre><code>// variables/register before\n\n{\n  // stage 0 scope\n} #&gt; {               // no identifier, 1 stage by default\n  // stage 1 scope\n} #&gt;foo[2] {            // no identifier, 2 stages\n  // stage 2-3 scope\n} #&gt;bar[1] {     // 'free_stage' identifier, 1 stages\n  // stage 4 scope\n} #&gt; {\n  // stage 5 scope\n}\n\n// variables/register after pipestage\n</code></pre> <p>The semantics of <code>pipestage</code> are as follows:</p> <ul> <li> <p>Explicitly declared registers (<code>reg foo</code>) are not affected by <code>pipestage</code></p> </li> <li> <p>Variables declared before are \"pipelined\" inside each of the <code>pipestage</code> scopes.</p> </li> <li> <p>Variables declared in a stage are pipelined to all the following stages   unless the variable is private (<code>var priv_example::[private]=3</code>)</p> </li> <li> <p>The pipelined variables are not visible in the scope after the <code>pipestage</code>   sequence.</p> </li> <li> <p>The original non-pipelined variable can be accessed with <code>v#[0]</code>.</p> </li> </ul> <p>To illustrate the semantics, imagine a module where the input <code>i</code> is a monotonically increasing sequence (<code>0,1,2,3,4,5,6,7....</code>).</p> <pre><code>assert i==0 or (i#[-1] + 1 == i)\n\nlet i_let = i\n\nvar i_var0 = i\nvar i_var1 = i\n\nreg i_reg0 = i        // initialization only\nreg i_reg1:i_reg0 = _\n\ni_reg1 = i                // every cycle\n\n{\n  assert i == i#[0]       // i#[0] is unflop input (or first defined) value\n  assert i == i_let\n  assert i == i_var0\n  assert i == i_var1\n\n  assert 0 == i_reg0\n  assert i == i_reg1\n\n  let _local_var = 3\n\n  let pub_var = 100 + i\n\n} #&gt; {\n  assert _local_var!=0      // compile error, _local_var is not in scope\n  assert pub_var == 100 + i // pipelined pub_var\n\n  // both inputs and variables flop, so asserts hold\n  assert i == i_let\n  assert i == i_var0\n  assert i == i_var1\n\n  assert 0 == i_reg0        // i_reg0 never changes, so 0 is fine\n  assert i#[-1] == i_reg1   // last i-reg, not current\n\n  assert i == 0 or (i == i#[0]+1)  // i#[0] is the unflop original\n}\n\nassert pub_var != 0 // compile error, pub_var is not in scope\n</code></pre> <p>The pipestage accept the <code>clock</code> and <code>posclk</code> attributes from register to allow the selection of different clock signal. Notice that it does not allow <code>reset</code> or <code>latch</code> or <code>async</code> because it does not require reset logic.</p> <p>Pipelining is one of the challenges of designing hardware. Even a simple pipestage code can result in incorrect hardware. The reason is that if two pipestage blocks generate an output simultaneously, there is no way to generate both outputs. The result is a compile or simulation error.</p> <pre><code>let bad_code = proc(my_clk, inp)-&gt;(o1,o2) {\n\n  {\n    o1 = 1\n    o2 = inp + 1  // o2? iff bad_code called this cycle and inp? is valid\n  } #&gt;my_pipe[lat=1,clock=my_clk] {\n    o1 = 2        // compile error, o1 driven simultaneous from multiple stages\n    o2 = inp + 2  // may be OK if inp is not valid every cycle\n  }\n\n}\n</code></pre>"},{"location":"pyrope/06c-pipelining/#pipestage-with-loops","title":"Pipestage with loops","text":"<p>The pipestage directive (<code>#&gt;identifier[cycles]{  }</code>) automatically creates a fully pipeline design with <code>cycles</code> pipeline depth. <code>cycles</code> must be bigger or equal than 1 and known at compile time. When <code>cycles</code> is not specified a <code>1</code> value is assumed.</p> <p>Pipestage can be applied to <code>while</code> and <code>loop</code> statements, not to <code>for</code> statements because <code>for</code> must be fully unrolled at compile time.</p> <p>When applied to loops, the loop becames a state machine with <code>cycles</code> the maximum number of simultaneous loop iterations. It effectively means that number of units or state machines that can perform the loop simultaneously. The identifier becomes a procedure attribute.</p> Fully PipelinedState-machineSlow 1 StageSlow 1 Stage (alt syntax)Pure Combinatinal <pre><code>let mul3=proc(a,b)-&gt;res {\n  let tmp = a*b\n  #&gt;full_case[lat=3,num=3] { // Same as full_case[lat=3]\n    res = tmp\n  }\n}\n</code></pre> <pre><code>let mul_slow=proc(a,b)-&gt;res {\n\n  let result  = 0\n  let rest    = a\n\n  while rest &gt;= b #&gt;_[lat=1,num=4] {  // lat=1 is latency per iteration\n    rest = rest - b\n    result += 1\n  }\n\n  res = result\n}\n</code></pre> <pre><code>let mul1=proc(a,b)-&gt;(reg res) {\n  res = a*b\n}\n</code></pre> <pre><code>let mul1=proc(a,b)-&gt;(res) {\n  #&gt;full_case_again[lat=1] { \n    res = a*b\n  }\n}\n</code></pre> <pre><code>let mul0=proc(a,b)-&gt;(res) {\n  res = a*b\n}\n</code></pre> <p>To understand the fully pipelined behavior, the following shows the pipestage against the more direct implementation with registers.</p> <pre><code>if cond {\n  var p1 = inp1\n  var out = _\n\n  {\n    var _l1 = inp1 + 1\n\n    var p2 = inp1 + 2\n  } #&gt; {\n    out = p1 + p2\n  }\n\n  res = out\n}\n\n// Non pipestage equivalent\nif cond {\n  var p1 = inp1\n  var out = _\n\n  {\n    reg p1r = _\n    reg p2r = _\n\n    var l1::[private] = inp1 + 1  // private\n    var p2 = inp1 + 2             // public\n\n    out = p1r + p2r               // registered values\n\n    p1r = p1\n    p2r = p2\n  }\n\n  res = out\n}\n</code></pre>"},{"location":"pyrope/06c-pipelining/#retiming","title":"Retiming","text":"<p>The registers manually inserted with the <code>reg</code> directive are preserved and annotated so that synthesis retiming can not change them. This means that by default register can not be duplicated or logic can move around.</p> <p>A register can be marked with the <code>retime</code> flag, in which case the synthesis tools are allowed to perform the following optimizations:</p> <ul> <li> <p>Retime or move logic across which effectively changes the meaning of the register.</p> </li> <li> <p>Duplication is allowed when frequency improvements grant it.</p> </li> <li> <p>Elimination. If there are no reads or no writes, the register can be remove   and replaced with a constant (no writes) or just removed (no reads).</p> </li> <li> <p>Copy propagation is allowed across registers. This is not possible without   retime because the scan chain could reconfigure the flop.</p> </li> </ul> <p>The registers automatically inserted with the <code>pipestage</code> command are marked with <code>retime</code> true. Additionally, retime can be set in any register:</p> <pre><code>reg my_reg::[retime=true,clock=my_clk] = 0\n</code></pre>"},{"location":"pyrope/06c-pipelining/#multiply-add-example","title":"Multiply-Add example","text":"<p>To illustrate the confusion/complication the following example illustrates a multiplier that takes 3 cycles and an adder that takes 1 cycle to complete, and the conceptual problems of integrating them:</p> PipestageExplicit Stages <pre><code>let block = proc(in1,in2)-&gt;(out) {\n  {\n    let tmp = in1 * in2\n  } #&gt;some_id[lat=3] {\n    out = tmp + in1#[0]\n  }\n}\n</code></pre> <pre><code>add1 = proc(a,b) {     // 1 cycle add\n  reg r  = _\n  let rr = r           // get flop value\n  r = a+b\n  return rr\n}\nlet mul3 = proc(a,b) { // 3 cycle multiply\n  reg reg1 = _\n  reg reg2 = _\n  reg reg3 = _\n  reg3 = reg2\n  reg2 = reg1\n  reg1 = a * b\n  return reg3\n}\n\nlet block = proc(in1,in2)-&gt;(out) {\n  let x =#[..] mul3(in1, in2)\n  out   =#[..] add1(x,in3)\n}\n</code></pre> <p>In general, <code>#</code> is used when dealing with registers. The previous example use <code>procedures</code> (<code>proc ... {...}</code>) instead of <code>functions</code> (<code>fun ... {...}</code>) because functions only have combinational logic. When the procedures are called, the assigned variable needs the <code>=#[..]</code>. This is to explicitly indicate to Pyrope that the function called (<code>mul3</code>, <code>add1</code>) can have pipeline outputs. This helps the tool but more importantly the programmer because it helps to check assumptions about the function connections. The typical assignment <code>=</code> only connects combinational logic.</p> <p>The previous code connects two inputs (<code>in1</code>/<code>in2</code>) to a multiplier, and then connects the result of the multiplier to an adder. The inputs are also passed to the adder. This results in the following functionality:</p> graph LR     in1[in1] --a--&gt; m0(mul3 cycle 0)     in2[in2] --b--&gt; m0      m0 --&gt; m1(mul3 cycle 1)     m1 --&gt; m2(mul3 cycle 2)      in1--a--&gt; a0[add1 cycle 1]     m2 --b--&gt; a0     a0 --&gt; out[out]  <p>The issue in most HDLs is that the connection is unaware of the pipelining, and it is left up to the programmer to understand and check the potential pipeline stages inside <code>add1</code> and <code>mul3</code>. This lack of pipelining awareness in the language syntax is common in most HDLs.</p> <p>In Pyrope, the <code>=#[..]</code> must be used when there is any path that starts from the inputs of the function passes through a pipeline stage to generate the assignment. If all the paths have exactly 1 flop in between, it is a 1 stage pipeline, if some paths have 2 flops and others 3, it is a 2 or 3 pipeline stages. Sometimes, there are loops, and the tool has 1 to infinite pipeline stages.</p> <p>The default pipeline assignment <code>=#[..]</code> just checks that it is possible to have pipeline stages between the module/function inputs and the assignment value. To restrict the check, it accepts a range. E.g: <code>=#[3]</code> means that there are exactly 3 flops or cycles between inputs and the assignment. <code>=#[0..&lt;4]</code> means that there are between 0 and 3 cycles, and open range could be used when there are loops (E.g: <code>=#[2..]</code>).</p> <pre><code>let x = mul3(in1, in2)      // compile error: 'mul3' is pipelined\nlet x =#[..] mul3(in1, in2) // OK\nout  =#[..] add1(x,in3)     // OK (in3 has 0 cycles, x has 3 cycles)\nout  =#[1] add1(x,in3)      // compile error: 'x' is pipelined with '3' cycles\nout  =#[3] add1(x,in3)      // compile error: 'in3' is pipelined with '1' cycle\nout  =#[1..&lt;4] add1(x,in3)  // OK\n</code></pre> <p>The designer likely wanted to implement a multiply-add. As such, the input to the adder should be from the same cycle as the multiplied started to operate. Otherwise, values across cycles are mixed.</p> graph LR   in1[in1] --a--&gt; m0(mul3 cycle 0)   in2[in2] --b--&gt; m0    m0 --&gt; m1(mul3 cycle 1)   m1 --&gt; m2(mul3 cycle 2)    in1  --&gt; in1_0(flop cycle 0)   in1_0--&gt; in1_1(flop cycle 1)   in1_1--&gt; in1_2(flop cycle 2)   in1_2--a--&gt; a0[add1 cycle 0]   m2 --b--&gt; a0   a0 --&gt; out[out]  <p>It is possible to balance the pipeline stages explicitly, the issue is that it is error-prone because it requires knowing exactly the number of cycles for <code>mul3</code>. </p> PipestageExplicitly added pipeline stages <pre><code>{\n  let tmp = in1 * in2\n} #&gt;fully_pipe[lat=3] {\n  out = tmp + in1\n}\n</code></pre> <pre><code>x =#[..] mul3(in1, in2)\ny = in1#[-3]\nout =#[..] add1(a=x,b=y)    // connect in1 from -3 cycles\n</code></pre> <p>Observation</p> <p>The explicit <code>v#[-cycles]</code> inserts registers and access the result <code>cycles</code> before. This same syntax can be used with assertions similar to the Verilog <code>$past(v, cycles)</code>.</p>"},{"location":"pyrope/06c-pipelining/#alu-example","title":"ALU example","text":"<p>Pipestages allow to build fully pipelined structures but also non-pipelined state machines when applied to loops. This creates potential contention that the designer must decide how to manage. This contention can be propagated outside the procedure with attributes.</p> <p>The ALU example illustrates the contention by creating an ALU with 3 different pipelines (add,mul,div) that have different latencies and contention.</p> <pre><code>let quick_log2 = fun(a) {\n\n  cassert a&gt;=1\n\n  var i = 1\n  var v = 0\n  while i &lt; a.[bits] {\n    v |= i\n    i *= 2\n  }\n\n  return v\n}\n\nlet div=proc(a,b,id)-&gt;(res,id) {\n  loop #&gt;free_div_units[4] {\n    return (a &gt;&gt; quick_log2(b), id) when b@+[..] == 1\n    #&gt;my_fsm[lat=5,num=1] {\n      res = (a/b, id)\n    }\n  }\n}\n\nlet mul=proc(a,b,id)-&gt;(res, id) {\n  #&gt;pending_counter[lat=3,num=2] {\n    res = a*b\n    id  = id\n  }\n}\n\nlet add=proc(a,b,id)-&gt;(res,id) {\n  #&gt;add_counter[lat=1] {         // Fully pipeline, num not specified\n    res = a+b\n    id  = id\n  }\n}\n\nlet alu = proc(a,b,op, id)-&gt;(res,id) {\n\n  self.[total_free_units] = 1 \n     + mul.[pending_counter] \n     + div.[free_div_units]\n     + add.[add_counter]\n\n  self.[div_units] = div.[free_div_units]\n\n  match op {\n    == OP.div {\n      assert div.[free_div_units]&gt;0\n      (res,id) = div(a,b,id)\n    }\n    == OP.mul { (res,id) = mul(a,b,id) }\n    == OP.add { (res,id) = add(a,b,id) }\n  }\n}\n\ntest \"alu too many div\" {\n\n cassert alu.[total_free_units] == (1+3+4)\n\n let r1 = alu(13,3, OP.div, 1)\n assert alu.div_units==3\n let r2 = alu(13,3, OP.div, 2)\n assert alu.div_units==2\n let r3 = alu(13,3, OP.div, 3)\n assert alu.div_units==1\n let r4 = alu(13,3, OP.div, 4)\n assert alu.div_units==0\n\n assert !r1? and !r2? and !r3? and !r4? // still invalid\n\n let r5 = alu(13,4, OP.mul,5)\n cassert mul.[pending_counter] == 2\n}\n</code></pre>"},{"location":"pyrope/07-typesystem/","title":"Type system","text":"<p>Type system assign types for each variable (type synthesis) and check that each variable use/expression respects the allowed types (type check). Additionally, a language can also use the type synthesis results to implement polymorphism.</p> <p>Most HDLs do not have modern type systems, but they could benefit like in other software domains. Unlike software, in the hardware, we do not need to have many integer sizes because hardware can implement any size. This simplifies the type system allowing unlimited precision integers but it needs a bitwidth inference mechanism.</p> <p>Additionally, in hardware, it makes sense to have different implementations that adjust for performance/constraints like size, area, FPGA/ASIC. Type systems could help in these areas.</p>"},{"location":"pyrope/07-typesystem/#types-vs-cassert","title":"Types vs <code>cassert</code>","text":"<p>To understand the type check, it is useful to see an equivalent <code>casser</code> translation. The type system has two components: type synthesis and type check. The type check can be understood as a <code>cassert</code>.</p> <p>After type synthesis, each variable has an associated type. Pyrope checks that for each each assignment, the left-hand side (LHS) has a compatible type with the right-hand side (RHS) of the expression. Additional type checks happen when variables have a type check explicitly set (<code>variable:type</code>) in the rhs expression.</p> <p>Although the type system is not implemented with asserts, it is an equivalent way to understand the type system \"check\" behavior.  Although it is possible to declare just the <code>cassert</code> for type checks, the recommendation is to use the explicit Pyrope type syntax because it is more readable and easier to optimize.</p> Snippet with typesSnippet with comptime assert <pre><code>var b = \"hello\"\n\nvar a:u32 = 0\n\na += 1\n\na = b                       // incorrect\n\n\nvar dest:u32 = 0\n\ndest = foo:u16 + v:u8\n</code></pre> <pre><code>var b = \"hello\"\n\nvar a:u32 = 0\n\na += 1\ncassert a does u32\na = b                       // incorrect\ncassert b does u32  // fails\n\nvar dest:u32 = 0\ncassert (dest does u32) and (foo does u16) and (v does u8)\ndest = foo:u16 + v:u8\n</code></pre>"},{"location":"pyrope/07-typesystem/#building-types","title":"Building types","text":"<p>Each variable can be a basic type. In addition, each variable can have a set of constraints from the type system. Pyrope type system constructs to handle types:</p> <ul> <li> <p><code>var</code> and <code>let</code> allows declaring types.</p> </li> <li> <p><code>a does b</code>: Checks 'a' is a superset or equal to 'b'. In the future, the   Unicode character \"\\u02287\" could be used as an alternative to <code>does</code> (<code>a</code> \u2287 <code>b</code>).</p> </li> <li> <p><code>a:b</code> is equivalent to <code>a does b</code> for type check, but it is also used by type   synthesis when used in the left-hand-side of assignments.</p> </li> <li> <p><code>a equals b</code>: Checks that <code>a does b</code> and <code>b does a</code>. Effectively checking   that they have the same type. Notice that this is not like checking for   logical equivalence, just type equivalence.</p> </li> </ul> <pre><code>let t1 = (a:int=1  , b:string)\nlet t2 = (a:int=100, b:string)\nvar v1 = (a=33     , b=\"hello\")\n\nlet f1 = fun() {\n  return (a=33     , b=\"hello\")\n}\n\nassert t1    equals t2\nassert t1    equals v1\nassert f1()  equals t1\nassert _:f1 !equals t1\nassert _:t1  equals t2\n</code></pre> <p><code>equals</code> and <code>does</code> check for types. Sometimes, the type can have a function call and you do not want to call it. The solution in this case is to use the <code>:type</code> to avoid the function call.</p> <p>Since the <code>puts</code> command understands types, it can be used on any variable, and it is able to print/dump the results.</p> <pre><code>let At:int(33..) = _      // number bigger than 32\nlet Bt=(\n  ,c:string = _\n  ,d=100\n  ,setter = fun(ref self, ...args) { self.c = args }\n)\n\nvar a:At=40\nvar a2 = At(40)\ncassert a == a2\n\nvar b:Bt=\"hello\"\nvar b2 = Bt(\"hello\")\ncassert b == b2\n\nputs \"a:{} or {}\", a, at // a:40 or 33\nputs \"b:{}\", b           // b:(c=\"hello\",d=100)\n</code></pre>"},{"location":"pyrope/07-typesystem/#type-equivalence","title":"Type equivalence","text":"<p>The <code>does</code> operator is the base to compare types. It follows structural typing rules. These are the detailed rules for the <code>a does b</code> operator depending on the <code>a</code> and <code>b</code> fields:</p> <ul> <li> <p>false when <code>a</code> and <code>b</code> are different basic types (<code>boolean</code>, <code>fun</code>,   <code>integer</code>, <code>proc</code>, <code>range</code>, <code>string</code>, <code>enums</code>).</p> </li> <li> <p>true when <code>a</code> and <code>b</code> have the same basic type of either <code>boolean</code> or <code>string</code>.</p> </li> <li> <p>true when <code>a</code> and <code>b</code> are <code>enum</code> and <code>a</code> has all the possible enumerates   fields in <code>b</code> with the same value.</p> </li> <li> <p><code>a.max&gt;=b.max and a.min&lt;=b.min</code> when <code>a</code> and <code>b</code> are integers. The <code>max/min</code>   are previously constrained values in left-hand-side statements, or inferred   from right-hand-side if no lhs type is specified.</p> </li> <li> <p><code>(a@[..] &amp; b@[..]) == b@[..]</code> when <code>a</code> and <code>b</code> are <code>range</code>. This means that the <code>a</code>   range has at least all the values in <code>b</code> range.</p> </li> <li> <p>There are two cases for tuples. If all the tuple entries are named, <code>a does   b</code> is true if for all the root fields in <code>b</code> the <code>a.field does b.field</code>. When   either <code>a</code> or <code>b</code> have unnamed fields, for each field in <code>b</code> the name but   also position should match. The conclusion is that if any field has no name,   all the fields should match by position and/or name if available.</p> </li> <li> <p><code>a does b</code> is false if the explicit array size of <code>a</code> is smaller than the   explicit array size of <code>b</code>. If the size check is true, the array entry type   is checked. <code>_:[]x does _:[]y</code> is false when <code>_:x does _:y</code> is false.</p> </li> <li> <p>The lambdas have a more complicated set of rules explained later.</p> </li> </ul> <pre><code>assert (a:int:(max=33,min=0) does (a:int(20,5)))\nassert (a:int(0..=33)       !does (a:int(50,5)))\n\nassert  (a:string,b:int) does (a:\"hello\", b:33)\nassert ((b:int,a:string) !does (a:\"hello\", b:33)) // order maters in tuples\n\nassert  _:fun(x,xxx2)-&gt;(y,z) does _:fun(x     )-&gt;(y,z)\nassert (_:fun(x     )-&gt;(y,z) !does _:fun(x,xxx2)-&gt;(y,z))\n</code></pre> <p>For named tuples, this code shows some of the corner cases:</p> <pre><code>let t1 = (a:string, b:int)\nlet t2 = (b:int, a:string)\n\nvar a:t1  = (\"hello\", 3)     // OK\nvar a1:t1 = (3, \"hello\")     // compile error, positions do not match\nvar b:t1  = (a=\"hello\", 3)   // OK\nvar b1:t1 = (3, a=\"hello\")   // compile error, positions do not match\nvar c:t1  = (a=\"hello\", b=3) // OK\nvar c1:t1 = (b=3, a=\"hello\") // OK\n\nvar d:t2 = c                 // OK, both fully named\nassert d.0 == c.1 and c.0 == d.1\nassert d.a == c.a and d.b == c.b\n</code></pre> <p>Ignoring the value is what makes <code>equals</code> different from <code>==</code>. As a result different functionality functions could be <code>equals</code>.</p> <pre><code>let a = fun() { 1 }\nlet b = fun() { 2 }\nassert a equals _:fun()    // 1 !equals :fun()\n\nassert a() != b()              // 1 != 2\nassert a() equals b()          // 1 equals 2\n\nassert _:a equals _:fun()\n</code></pre>"},{"location":"pyrope/07-typesystem/#type-check-with-values","title":"Type check with values","text":"<p>Many programming languages have a <code>match</code> with structural checking. Pyrope <code>does</code> allows to do so, but it is also quite common to filter/match for a given value in the tuple. This is not possible with <code>does</code> because it ignores all the field values. Pyrope has a <code>case</code> that extends the <code>does</code> comparison and also checks that for the matching fields, the value is the same.</p> <p>The previous explanation of <code>a does b</code> and <code>a case b</code> ignored types. When types are present, both need to match type.</p> <pre><code>cassert (a:u32=0, b:bool) does (a:u32, c:string=\"hello\", b=false)\ncassert (a:u32=0, c:string=\"hello\", b=false) case (a = 0, b:bool) // b is nil\n\ncassert (a:u32=0, c:string=\"hello\", b=false) !case (a:u32 = 1 , b:bool=nil )\ncassert (a:u32=0, c:string=\"hello\", b=false) !case (a:bool=nil, b:bool=nil )\ncassert (a:u32=0, c:string=\"hello\", b=false) !case (a = 0     , b     =true)\n</code></pre>"},{"location":"pyrope/07-typesystem/#nominal-type-check","title":"Nominal type check","text":"<p>Pyrope has structural type checking, but there is a keyword <code>is</code> that allows to check that the type name matches <code>a is b</code> returns true if the type of <code>a</code> has the same name as the type of <code>b</code>. The <code>a is b</code> is a boolean expression like <code>a does b</code>, not a <code>a:b</code> type check. This means that it can be used in <code>where</code> statements or any conditional code.</p> <p><code>a is b</code> is equivalent to check the <code>a</code> variable declaration type name against the <code>b</code> variable declaration type name. If their declaration had no type, the inferred type name is used.</p> <pre><code>let a = 3\nlet b = 200\ncassert a is b\n\nlet c:u32 = 10\ncassert a !is c\ncassert a::[typename] == \"int\" and c::[typename] == \"u32\"\n\nlet d:u32 = nil\ncassert c is d\n\nlet e = (a:u32=1)\nlet f:(a:u32) = 33\ncassert e is f\n</code></pre> <p>Since it checks equivalence, when <code>a is b == b is a</code>.</p> <pre><code>let X1 = (b:u32)\nlet X2 = (b:u32)\n\nlet t1:X1 = (b=3)\nlet t2:X2 = (b=3)\nassert (b=3) !is X2  // same as (b=3) !is X2\nassert t1 equals t2\nassert t1 !is t2\n\nlet t4:X1 = (b=5)\n\nassert t4  equals t1\nassert t4  is     t1\nassert t4 !is     t2\n\nlet f2 = fun(x) where x is X1 {\n  x.b + 1\n}\n</code></pre>"},{"location":"pyrope/07-typesystem/#enums-with-types","title":"Enums with types","text":"<p>Enumerates (enums) create a number for each entry in a set of identifiers. Pyrope also allows associating a tuple or type for each entry. Another difference from a tuple is that the enumerate values must be known at compile time.</p> <pre><code>let Rgb = (\n  ,c:u24\n  ,setter = proc(ref self, c) { self.c = c }\n)\n\nlet Color = enum(\n  ,Yellow:Rgb = 0xffff00\n  ,Red:Rgb    = 0xff0000\n  ,Green      = Rgb(0x00ff00) // alternative\n  ,Blue       = Rgb(0x0000ff)\n)\n\nvar y:Color = Color.Red\nif y == Color.Red {\n  puts \"c1:{} c2:{}\\n\", y, y.c  // prints: c1:Color.Red c2:0xff0000\n}\n</code></pre> <p>It is also possible to support an algebraic data type with enums. This requires each enumerate entry to have an associated type. In can also be seen as a union type, where the enumerate has to be either of the enum entries where each is associated to a type.</p> <pre><code>let ADT=enum(\n  ,Person:(eats:string) = _\n  ,Robot:(charges_with:string) = _\n)\n\nlet nourish = fun(x:ADT) {\n  match x {\n    == ADT.Person { puts \"eating:{}\"  , x.eats         }\n    == ADT.Robot  { puts \"charging:{}\", x.charges_with }\n  }\n}\n\ntest \"my main\" {\n  (_:Person=\"pizza\", _:Robot=\"electricity\").each(nourish)\n}\n</code></pre>"},{"location":"pyrope/07-typesystem/#bitwidth","title":"Bitwidth","text":"<p>Integers can be constrained based on the maximum and minimum value (not by the number of bits).</p> <p>Pyrope automatically infers the maximum and minimum values for each numeric variable. If a variable width can not be inferred, the compiler generates a compilation error. A compilation error is generated if the destination variable has an assigned size smaller than the operand results.</p> <p>The programmer can specify the maximum number of bits, or the maximum value range. The programmer can not specify the exact number of bits because the compiler has the option to optimize the design.</p> <p>In fact, internally Pyrope only tracks the <code>max</code> and <code>min</code> value. When the <code>sbits/ubits</code> is used, it is converted to a <code>max/min</code> range. Pyrope code can set or access the bitwidth attributes for each integer variable.</p> <ul> <li><code>$max</code>: the maximum number</li> <li><code>$min</code>: the minimum number</li> <li><code>$sbits</code>: the number of bits to represent the value</li> <li><code>$ubits</code>: the number of bits. The variable must be always positive or a compile error.</li> </ul> <p>Internally, Pyrope has 2 sets of <code>max/min</code>. The constrained and the current. The constrained is set during type declaration. The current is computed based on the possible max/min value given the current path/values. The current should never exceed the constrained or a compile error is generated. Similarly, the current should be bound to a given size or a compile error is generated.</p> <p>The constrained does not need to be specifed. In this case, the hardware will use whatever current value is found. This allows to write code that adjust to the needed number of integer bits.</p> <p>When the attributes are read, it reads the current. it does not read the constrained.</p> <pre><code>var val:u8 = 0   // designer constraints a to be between 0 and 255\nassert val.[sbits] == 0\n\nval = 3          // val has 3 bits (0sb011 all the numbers are signed)\n\nval = 300        // compile error, '300' overflows the maximum allowed value of 'val'\n\nval = 1          // max=1,min=1 sbits=2, ubits=1\nassert val.[ubits] == 1 and val.[min]==1 and val.[max]==1 and val.[sbits]==2\n\nval::[wrap] = 0x1F0 // Drop bits from 0x1F0 to fit in constrained type\nassert val == 240 == 0xF0\n\nval = u8(0x1F0)    // same\nassert val == 0xF0\n</code></pre> <p>Pyrope leverages LiveHD bitwidth pass to compute the maximum and minimum value of each variable. For each operation, the maximum and minimum are computed. For control-flow divergences, the worst possible path is considered.</p> <pre><code>var a = 3                  // a: current(max=3,min=3) constrain()\nvar c:int(0..=10) = _      // c: current(max=0,min=0) constrain(max=10,min=0)\nif b {\n  c = a+1                  // c: current(max=4,min=4) constrain(max=10,min=0)\n}else{\n  c = a                    // c: current(max=3,min=3) constrain(max=10,min=0)\n}\n                           // c: current(max=4,min=3) constrain(max=10,min=0)\n\nvar e::[sbits = 4] = _     // e: current(max=0,min=0) constrain(max=7,min=-8)\ne = 2                      // e: current(max=2,min=2) constrain(max=7,min=-8)\nvar d = c                  // d: current(max=4,min=3) constrain()\nif d==4 {\n  d = e + 1                // d: current(max=3,min=3) constrain()\n}\nvar g:u3 = d               // g: current(max=4,min=3) constrain(max=7,min=0)\nvar h = c@[0,1]            // h: current(max=3,min=0) constrain()\n</code></pre> <p>Bitwidth uses narrowing to converge (see internals). The GCD example does not specify the input/output size, but narrowing allows it to work without typecasts.  To understand, the comments show the max/min bitwidth computations.</p> <pre><code>if cmd? {\n  (x,y) = cmd   // x.max=cmd.a.max; x.min = 0 (uint) ; ....\n}elif x &gt; y {\n                // narrowing: x.min = y.min + 1 = 1\n                // narrowing: y.max = x.min - 1\n  x = x - y     // x.max = x.max - x.min = x.max - 1\n                // x.min = x.min - y.max = 1\n}else{          // x &lt;= y\n                // narrowing: x.max = y.min\n                // narrowing: y.min = x.min\n  y = y - x     // y.max = y.max - x.min = y.max\n                // y.min = y.min - x.max = 0\n}\n                // merging: x.max = x.max ; x.min = 0\n                // merging: y.max = y.max ; y.min = 0\n                // converged because x and y is same or smaller at beginning\n</code></pre> <p>The bitwidth pass may not converge to find a valid size even with narrowing. In this case, the programmer must insert a typecast or operation to constrain the bitwidth by typecasting. For example, this could work:</p> <pre><code>reg x = 0\nreg y = 0\nif cmd? {\n  (x,y) = cmd\n}elif x &gt; y {\n  x = x - y\n}else{\n  y = y - x\n}\nx:cmd.a:[wrap] = x  // use cmd.a type for x, and drop bits as needed\ny = cmd.b(y)        // typecast y to cmd.b type (this can add a mux)\n</code></pre> <p>Pyrope uses signed integers for all the operations and transformations, but when the code is optimized it does not need to waste bits when the most significant bit is known to be always zero (positive numbers like u4). The verilog code generation or the synthesis netlist uses the bitwidth pass to remove the extra unnecessary bit when it is guaranteed to be zero. This effectively \"packs\" the encoding.</p>"},{"location":"pyrope/07-typesystem/#variants","title":"Variants","text":"<p>A Pyrope variant is the equivalent of an union type. A variant type spifices a set of types allowed for a given variable. In Pyrope, a variant looks like a tuple where each entry has a different type. Unlike tuples all the \"space\" or bits used are shared because the tuple can have only one entry with data at a given time.</p> <p>Pyrope supports variants but not unions. The difference between typical (like C++) <code>union</code> and <code>variant</code> is that union can be used for a typecast to convert between values, the variant is the same but it does not allow bit convertion. It tracks the type from the assignment, and an error is generated if the incorrect type is accesed. Pyrope requires explicit type conversion with bitwise operations.</p> <p>Variant shares syntax with enums declaration, but the usage and functionality is quite different. Enums do not allow to update values and variants are tuples with multiple labels sharing a single storage location.</p> <p>The main advantage of variant is to save space. This means that the most typical use is in combination with registers or memories, when alternative types can be stored across cycles.</p> <pre><code>let e_type=enum(str:String = \"hello\",num=22)\nlet v_type=variant(str:String, num:int) // No default value in variant\n\nvar vv:v_type = (num=0x65)\ncassert vv.num == 0x64\nlet xx = vv.str                         // compile or simulation error\n</code></pre> <p>The variant variable allows to explicitly or implicitly access the subtype. Variants may not be solved at compile time, and the error will be a simulation error. A <code>comptime</code> directive can force a compile time-only variant.</p> <pre><code>let Vtype=variant(str:String,num:int,b:bool)\n\nlet x1a:Vtype = \"hello\"                 // implicit variant type\nlet x1b:Vtype = (str=\"hello\")           // explicit variant type\n\nvar x2:Vtype:[comptime] = \"hello\"       // comptime\n\ncassert x1a.str == \"hello\" and x1a == \"hello\"\ncassert x1b.str == \"hello\" and x1b == \"hello\"\n\nlet err1 = x1a.num                      // compile or simulation error\nlet err2 = x1b.b                        // compile or simulation error\nlet err3 = x2.num                       // compile error\n</code></pre> <p>As a reference, <code>enums</code> allow to compare for field but not update enum entries.</p> <pre><code>var ee = e_type\nee.str = \"new_string\"       // compile error, enum is immutable\n\nmatch ee {\n == e_type.str { }\n == e_type.num { }\n}\n</code></pre>"},{"location":"pyrope/07-typesystem/#typecasting","title":"Typecasting","text":"<p>To convert between tuples, an explicit setter is needed unless the tuple fields names, order, and types match.</p> <pre><code>let at=(c:string,d:u32)\nlet bt=(c:string,d:u100)\n\nlet ct=(\n  ,d:u32    = _\n  ,c:string = _\n)\n// different order\nlet dt=(\n  ,d:u32    = _\n  ,c:string = _\n  ,setter = proc (ref self, x:at) { self.d = x.d ; self.c = x.c }\n)\n\nvar b:bt=(c=\"hello\", d=10000)\nvar a:at=_\n\na = b          // OK c is string, and 10000 fits in u32\n\nvar c:ct= a    // OK even different order because all names match\n\nvar d:dt = a   // OK, call intitial to type cast\n</code></pre> <ul> <li>To string: The <code>format</code> allows to convert any type/tuple to a string.</li> <li>To integer: <code>variable@[..]</code> for string, range, and bool, union otherwise.</li> <li><code>union</code> allows to convert across types by specifying the size explicitly.</li> </ul>"},{"location":"pyrope/07-typesystem/#introspection","title":"Introspection","text":"<p>Introspection is possible for tuples.</p> <pre><code>a = (b=1,c:u32=2)\nvar b = a\nb.c=100\n\nassert a equals b\nassert a.size == 2\nassert a['b'] == 1\nassert a['c'] equals u32\n\nassert   a has 'c'\nassert !(a has 'foo')\n\nassert a.[id] == 'a'\nassert a.0.[id] == ':0:b' and a.b.[id] == ':0:b'\nassert a.1.[id] == ':1:c' and a.c.[id] == ':1:c'\n</code></pre> <p>Function definitions allocate a tuple, which allows to introspect the function but not to change the functionality. Functions have 3 fields <code>inputs</code>, <code>outputs</code>, <code>where</code>. The <code>where</code> is a function that always returns true if unset at declaration.</p> <pre><code>let fu = fun(a,b=2) -&gt; (c) where a&gt;10 { c = a + b }\nassert fu.[inp] equals ('a','b')\nassert fu.[out] equals ('c')\nassert fu.[where](a=200) and !fu.[where](a=1)\n</code></pre> <p>This means that when ignoring named vs unnamed calls, overloading behaves like this:</p> <pre><code>let x:u32 = fn(a1,a2)\n\nlet model_poly_call = fun(fn, ...args)-&gt;(out) {\n  for f in fn {\n     continue unless f.[inp] does args\n     continue unless f.[out] does out\n     return f(args) when i.[where](args)\n  }\n}\nlet x:u32 = model_poly_call(fn, a1, a2)\n</code></pre> <p>There are several uses for introspection, but for example, it is possible to build a function that returns a randomly mutated tuple.</p> <pre><code>let randomize::[debug] = fun(ref self) {\n  let rnd = import(\"prp/rnd\")\n  for i in ref self {\n    if i equals _:int {\n      i = rnd.between(i.[max],i.[min])\n    }elif i equals _:bool {\n      i = rnd.boolean()\n    }\n  }\n  self\n}\n\nlet x = (a=1,b=true,c=\"hello\")\nlet y = x.randomize\n\nassert x.a==1 and x.b==true and x.c==\"hello\"\ncover  y.a!=1\ncover  y.b!=true\nassert y.c==\"hello\"  // string is not supposed to mutate in randomize()\n</code></pre>"},{"location":"pyrope/07-typesystem/#global-scope","title":"Global scope","text":"<p>There are no global variables or functions in Pyrope. Variable scope is restricted by code block <code>{ ... }</code> and/or the file. Each Pyrope file is a function, but they are only visible to the same directory/project Pyrope files.</p> <p>There are only two ways to access variables outside Pyrope file. The <code>import</code> statement allows referencing public lambdas from other files. The register declarations allow to assign an ID, and other files can access the register by \"reference\".</p>"},{"location":"pyrope/07-typesystem/#import","title":"import","text":"<p><code>import</code> keyword allows to access functions not defined in the current file. Any call to a function or tuple outside requires a prior <code>import</code> statement.</p> <pre><code>// file: src/my_fun.prp\nlet fun1    = fun(a,b) { a+b }\nlet fun2    = fun(a) {\n  let inside = fun() { 3 }\n  a\n}\nlet another = fun(a) { a }\n\nlet mytup = (\n  ,call3 = fun() { puts \"call called\" }\n)\n</code></pre> <pre><code>// file: src/user.prp\na = import(\"my_fun/*fun*\")\na.fun1(a=1,b=2)         // OK\na.another(a=1,2)        // compile error, 'another' is not an imported function\na.fun2.inside()         // compile error, `inside` is not in top scope variable\n\nlet fun1 = import(\"my_fun/fun1\")\nlec fun1, a.fun1\n\nx = import(\"my_fun/mytup\")\n\nx.call3()                // prints call called\n</code></pre> <p>The <code>import</code> points to a file setup code list of public variables or types. The setup code corresponds to the \"top\" scope in the imported file. The import statement can only be executed during the setup phase. The import allows for cyclic dependencies between files as long as there is no true cyclic dependency between variables. This means that \"false\" cyclic dependencies are allowed but not true ones.</p> <p>The import behaves like cut and pasting the imported code. It is not a reference to the file, but rather a cut and paste of functionality. This means that when importing a variable, it creates a copy. If two files import the same variable, they are not referencing the same variable, but each has a separate copy.</p> <p>The import is delayed until the imported variable is used in the local file. There is no order guarantee between imported files, just that the code needed to compute the used imported variables is executed before.</p> <p>The import statement is a filename or path without the file extension. Directories named <code>code</code>, <code>src</code>, and <code>lib</code> are skipped. No need to add them in the path. <code>import</code> stops the search on the first hit. If no match happens, a compile error is generated.</p> <p><code>import</code> allows specialized libraries per subproject.  For example, xx/yy/zz can use a different library version than xx/bb/cc if the library is provided by yy, or use a default one from the xx directory.</p> <pre><code>let a = import(\"prj1/file1\")\nlet b = import(\"file1\")       // import xxx_fun from file1 in the local project\nlet c = import(\"file2\")       // import the functions from local file2\nlet d = import(\"prj2/file3\")  // import the functions from project prj2 and file3\n</code></pre> <p>Many languages have a \"using\" or \"import\" or \"include\" command that includes all the imported functions/variables to the current scope. Pyrope does not allow that, but it is possible to use a mixin to add the imported functionality to a tuple.</p> <pre><code>let b = import(\"prp/Number\")\nvar a = import(\"fancy/Number_mixin\")\n\nlet Number = b ++ a // patch the default Number class\n\nvar x:Number = 3\n</code></pre>"},{"location":"pyrope/07-typesystem/#register-reference","title":"Register reference","text":"<p>While import \"copies\" the contents, <code>regref</code> or Register reference allows to reference (not copy) an existing register in the call hierarchy.</p> <p>The syntax of <code>regref</code> is similar to <code>import</code> but the semantics are very different. While <code>import</code> looks through Pyrope files, <code>regref</code> looks through the instantiation hierarchy for matching register names. <code>regdef</code> only can get a reference to a register, it can not be used to import functions or variables.</p> <pre><code>let do_increase = proc() {\n  reg counter = 0\n\n  counter:u32:[wrap] = counter + 1\n}\n\nlet do_debug = proc() {\n  let cntr = regref(\"do_increase/counter\")\n\n  puts \"The counter value is {}\", cntr\n}\n</code></pre> <p>Verilog has a more flexible semantics with the Hierarchical Reference. It also allows to go through the module hierarchy and read/write the contents of any variable. Pyrope only allows you to reference registers by unique name. Verilog hierarchical reference is not popular for 2 main reasons: (1) It is considered \"not nice\" to bypass the module interface and touch an internal variable; (2) some tools do not support it as synthesizable; (3) the evaluation order is not clear because the execution order of the modules is not defined.</p> <p>Allowing only a single lambda to update registers avoids the evaluation order problem. From a low level point of view, the updates go to the register <code>din</code> pin, and the references read the register <code>q</code> pin. The register references follow the model of single writer multiple reader.  This means that only a single lambda can update the register, but many lambdas can read the register. This allows to be independent on the <code>lambda</code> evaluation order.</p> <p>The register reference uses instantiated registers. This means that if a lambda having a register is called in multiple places, only one can write, and the others are reading the update. It is useful to have configuration registers. In this case, multiple instances of the same register can have different values. As an illustrative example, a UART can have a register and the controller can set a different value for each uart base register.</p> <pre><code>// file remote.prp\n\nlet xxx = proc(some,code) {\n  reg uart_addr:u32 = _\n  assert 0x400 &gt; uart_addr &gt;= 0x300\n}\n\n// file local.prp\nlet setup_xx = proc() {\n  var xx = regref(\"uart_addr\") // match xxx.uart_addr if xxx is in hierarchy\n  var index = 0\n  for val in ref xx {          // ref does not allow enumerate\n    val = 0x300+index*0x10     // sets uart_addr to 0x300, 0x310, 0x320...\n    index += 1\n  }\n}\n</code></pre> <p>Maybe the best way to understand the <code>regdef</code> is to see the differences with the <code>import</code>:</p> <ul> <li>Instantiation vs File hierarchy</li> <li><code>regref</code> finds matches across instantiated registers.</li> <li><code>import</code> traverses the file/directory hierarchy to find one matche.</li> <li>Success vs Failure</li> <li><code>regref</code> keeps going to find all the matches, and it is possible to have a zero matches</li> <li><code>import</code> stops at the first match, and a compile error is generated if there is no match.</li> </ul>"},{"location":"pyrope/07-typesystem/#mocking-library","title":"Mocking library","text":"<p>One possible use of the register reference is to create a \"mocking\" library. A mocking library instantiates a large design but forces some subblocks to produce some results for testing. The challenge is that it needs undriven registers. During testing, the <code>peek</code>/<code>poke</code> is more flexible and it can overwrite an existing value. The peek/poke use the same reference as <code>import</code> or register reference.</p> <pre><code>let bpred = ( // complex predictor\n  ,let taken = fun(){ self.some_table[som_var] &gt;=0 }\n)\n\ntest \"mocking taken branches\" {\n  poke \"bpred_file/taken\", true\n\n  var l = core.fetch.predict(0xFFF)\n}\n</code></pre>"},{"location":"pyrope/07-typesystem/#operator-overloading","title":"Operator overloading","text":"<p>There is no operator overload in Pyrope. <code>+</code> always adds Numbers, <code>++</code> always concatenates a tuple or a String, <code>and</code> is always for boolean types,...</p>"},{"location":"pyrope/07-typesystem/#gettersetter-method","title":"Getter/Setter method","text":"<p>Pyrope tuples can use the same syntax as a lambda call or a direct assignment. Both the assignment and the lambda call follow the same rules for ambiguity as the default lambda calls. This means that fields must be named unless single character names, or variable name matches argument name, or there is no type ambiguity.</p> <pre><code>let Typ1 = (\n  ,a:string = \"none\"\n  ,b:u32    = 0\n)\n\nlet w      = Typ1(a=\"foo\", b=33)  // OK\nlet x:Typ1 = (a=\"foo\", b=33)      // OK, same as before\n\nlet v:Typ1 = Typ1(a=\"foo\", b=33)  // OK, but redundant Typ1\nlet y:Typ1 = (\"foo\", 33)          // OK, because no conflict by type\n\nvar z:Typ1 = _                    // OK, default field values\ncassert z.a == \"none\" and z.b == 0\nz = (\"foo\",33)\n\ncassert v==w==x==y==z\n</code></pre> <p>Pyrope allows a setter method to intercept assignments or construction. The same setter method is called in all the previous cases.</p> <p>The setter method can use single character arguments for array index, but they must respect the declaration order.</p> <pre><code>let Typ2 = (\n  ,a:string = \"none\"\n  ,b:u32    = 0\n  ,setter = proc(ref self, a, b) { self.a = a ; self.b = b }\n)\n\nvar x:Typ2 = (a=\"x\", b=0)\nvar y:Typ2 = (a=\"x\", b=0)\n\nx[\"hello\"] = 44\ny = (\"hello\",44)\ncassert x==y\n</code></pre> <p>Tuples can be multi-dimensional, and the index can handle multiple indexes at once.</p> <pre><code>let Matrix8x8 = (\n  ,data:[8][8]u16 = _\n  ,setter = fun(ref self, x:int(0,7), y:int(0,7), v:u16) {\n    self.data[x][y] = v\n  } ++ fun(ref self, x:int(0,7), v:u16) {\n    for ent in ref data[x] {\n      ent = v\n    }\n  } ++ fun(ref self) { // default initialization\n    for ent in ref data {\n      ent = 0\n    }\n  }\n)\n\nlet m:Matrix8x8 = _\ncassert m.data[0][3] == 0\n\nm[1,2] = 100\ncassert m.data[1][2] == 100\nm[1] = 3\ncassert m.data[1][2] == 3\nm[4][5] = 33\ncassert m.data[4][5] == 33\n\nm[1] = 40\ncassert m[1] == (3,40,3,3,3,3,3,3)\n</code></pre> <p>The default <code>getter</code>/<code>setter</code> allows for indexing each of the dimentions and returns a slice of the object. Since they can be overwritten, the explicit overload selects which to pick.</p> <pre><code>let Matrix2x2 = (\n  ,data:[2][2]u16 = _\n  ,getter = fun(ref self, x:int(0,2), y:int(0,2)) {\n    self.data[x][y] + 1\n  }\n\n)\n\nlet n:Matrix2x2 = _\nn.data[0][1] = 2      // default setter\n\ncassert n[0][1] == 3  // getter does + 1\ncassert n[0] == (0,3) // compile error, no getter for fun(ref self, x)\n</code></pre> <p>The symmetric getter method is called whenever the tuple is read. Since each variable or tuple field is also a tuple, the getter/setter allow to intercept any variable/field. The same array rule applies to the getter.</p> <pre><code>let My_2_elem = (\n  ,data:[2]string = _\n  ,setter = proc(ref self, x:uint(0..&lt;2), v:string) {\n    self.data[x] = v\n  } ++ proc(ref self, v:My_2_elem) {\n    self.data = v.data\n    } ++ proc(ref self) { // default _ assignment\n      self.data = _\n    }\n  ,getter = fun(self)         { self.data    }\n         ++ fun(self, i:uint) { self.data[i] }\n)\n\nvar v:My_2_elem = _\nvar x:My_2_elem = _\n\nv = (x=0, \"hello\")\nv[1] = \"world\"\n\ncassert v[0] == \"hello\"\ncassert v == (\"hello\", \"world\")  // not\n\nlet z = v\ncassert z !equals v   // v has v.data, z does not\n</code></pre> <p>The getter/setter can also be used to intercept and/or modify the value set/returned.</p> <pre><code>let some_obj = (\n  ,a1:string\n  ,a2 = (\n    ,_val:u32 = _                               // hidden field\n\n    ,getter=fun(self) { self._val + 100 }\n    ,setter=proc(ref self, x) { self._val = x+1 }\n  )\n  ,setter = proc(ref self, a,b){                  // setter\n    self.a1      = a\n    self.a2._val = b\n  }\n)\n\nvar x:some_obj = (\"hello\", 3)\n\nassert x.a1 == \"hello\"\nassert x.a2 == 103\nx.a2 = 5\n</code></pre> <p>The getter method can be overloaded. This allows to customize by return type:</p> <pre><code>let showcase = (\n  ,v:int = _\n  ,getter = fun(self)-&gt;(_:string) where self.i&gt;10 {\n    format(\"this is a big {} number\", self.v)\n  } ++ fun(self)-&gt;(_:int) {\n    self.v\n  }\n)\n\nvar s:showcase = _\ns.v = 3\nlet r1:string = s // compile error, no matching getter\nlet r2:int    = s // OK\n\ns.v = 100\nlet r3:string = s // OK\ncassert r3 == \"this is a bit 100 number\"\n</code></pre> <p>Like all the lambdas, the getter method can also be overloaded on the return type. In this case, it allows building typecast per type.</p> <pre><code>let my_obj = (\n  ,val:u32 = _\n  ,getter = fun(self)-&gt;(_:string ){ string(self.val) }\n       ++ fun(self)-&gt;(_:bool){ self.val != 0    }\n       ++ fun(self)-&gt;(_:int    ){ self.val         }\n)\n</code></pre>"},{"location":"pyrope/07-typesystem/#attribute-settergetter-value","title":"Attribute setter/getter value","text":"<p>The setter/getter can also access attributes:</p> <pre><code>var obj1::[attr1] = (\n  ,data:int = _\n  ,setter = fun(ref self, v) {\n    if v.[attr2] {\n      self.data.[attr3] = 33\n    }\n    cassert self.[attr1]\n  }\n)\n</code></pre>"},{"location":"pyrope/07-typesystem/#default-setter-value","title":"Default setter value","text":"<p>All the variable declarations need a explicit assigned value. The <code>_</code> allows to pick the default value based on the type. If the type is an integer, the <code>_</code> is equivalent to a zero. If the type is a boolean, the default or <code>_</code> is false. For more complicated tuple types, the setter will be called without any value.</p> <pre><code>let fint:int = _\ncassert fint == 0\n\nvar fbool:bool = _\ncassert fbool == 0\n\nlet Tup = (\n  ,v:string = _  // default to empty\n  ,setter = fun(ref self) { // no args, default setter for _\n     cassert self.v == \"\"\n     self.v = \"empty33\"\n  } ++ fun(ref self, v) {\n     self.v = v\n  }\n)\n\nvar x:Tup = _\ncassert x.v == \"empty33\"\n\nx = \"Padua\"\ncassert x.v == \"Padua\"\n\nvar y = Tup()\ncassert y.v == \"empty33\"\n\ny = \"ucsc\"\ncassert y.v == \"ucsc\"\n</code></pre>"},{"location":"pyrope/07-typesystem/#arraytuple-gettersetter","title":"Array/Tuple getter/setter","text":"<p>Array index also use the setter or getter methods.</p> <pre><code>var my_arr = (\n  ,vector:[16]u8 = 0\n  ,getter = fun(self, idx:u4) {\n     self.vector[idx]\n  }\n  ,setter = proc(ref self, idx:u4, val:u8) {\n     self.vector[idx] = val\n  } ++ proc(ref self) {\n     // default constructor declaration\n  }\n)\n\nmy_arr[3] = 300           // calls setter\nmy_arr.3  = 300           // calls setter\ncassert my_add[3] == 300  // calls getter\ncassert my_add.3  == 300  // calls getter\n</code></pre> <p>Unlike languages like C++, the setter is only called if there is a new value assigned. This means that the index must always be in the left-hand-side of an assignment.</p> <p>If the getter/setter uses a string argument, this also allows to access tuple fields.</p> <pre><code>let Point = (\n  ,priv_x:int:[private] = 0\n  ,priv_y:int:[private] = 0\n\n  ,setter = proc(ref self, x:int, y:int) {\n    self.priv_x = x\n    self.priv_y = y\n  }\n\n  ,getter = proc(self, idx:string) {\n    match idx {\n     == 'x' { self.priv_x }\n     == 'y' { self.priv_y }\n    }\n  }\n)\n\nlet p:Point = (1,2)\n\ncassert p['x'] == 1 and p['y'] == 2\ncassert p.x == 1 and p.y == 2          // compile error\n</code></pre>"},{"location":"pyrope/07-typesystem/#compare-method","title":"Compare method","text":"<p>The comparator operations (<code>==</code>, <code>!=</code>, <code>&lt;=</code>,...) need to be overloaded for most objects. Pyrope has the <code>lt</code> and <code>eq</code> methods to build all the other comparators. When non-provided the <code>lt</code> (Less Than) is a compile error, and the <code>eq</code> (Equal) compares that all the tuple fields are equal.</p> <pre><code>let t=(\n  ,v:string = _\n  ,setter = proc(ref self) { self.v = a }\n  ,lt = fun(self,other)-&gt;(_:bool){ self.v  &lt; other.v }\n  ,eq = fun(self,other)            { self.v == other.v } // infer return\n)\n\nvar m1:t = 10\nvar m2:t = 4\nassert m1 &lt; m2 and !(m1==m2)\nassert m1 &lt;= m2 and m1 != m2 and m2 &gt; m1 and m2 &gt;= m1\n</code></pre> <p>The default tuple comparator (<code>a == b</code>) compares values, not types like <code>a does b</code>, but a compile error is created unless <code>a equals b</code> returns true. This means that a comparison by tuple position suffices even for named tuples.</p> <pre><code>let t1=(\n  ,long_name:string = \"foo\"\n  ,b=33\n)\nlet t2=(\n  ,b=33\n  ,long_name:string = \"foo\"\n)\nlet t3=(\n  ,33\n  ,long_name:string = \"foo\"\n)\n\ncassert t1==t2\ncassert t1 !equals t3\nlet x = t1==t3           // compile error, t1 !equals t3\n</code></pre> <p>The comparator <code>a == b</code> when <code>a</code> or <code>b</code> are tuples is equivalent to: <pre><code>cassert (a==b) == ((a in b) and (b in a))\ncassert a equals b\n</code></pre></p> <p>With the <code>eq</code> overload, it is possible to compare named and unnamed tuples.</p> <pre><code>let t1=(\n  ,long_name:string = \"foo\"\n  ,b=33\n)\nlet t2=(\n  ,xx_a=33\n  ,yy_b = \"foo\"\n  ,eq = fun(self, o:t1) {\n    return self.xx_a == o.b and self.xx_y == o.long_name\n  } ++ fun(self, o:t2) {\n    return self.xx_a == o.xx_a and self.xx_y == o.xx_y\n  }\n)\n\ncassert t1==t2 and t2==t1\n</code></pre> <p>Since <code>a == b</code> can compare two different objects, it is not clear if <code>a.eq</code> or <code>b.eq</code> method is called. Pyrope has the following rule:</p> <ul> <li>If only one of the two has a defined method, that method is called.</li> <li>If both have defined methods, they should have the same set of <code>eq</code> methods or a compile error is created.</li> </ul> <p>It is also possible to provide a custom <code>ge</code> (Greater Than). The <code>ge</code> is redundant with the <code>lt</code> and <code>eq</code> (<code>(a &gt;= b) == (a==b or b&lt;a)</code>) but it allows to have more efficient implemetations:</p> <p>For integer operations, the Pyrope should result to the following equivalent Lgraph:</p> <ul> <li><code>a == b</code> is <code>__eq(a,b)</code></li> <li><code>a != b</code> is <code>__not(__eq(a,b))</code></li> <li><code>a  &lt; b</code> is <code>__lt(a,b)</code></li> <li><code>a  &lt; b</code> is <code>__lt(b,a)</code></li> <li><code>a &lt;= b</code> is <code>__lt(a,b) | __eq(a,b)</code> (without <code>ge</code>) or <code>__ge(b,a)</code></li> <li><code>a &gt;= b</code> is <code>__lt(b,a) | __eq(a,b)</code> (without <code>ge</code>) or <code>__ge(a,b)</code></li> </ul>"},{"location":"pyrope/07-typesystem/#non-pyrope-c-calls","title":"Non-Pyrope (C++) calls","text":"<p>Calling C++ or external code is still fully synthesizable if the code is available at compile time. An example could be calling a C++ API to read a json file during the setup phase to decide configuration parameters.</p> <pre><code>let cfg = __read_json()\n\nlet ext = if cfg.foo.bar == 3 {\n   foo\n}else{\n   bar\n}\n</code></pre> <p>Non-Pyrope calls have the same procedure/function distinction and use the same Pyrope lambda definition but they do not have the <code>where</code> clause.</p> <p>If no type is provided, a C++ call assumes a <code>proc(...inp)-&gt;(...out)</code> type is can pass many inputs/outputs and has permission to mutate values. Any call to a method with two underscores <code>__</code> is either a basic gate or a C++ function.</p> <pre><code>let __my_typed_cpp:fun(a,b)-&gt;(e) = _\n</code></pre> <p>Type defining non-Pyrope code is good to catch errors and also because declaring <code>function</code> allows to handle several cases of circular dependencies not possible with <code>procedure</code> import section</p>"},{"location":"pyrope/07b-structtype/","title":"Structural Typing","text":"<p>Pyrope uses structural typing somewhat simular to other languages like Typescript, but there are some difference simplifications like not references, everything is passed by value, no union types.</p>"},{"location":"pyrope/07b-structtype/#type-check","title":"Type check","text":"<p>The <code>x does y</code> checks that <code>x</code> does the same as <code>y</code> and maybe more. It type system syntax means that <code>x</code> is a subtype of <code>y</code>, or that <code>y</code> is a supertype <code>x</code>.</p> <p>Using the typical <code>Animal</code>, <code>Dog</code>, <code>Greyhound</code> set of tuples, <code>Dog does Animal</code> and <code>Greyhound does Dog</code>, but <code>not (Animal does Dog)</code>.</p> <p>Dealing with tuple assignments <code>y = x</code>, a compile error is generated unless the type system satisfies <code>y does x</code> or an explicit type conversion is provided. The basic behavior of <code>does</code> is explained in (Type equivalance)[07-typesystem.md#Type_equivalence].</p> <pre><code>let Animal = (\n  ,legs:int = _\n  ,name= \"unnamed\"\n  ,say_name = fun() { puts name }\n)\n\nlet Dog = Animal ++ (\n  ,setter = proc(ref self) { self.legs = 4 }\n  ,bark = fun() { puts \"bark bark\" }\n)\n\nlet Bird = Animal ++ (\n  ,seeds_eaten:int = _\n\n  ,setter = proc(ref self)  { self.legs = 2 }\n      ++ proc(ref self, a:Animal)    { self.legs = 2 ; name = \"bird animal\" }\n  ,eat_seeds = proc(ref self, n) { self.seeds_eaten += n }\n)\n\nlet Greyhound = Dog ++ ( // also extends Dog\n  ,race = fun() { puts \"running fast\" }\n)\n</code></pre> <pre><code>var a:Animal = _\nvar b:Bird = _\nvar d:Dog = _\n\nd = a // compile error, 'a does d' is false\nb = a // OK, explicit setter in Bird for Animal\n\na = d // OK, 'd does a' is true\na = b // OK, 'Bird does Animal' is true\n</code></pre> <p>When the <code>x</code> in <code>x = y</code> is an <code>integer</code> basic type, there is an additional check to guarantee that no precision is lost. Otherwise, an explicit <code>wrap</code> or <code>drop</code> directive must be used.</p>"},{"location":"pyrope/07b-structtype/#arrays","title":"Arrays","text":"<p>The same rules of assignments exists for arrays. In Pyrope, arrays can be mutable, but they can never be passed by reference. This means that the typical issue of mutable containers can not exists. </p> <pre><code>var a_vec:[]Animal = _\nvar b_vec:[]Bird = _\nvar d_vec:[]Dog = _\n\na_vec[0] = d:Dog    // OK\na_vec[1] = b:Bird   // OK\n\nd_vec[0] = d:Dog        // OK  'd does d'\nd_vec[0] = g:Greyhound  // OK  'g does d'\nd_vec[0] = b:Bird       // Compile error\nd_vec[0] = a:Animal     // Compile error\n\nb_vec[0] = d:Dog        // OK, explicit conversion\nb_vec[0] = g:Greyhound  // OK, explicit conversion\nb_vec[0] = b:Bird       // OK, 'b does b'\nb_vec[0] = a:Animal     // OK, explicit conversion\n\nlet do_animal_vec = fun(a_vec:[]Animal)-&gt;(r:[]Animal) {\n  r = a_vec\n  r[0] = d:Dog  // OK `d does r[0]`\n}\n\nvar x = do_animal_vec(b_vec:[]Bird) // OK\nassert x does _:[]Animal  // not :[]Bird\n</code></pre>"},{"location":"pyrope/07b-structtype/#basic-types","title":"Basic types","text":"<p>One of the complains about structural type system is that two types with exactly the same tuple fields have the same type. In Pyrope, the field name should match. Since every element is a type of one, read/writing a named tuple of one does not need the field, and hence it allows to create different types:</p> <pre><code>let Age = (\n  ,age:int = _\n)\nlet Weight = (\n  ,weight:int = _\n)\n\nassert Age !does Weight\n\nvar a:Age = 3\nassert a == a.age == a.0 == 3\n\nvar w:Weight = 100\n\nlet err = a == w // compile error, not (a equals w) or overload\n</code></pre>"},{"location":"pyrope/07b-structtype/#lambda","title":"Lambda","text":"<p>A way to classify a language is to look at the generics and lambda calls. Languages can have type constraints or type classes. Type classes (Hakell, Rust, Swift) specify the \"consent\" of argumetns or return types allowed for lambda or generic. Type constrains (C++, typescript, Pyrope) constraints the arguments or return types allowed. Pyrope follows a type constraint approach.</p> <p>The following <code>f</code> method has no constraints on the input arguments. It can pass anything, but constraints the return value to be an integer.</p> <pre><code>let f = fun(a,b) -&gt; (r:int) { r = xx(a) + xx(b) }\n</code></pre> <p>The type can be inferred for arguments and return values. If the lambda definition has no type constraints. A \"different\" implementation lambda exist for each combination of inferred types. It behaves like if the the lambda were inlined in the caller.</p> <p>The constraints can be different per type, or use a more familiar generic syntax. The <code>f1</code> example constraints <code>a</code> and <code>b</code> arguments to have a type that satisfies <code>(a does Some_type_class) and (b does Some_type_class)</code>.</p> <pre><code>let f1 = fun&lt;T:Some_type_class&gt;(a:T,b:T) -&gt; (r:int) { r = xx(a) + xx(b) }\n</code></pre> <p>While performing assignments checks that the left-hand-side tuple fields are fully populated (<code>x=y</code>) by checking that <code>y does x</code>. The same check happens for the lambda calls, but the check is performed when a lambda is passed as an argument.</p> <p>For each lambda call (<code>ret_val = f(a1,a2)</code>), the type system check against the defined lambda (<code>f = fun(ad1:ad1_t, ad2)-&gt;(rd1:rd1_t, rd2)</code>). In this case, the check for the calling arguments (<code>(a1,a2) does (:ad1_t, :())</code>) should be satisfied. Notice that some of the inputs (<code>ad2</code>) have no defined type, so those unspecified arguments always satisfies by the type check. </p> <p>The return tuple is also used in the type system (<code>ret_val does (:rd1_t, :())</code>), the check is the same as in an assignment (<code>lhs does rhs</code>). In overloading cases explained later, the return type could also be part of the overloading check.</p> <pre><code>let fa_t:fun(a:Animal)-&gt;() = _\nlet fd_t:fun(d:Dog)-&gt;() = _\n\nlet call_animal = fun(a:Animal)-&gt;() {\n   puts a.name // OK\n}\nlet call_dog:fd_t = fun(d:Dog)-&gt;() {    // OK to add type in lhs\n   d.bark()    // OK\n}\n\nlet f_a = fun(fa:fa_t) { \n  var a:Animal = _\n  var d:Dog = _\n  fa(a)  // OK\n  fa(d)  // OK, `d does Animal` is true\n}\nf_a(call_animal) // OK\nf_a(call_dog)    // compile error, `fa_t does call_dog` is false\n\nlet f_d = fun(fd:fd_t) { \n  var a:Animal = _\n  var d:Dog = _\n  fd(a)  // compile error, `a does Dog` is false\n  fd(d)  // OK\n}\nf_d(call_animal) // OK, `fd_t does call_animal` is true\nf_d(call_dog)    // OK\n</code></pre> <p>In tuple comparisons, <code>does</code> and <code>==</code>, the tuple field position is not used when both tuples are fully named. If tuple field is unnamed, both existing names and positions should match in the comparison.  For fully named tuples, when all the fields have names,  <code>(a=1,b=2) does (b=2,a=1)</code> is true. </p> <p>The same rule also applies to lambda calls. If all the arguments are named, the relative call argument position is independent. If an argument is an expression or unnamed, the position is important.</p> <p>A special case is the in-place operator (<code>...</code>) during lambda definition.  Even for fully named tuples, the position is used.  One one in-place operator is allowed per lambda definition <code>(a,b,...x,c)</code>, the <code>does</code> operator uses name and position like in unnamed tuples even if all the fields are named. First, it matches the position and names provided, and then checks the rest to the in-place with the relative order left.</p> <pre><code>let m = fun(a:int,...x:(_:string,c:int,d), y:int)-&gt;() { \n  assert a == 1\n  assert x.0 == \"here\"\n  assert x.1 == 2 == x.c\n  assert y == 3\n  if d does int { // inferred type\n    assert d == 33\n  }else{\n    assert d == \"x\"\n  }\n}\n\nm(1,\"here\",2,\"x\",3)         // OK\nm(a=1,\"here\",2,\"x\",3)       // OK\nm(a=1,\"here\",c=2,\"x\",3)     // OK\nm(a=1,\"here\",c=2,33,y=3)    // OK\n\nm(\"1\",\"here\",2,33,3)       // compile error, a:int\nm(\"1\",\"here\",2,3)          // compile error, x has 3 fields\n</code></pre> <p>For all the checks that are not function reference or in-place, the <code>x does y</code> check could be summarized as <code>x</code> is a superset of <code>y</code>. <code>x</code> has all the functionality of <code>y</code> and maybe more. In a more formal compiler nomenclature <code>x does y</code> applied to tuples is called a covariant relationship. It is covariant because adding the same extra fields to both <code>x</code> and <code>y</code> keeps the semantics (<code>((foo=3,...x) does (foo=3,...y)) == x does y</code>). This allows to extend the tuple semantics and the relationship is preserved.</p> <p>When <code>x</code> and <code>y</code> are in a lambda passed as reference to another lambda (lambda reference), the relationship is not covariant but contravariant. <code>Dog does Animal</code> is true, but <code>:fun(x:Dog)-&gt;() does _:fun(x:Animal)-&gt;()</code> is false. The reason is shown in the previous example. The <code>fun(fd:fd_t)</code> can be called with <code>call_animal</code> because the fields accessed by <code>call_animal</code> are only a subset of <code>Dog</code> and hence if called inside <code>f_d</code> it can handle the <code>Dog</code> type. The opposite is not the case.</p> <p><code>:fun(x1)-&gt;(x2) does _:fun(y1)-&gt;y2</code> check is equivalent to <code>(y1 does x1) and (x2 does y2)</code>.</p> <p>Given a lambda passed as argument (<code>:fun(x:fun(c:c_t)-&gt;(d:d_t))-&gt;(y)</code>), the check when passing the lambda as argument to <code>x</code> a function like <code>fun(w:w_t)-&gt;(z:z_t)</code>. In this case, the <code>:fun(:w_t)-&gt;(_:z_t) does fun(:c_t)-&gt;(_:d_t)</code> is a contravariant test for inputs and covariant for outputs. This makes it equivalent to <code>(_:c_t does _:w_t) and (_:z_t does _:d_t)</code>.</p> <p>If the same type is used as input and output is an equivalence check (<code>((a does b) and (b does a)) == (a equals b)</code>). In programming languages this is called an invariance or bivariance.</p> <p>Pyrope uses the typical check in modern languages where the function arguments are contravariant and the return type is covariant. In Pyrope, the return type is checked in the covariant and contravariant checks.</p>"},{"location":"pyrope/07b-structtype/#lambda-overloading","title":"Lambda overloading","text":"<p>Pyrope does not have global scope for defined lambdas. Instead, all the lambda must reside in a local variable or must be \"imported\". Nevertheless, a local variable can have multiple lambdas. It is similar to Odin's \"explicit procedure overloading\". This section explains how is the overloading selection in this case.</p> <p>By overloading, this section refers to typical ad-hoc polymorphism where the same function/procedure name can have different functionality for different types.</p> <p>For Pyrope overloading, lambdas are typically added at the end <code>++=</code> of the tuple. This means that it is NOT overwriting an existing functionality, but providing a new call capability.</p> <p>There is a priority of overloading in the tuple order. If the intention is to intercept, the lambda must be added at the head of the tuple entry.</p> <pre><code>let base = (\n  ,fun1 = fun() { 1 }         // catch all\n  ,fun2 = fun() { 2 }         // catch all\n  ,fun3 = fun() { 3 }         // catch all\n)\nlet ext = base ++ (\n  ,fun1 =   fun (a,b){ 4 }  // overwrite allowed with extends\n  ,fun2 ++= fun (a,b){ 5 }  // append\n  ,fun2 ++= fun ()   { 6 }  // append\n  ,fun3 =   fun(a,b) { 7 } ++ base.fun3 // prepend\n  ,fun3 =   fun()    { 8 } ++ base.fun3 // prepend\n)\n\nvar t:ext = _\n\n// t.fun1 only has ext.fun1\nassert t.fun1(a=1,b=2) == 4\nt.fun1()                 // compile error, no option without arguments\n\n// t.fun2 has base.fun2 and then ext.fun2\nassert t.fun2(1,2) == 5  // EXACT match of arguments has higher priority\nassert t.fun2() == 2     // base.fun2 catches all ahead of ext.fun2\n\n// t.fun3 has ext.fun3 and then base.fun3\nassert t.fun3(1,2) == 7  // EXACT match of arguments has higher priority\nassert t.fun3() == 8     // ext.fun3 catches all ahead of ext.fun3\n</code></pre> <p>A more traditional \"overload\" calling the is possible by calling the lambda directly:</p> <pre><code>let x = base ++ (\n  ,fun1 = fun() { return base.fun1() + 100 }\n)\n</code></pre> <p>To allow overloading the base <code>lambda</code> as <code>var</code>. By concatenating lambdas to a variable, we effectively create an unnamed tuple with multiple entries. Since all the variables are tuples of size one too, the following rules apply to any lambda call:</p> <ul> <li> <p>Given a lambda call <code>f(a:a_t)-&gt;(_:r_t)</code> with defined call and return types.   Iterate and pick all the lambda definitions <code>f(x)-&gt;(y)</code> that satisfy <code>x does   a_t and y does r_t</code> using the previously explained lambda checks.</p> </li> <li> <p>If the <code>r_t</code> is unknown at call time, use only the call arguments <code>x does   a_t</code>. Check that all the matching lambdas have the same defined return type.   Otherwise a compile error is generated indicating that the type can not be   infered.</p> </li> <li> <p>If the list is empty, generate a compile error (no possible lambda to call).</p> </li> <li> <p>Once a list of ordered modules is found, evaluate the <code>where COND</code>. <code>COND</code>   can include inputs, self, and outputs. If a <code>COND</code> is comptime true (no   <code>COND</code> is the same as <code>true</code>), stop selecting additional modules. If <code>COND</code>   is comptime <code>false</code> remove from the list and continue. All the selected   modules will be executed, but the output will be selected based on priority   order based on the <code>COND</code> result at runtime.</p> </li> <li> <p>If the list has more than one entry, and any of them is a <code>proc</code>, generate a   compile error. Dynamic dispatch only works with functions <code>fun</code>.</p> </li> </ul> <p>If the <code>where COND</code> is not compile time there must be a <code>where true</code> condition to catch the default behavior. </p> <p>The previous rules imply that Pyrope has some type of dynamic dispatch. The types for the inputs and outputs must be known at compile time (static dispatch) but the <code>where</code> condition may be known at run-time as long as the lambda is immutable (<code>fun</code>).</p> <p>The <code>where</code> condition is not considered part of the type system, but a syntax sugar to allow several function implementations depending on some condition. The alternative and equivalent syntax is to add all the <code>if/else</code> chain at every call but this result in not so maintanable code.</p> <pre><code>var fun_list = fun(a,b){ a+b}\nfun_list ++= fun(a,b,c){ a+b+c }\nfun_list ++= fun(a,b,c,d){ a+b+c+d }\n\nassert fun_list.[size] == 3    // 3 lambda entries in fun_list\n\nassert fun_list(1,2) == 3\nassert fun_list(1,2,4) == 7\nassert fun_list(1,2,4,5) == 12\nassert fun_list(1,2,4,5,6) == 18 // compile error, no function with 5 args\n\n\nfun_list ++= fun(a,b){ 100}\nassert fun_list(1,2) == 3\n\nfun_list = fun(a,b){ 200} ++ fun_list\nassert fun_list(1,2) == 200\n</code></pre> <p>For untyped named argument calls:</p> <pre><code>var f1 = fun(a,b){ a+b+100 }\n  f1 ++= fun(x,y){ x+y+200 }\n\nassert f1(a=1,b=2) == 103\nassert f1(x=1,y=2) == 203\nassert f1(  1,  2) == 103  // first in list\n</code></pre> <p>For typed calls:</p> <pre><code>var fo = fun(a:int,b:string)-&gt;(_:bool)  { true    }\n  fo ++= fun(a:int,b:int   )-&gt;(_:bool)  { false   }\n  fo ++= fun(a:int,b:int   )-&gt;(_:string){ \"hello\" }\n\nlet a = fo(3,hello)\nassert a == true\n\nlet b = fo(3,300)        // first in list return bool\nassert b == false\n\nlet c:int = fo(3,300)    // compile error, no lambda fulfills constrains\nlet c:string = fo(3,300)\nassert c == \"hello\"\n</code></pre> <p>For conditional argument calls:</p> <pre><code>var f1 = fun(a,b)      where a &gt;  40 { b+100    }\n      ++ fun(a,b)-&gt;(x) where x &gt; 300 { b+200    } // output x\n      ++ fun(a,b)-&gt;(a) where a &gt;  20 { b+300    } // input a\n      ++ fun(a,b)-&gt;(x) where x &gt;  10 { b+400    } // output x\n      ++ fun(a,b)                    { a+b+1000 } // default\n\nvar fun_manual = fun(a,b){  // equivalent but not as maintenable\n  if a&gt;40 {\n    return b+100\n  }\n  let x = b + 200\n  if x&gt;300 {\n    return (x=x)\n  }\n  if a&gt;20 {\n    return b+300\n  }\n  let tmp = a + b\n  if tmp &gt;10 {\n    return (a=tmp)\n  }\n  return a+b+1000\n}\n\ntest \"check equiv\" {\n  for a in -100..=100 {\n    for b in -100..=100 {\n      assert f1(a,b) == fun_manual(a,b)\n    }\n  }\n}\n</code></pre>"},{"location":"pyrope/07b-structtype/#parametric-polymorphism","title":"Parametric polymorphism","text":"<p>Add-hoc polymorphism overloads a function, and parametric polymorphism allows to  parametrize types based on arguments.</p> <pre><code>let Param_type = fun(a) { return (xx:a = _) }\n\nlet x:Param_type(string) = (xx=\"hello\")\nlet x:Param_type(int)    = (xx=130)\n</code></pre>"},{"location":"pyrope/07b-structtype/#summary-polymorphism","title":"Summary polymorphism","text":"<p>Subtype polymorphism: A subtype provides functionality/api for another super type. <pre><code>let Animal = (\n  ,speak:fun(self) = _\n)\nlet Cat: Animal = (\n  ,speak = fun(self) { puts \"meaow\" }\n)\nlet Bird: Animal = (\n  ,speak = fun(self) { puts \"pio pio\" }\n)\n</code></pre></p> <p>Parametric polymorphism: Same function works for many types <pre><code>let smallest = fun(...a) {\n  let x = a[0]\n  for i in a[1..] {\n    x = i when i &lt; x\n  }\n  return x\n}\n</code></pre></p> <p>Ad-hoc polymorphism: capacity to overload the same lambda name with different types. <pre><code>let speak = fun(a:Bird) { puts \"pio pio\"  } \n         ++ fun(a:Cat) { puts \"meaow\" }\n</code></pre></p> <p>Coercion polymorphism: Capacity to cast a type to another <pre><code>let Type1 = (\n  ,setter = fun(ref self, a:int) {  }\n)\nlet a:Type1 = 33\n</code></pre></p>"},{"location":"pyrope/07b-structtype/#traits-and-mixin","title":"Traits and mixin","text":"<p>There is no object inheritance in Pyrope, but tuples allow to build mixin and composition.</p> <p>A mixin is when an object or class can add methods and the parent object can access them. In several languages, there are different constructs to build them (E.g: an include inside a class in Ruby). Since Pyrope tuples are not immutable, new methods can be added like in mixin.</p> <pre><code>let Say_mixin = (\n  ,say = fun(s) { puts s }\n)\n\nlet Say_hi_mixin = (\n  ,say_hi  = fun() {self.say(\"hi {}\", self.name) }\n  ,say_bye = fun() {self.say(\"bye {}\", self.name) }\n)\n\nlet User = (\n  ,name:string = _\n  ,setter = proc(ref self, n:string) { self.name = n }\n)\n\nlet Mixing_all = Say_mixin ++ Say_hi_mixin ++ User\n\nvar a:Mixing_all=\"Julius Caesar\"\na.say_hi()\n</code></pre> <p>Mixin is very expressive by allowing redefining methods. If two tuples have the same field a tuple, the concatenated operator (<code>++</code>) will create an entry with two or more sub-entries. This is likely an error with basic types but useful to handle explicit method overload.</p> <p>In a way, the concatenate just adds methods from two tuples to create a new tuple. In programming languages with object-oriented programming (OOP), there are many keywords (<code>virtual</code>, <code>final</code>, <code>override</code>, <code>static</code>...) to constrain how methods can be updated/changed. In Pyrope, the <code>let</code> and <code>var</code> keywords can be added to any tuple field. The <code>let</code> makes the entry immutable when applied to a method, it behaves like a <code>final</code> keyword in most languages.</p> <p>There are also two ways to concatenate tuples in Pyrope. <code>t1 ++ t2</code> and <code>(...t1, ...t2)</code>:</p> <ul> <li> <p><code>t1 ++ t2</code> concatenates each field in both tuples. A compile error is   generated if <code>t1</code> field is a <code>let</code> with a defined value, and <code>t2</code> has also   the same defined field.</p> </li> <li> <p><code>(...t1, ...t2)</code> inserts in-place, triggers a compile error if the same   public field appears in both tuples and it is defined in both. <code>private</code>   fields are privatized and hence do not trigger overload failure.</p> </li> </ul> <pre><code>let Int1 = (\n  ,var counter:int:[private] = 0\n  ,add = proc(ref self, v) { self.counter += v }\n  ,get = fun(self) -&gt; (_:int) { self.counter }\n  ,api_pending: proc(ref self, x:int) -&gt; (o:string) = _\n)\n\nlet Int2 = (\n  ,var counter:int:[private] = 0\n  ,accumulate = proc(ref self, v) { self.counter += v ; return self.counter }\n  ,api_pending:proc(ref self, x:string) -&gt; (o:string) = _\n)\n\nlet Combined = (...Int1, ...Int2\n  ,api_pending = proc(ref self, x:int) -&gt; (o:string) {\n    self.add(x)\n    string(self.accumulate(self.get()))\n  }\n)\n</code></pre> <p>It is also important to notice that when one of the tuples as an entry, it can have an undefined value (<code>nil</code> or <code>0sb?</code>).  If the entry value is undefined, neither concatenate (<code>++</code>) or in-place insert (<code>...</code>) trigger a compile error. This is quite useful for defining interfaces because the default value for a function is <code>nil</code>.</p> <pre><code>let Interface = (\n  ,let add:fun(ref self, x) = _ // nil or undefined method\n  ,let sub = fun(ref self,x ) { self.add(-x) }\n)\n\nInterface.add(3)                // compile error, undefined method\n\nlet My_obj = (\n  ,val1:u8 = 0\n  ,let add = fun(ref self, x) { self.val += x }\n) ++ Interface                  // OK, but not recommended\n\nlet My_obj2 = (\n  ,...Interface                 // recommended\n  ,val1:u8 = 0\n  ,let add = fun(ref self, x) { self.val += x }\n)\ncassert My_obj equals My_obj2   // same behavioir no defined overlap fiels\n\nlet xx:My_obj = _               // default initialization\n\ncassert xx.val1 == 0\nxx.add(3)\ncassert xx.val1 == 3\nxx.sub(2)\ncassert xx.val1 == 1\n</code></pre> <p>Pyrope does not directly check that all the undefined methods are implemented, but this will trigger a compile error whenever the undefined method is used. This is different from most static type languages, but a bit closer to dynamically typed languages. The difference is that the check is at compile time, but an error happens ONLY if the method is used anywhere in the instantiated project.</p> <p>To build tuples that implement the functionality of other tuples, the recommended technique is to use the in-place operator. It checks that there is no defined overlap between both tuples.</p> <p>An issue with in-place operator is when more than one tuple has the <code>setter</code> method. If the tuples are concatenated with <code>...</code> and error is triggered, if the tuples are concatenated with <code>++</code> it does not check if methods overlap. Neither is the expected solution for a mixin. </p> <p>The solution is to remove fields from the in-place concatenation and to explicitly create the new methods with some support method.</p> <pre><code>let exclude = fun(o,...a) {\n  let new_tup = ()\n  for (key,idx,e) in zip(o.keys(),o.enumerate()) {\n    // create single tupe and append to preserve key and position order\n    let sing_tup = ()\n    sing_tup[key] = e\n    new_tup ++= sing_tup unless key in o\n  }\n  new_tup\n}\n\nlet Shape = (\n  ,name:string = _\n  ,area:fun (self )-&gt;(_:i32)  = _            // undefined \n  ,increase_size:proc(ref self, x:i12) = _  // undefined \n\n  ,setter=proc(ref self, name ) { self.name = name } // implemented, use =\n  ,say_name=fun(self) { puts \"name:{}\", name }\n)\n\nlet Circle = (\n  ,...exclude(Shape,'setter')\n\n  ,setter        = proc(ref self) { Circle.setter(this, \"circle\") }\n  ,increase_size = proc(ref self, x:i12) { self.rad *= x }\n  ,rad:i32       = _\n  ,area = fun(self) -&gt; (_:i32) {\n     let pi = import(\"math\").pi\n     return pi * self.rad * self.rad\n  }\n):Shape  // extra check that the exclude did not remove too many fields\n</code></pre>"},{"location":"pyrope/07b-structtype/#row-type","title":"Row type","text":"<p>Pyrope has structural typing, but also allows to infer the types. The <code>where</code> statement can be used to implement some functionality that resembles the row type inference. The <code>where</code> clause is followed by a list of comma separated conditions that must evaluate true for the function to be valid.</p> <pre><code>let rotate = fun(a) where a has 'x', a has 'y' and_then a.y!=30 {\n  var r = a\n  r.x = a.y\n  r.y = a.x\n  return r\n}\n</code></pre> <p>The previous rotate function is difficult to implement with a traditional structural typing.</p>"},{"location":"pyrope/08-memories/","title":"Memories","text":"<p>A significant effort of hardware design revolves around memories. Unlike Von Neumann models, memories must be explicitly managed. Some list of concerns when designing memories in ASIC/FPGAs:</p> <ul> <li>Reads and Writes may have different number of cycles to take effect</li> <li>Reset does not initialize memory contents</li> <li>There may not be data forwarding if a read and a write happen in the same cycle</li> <li>ASIC memories come from memory compilers that require custom setup pins and connections</li> <li>FPGA memories tend to have their own set of constraints too</li> <li>Logic around memories like BIST has to be added before fabrication</li> </ul> <p>This constrains the language, it is difficult to have a typical vector/memory provided by the language that handles all these cases. Instead, the complex memories are managed by the Pyrope standard library.</p> <p>The flow directly supports arrays/memories in two ways:</p> <ul> <li>Async memories or arrays</li> <li>RTL instantiation</li> </ul>"},{"location":"pyrope/08-memories/#async-memories-or-arrays","title":"Async memories or arrays","text":"<p>Asynchronous memories, async memories for short, have the same Pyrope tuple interface. The difference between tuples/arrays and async memories is that the async memories preserve the array contents across cycles. In contrast, the array contents are cleared at the end of each cycle.</p> <p>In Pyrope, an async memory has one cycle to write a value and 0 cycles to read. The memory has forwarding by default, which behaves like a 0 cycle read/write. From a non-hardware programmer, the default memory looks like an array with persistence across cycles.</p> <p>Pyrope async memories behave like what a \"traditional software programmer\" will expect in an array. This means that values are initialized and there is forwarding enabled. This is not what a \"traditional hardware programmer\" will expect. In languages like CHISEL there is no forwarding or initialization. In Pyrope is possible to have different options of async memories, but those should use the RTL interface.</p> <p>The async memories behave like tuples/arrays but there is a small difference, the persistence of state between clock cycles. To be persistent across clock cycles, this is achieved with a <code>reg</code> declaration. When a variable is declared with <code>var</code> the contents are lost at the end of the cycle, when declared with <code>reg</code> the contents are preserved across cycles.</p> <p>In most cases, the arrays and async memories can be inferred automatically. The maximum/minimum value on the index effectively sets the size and the default initialization is zero.</p> <pre><code>reg mem:[] = 0\nmem[3]   = something // async memory\nvar array:[] = _\narray[3] = something // array no cross cycles persistence\n</code></pre> <pre><code>var index:u7 = _\nvar index2:u6 = _\n\narray[index] = something\nsome_result  = array[index2+3]\n</code></pre> <p>In the previous example, the compiler infers that the bundle at most has 127 entries.</p> <p>There are several constructs to declare arrays or async memories:</p> <pre><code>reg mem1:[16]i8 = 3   // mem 16bit memory initialized to 3 with type i8\nreg mem2:[16]i8 = _   // mem 16bit memory initialized to 0 with type i8\nvar mem3:[] = 0sb?        // array infer size and type, 0sb? initialized\nvar mem4:[13] = 0         // array 13 entries size, initialized to zero\n</code></pre> <p>Pyrope allows slicing of bundles and hence arrays.</p> <pre><code>x1 = array[first..&lt;last]  // from first to last, last not included\nx2 = array[first..=last]  // from first to last, last included\nx3 = array[first..+size]  // from first to first+size, first+size. not included\n</code></pre> <p>Since bundles are multi-dimensional, arrays or async memories are multi-dimensional too.</p> <pre><code>a[3][4] = 1\n\nvar b:[4][8]u8 = 13\n\nassert b[2][7] == 13\nassert b[2][10]      // compile error, '10' is out of bound access for 'b[2]'\n</code></pre> <p>It is possible to initialize the async memory with an array. The initialization of async memories happens whenever <code>reset</code> is set on the system. A key difference between arrays (no clock) and memories is that arrays initialization value must be <code>comptime</code> while <code>memories</code> and <code>reg</code> can have a sequence of statements to generate a reset value. </p> Pyrope array syntaxExplicit initialization <pre><code>var mem1:[4][8]u5 = 0\nvar reset_value:[3][8]u5:[comptime] = _ // only used during reset\nfor i in 0..&lt;3 {\n  for j in 0..&lt;8 {\n    reset_value[i][j] = j\n  }\n}\nreg mem2 = reset_value   // infer async mem u5[3][8]\n</code></pre> <pre><code>var mem = ( \n  ,(u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0))\n  ,(u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0))\n  ,(u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0))\n  ,(u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0), u5(0))\n)\nreg mem2 = ( \n  ,(u5(0), u5(1), u5(2), u5(3), u5(4), u5(5), u5(6), u5(7))\n  ,(u5(0), u5(1), u5(2), u5(3), u5(4), u5(5), u5(6), u5(7))\n  ,(u5(0), u5(1), u5(2), u5(3), u5(4), u5(5), u5(6), u5(7))\n)\n</code></pre>"},{"location":"pyrope/08-memories/#sync-memories","title":"Sync memories","text":"<p>Pyrope asynchronous memories provide the result of the read address and update their contents on the same cycle. This means that traditional SRAM arrays can not be directly used. Most SRAM arrays either flop the inputs or flop the outputs (sense amplifiers). This document calls synchronous memories the memories that either has a flop input or an output.</p> <p>There are two ways in Pyrope to instantiate more traditional synchronous memories. Either use async memories with flopped inputs/outputs or do a direct RTL instantiation.</p>"},{"location":"pyrope/08-memories/#flop-the-inputs-or-outputs","title":"Flop the inputs or outputs","text":"<p>When either the inputs or the output of the asynchronous memory access is directly connected to a flop, the flow can recognize the memory as asynchronous memory. A further constrain is that only single dimension memories. Multi-dimensional memories or memories with partial updates need to use the RTL instantiation.</p> <p>To illustrate the point of simple single dimensional synchronous memories, this is a typical decode stage from an in-order CPU:</p> Flop the inputsFlop the outputs <pre><code>reg rf:[32]i64 = 0sb?   // random initialized\n\nreg a:(addr1:u5, addr2:u5) = (0,0)\n\ndata_rs1 = rf[a.addr1]\ndata_rs2 = rf[a.addr2]\n\na = (insn[8..=11], insn[0..=4])\n</code></pre> <pre><code>var rf:[32]i64 = 0sb?\n\nreg a:(data1:i64, data2:i64) = _\n\ndata_rs1 = a.data1\ndata_rs2 = a.data2\n\na = (rf[insn[8..=11]], rf[insn[0..=4]])\n</code></pre>"},{"location":"pyrope/08-memories/#rtl-instantiation","title":"RTL instantiation","text":"<p>There are several constraints and additional options to synchronous memories that the async memory interface can not provide: multi-dimension, partial updates, negative edge clock...</p> <p>Pyrope allows for a direct call to LiveHD cells with the RTL instantiation, as such that memories can be created directly.</p> <pre><code>// A 2rd+1wr memory (RF type)\n\nmem.addr    = (raddr0, raddr1, wraddr)\nmem.bits    = 4\nmem.size    = 16\nmem.clock   = my_clock\nmem.din     = (0, 0, din0)\nmem.enable  = (1, 1, we0)\n\nmem.fwd     = false\nmem.latency = (1, 1, 1)\nmem.wensize = 1 // we bit (no write mask)\nmem.rdport  = (-1,1,0) // 0 WR, !=0 -&gt; RD\n\nres =#[..] __memory(mem)\n\nq0 = res.0\nq1 = res.1\n</code></pre> <p>The previous code directly instantiates a memory and passes the configuration. </p> <p>Multi cycle memories are pipelined elements, and using them requires the <code>=#[..]</code> assignment and the same rules as pipeline flops apply (See pipelining).</p>"},{"location":"pyrope/08-memories/#multidimensional-arrays","title":"Multidimensional arrays","text":"<p>Pyrope supports multi-dimensional arrays, it is possible to slice the array by dimension. The entries are in a row-major order.</p> <pre><code>var d2:[2][2] = ((1,2),(3,4))\nassert d2[0][0] == 1 and d2[0][1] == 2 and d2[1][0] == 3 and d2[1][1] == 4\n\nassert d2[0] == (1,2) and d2[1] == (2,3)\n</code></pre> <p>The <code>for</code> iterator goes over each entry of the bundle/array. If a matrix, it does in row-major order. This allows building a simple function to flatten multi-dimensional arrays.</p> <pre><code>let flatten = fun(...arr) {\n  var res = 0\n  for i in arr {\n    res ++= i\n  }\n  return res\n}\n\nassert flatten(d2) == (1,2,3,4)\nassert flatten((((1),2),3),4) == (1,2,3,4)\n</code></pre>"},{"location":"pyrope/08-memories/#array-index","title":"Array index","text":"<p>Array index by default are unsigned integers, but the index can be constrained with tuples or by requiring an enumerate.</p> <pre><code>var x1:[2]u3 = (0,1)\nassert x1[0] == 0 and x1[1] == 1\n\nvar X=enum(\n  ,t1 = 0 // sequential enum, not one hot enum (explicit assign)\n  ,t2\n  ,t3\n)\n\nvar x2:[X]u3 = _\nx2[X.t1] = 0\nx2[X.t2] = 1\nx2[0]              // compile error, only enum index\n\nvar x3:[-8..&lt;7]u3 = _  // accept signed values\n\nvar x4:[100..&lt;132]u3 = _\n\nassert x4[100] == 0\nassert x4[3]       // compile error, out of bounds index\n</code></pre>"},{"location":"pyrope/08-memories/#reset-and-initialization","title":"Reset and initialization","text":"<p>Like the <code>let</code> and <code>var</code> statements, <code>reg</code> statements require an initialization value. While <code>let/var</code> initialize every cycle, the <code>reg</code> initialization is the value to set during reset.</p> <p>Like in <code>let/var</code> cases, the reset/initialization value can use the traditional Verilog uninitialized (<code>0sb?</code>) contents. The Pyrope semantics for any bit with <code>?</code> value is to respect arithmetic Verilog semantics at compile time, but to randomly generate a zero/ones for each simulation. As a result assertions can fail with unknowns.</p> <pre><code>reg r_ver = 0sb?\n\nreg r = _\nvar v = _\n\nassert v == 0 and r == 0\n\nassert !(r_ver != 0)    // it will randomly fail\nassert !(r_ver == 0)    // it will randomly fail\nassert !(r_ver != 0sb?) // it will randomly fail\nassert !(r_ver == 0sb?) // it will randomly fail\n</code></pre> <p>The reset for arrays may take several cycles to take effect, this can lead to unexpected results during the reset period. Memories and registers are randomly initialized before reset during simulation. There is no guarantee of zero initialization before reset.</p> <pre><code>var arr:[] = (0,1,2,3,4,5,6,7)\n\nalways assert arr[0] == 0 and arr[7] == 7  // may FAIL during reset\n\nreg mem:[] = (0,1,2,3,4,5,6,7)\n\nalways assert mem[7] == 7                  // may FAIL during reset\nalways assert mem[7] == 7 unless mem.reset // OK\nassert mem[7] == 7                         // OK, not checked during reset\n</code></pre>"},{"location":"pyrope/09-stdlib/","title":"Pyrope std lib","text":"<p>Like most languages, a standard library provides some common functionality around.</p> <p>The standard library provides methods for the basic types (Number, String, Boolean) and utility code like fifos. The utils must be imported but the basic types are imported by default.</p>"},{"location":"pyrope/10-internals/","title":"Internals","text":"<p>This section of the document provides a series of disconnected topics about the compiler internals that affects semantics.</p>"},{"location":"pyrope/10-internals/#tuple-operations","title":"Tuple operations","text":"<p>There are 3 basic operations/checks with tuples that affect many other operations: <code>a in b</code>, <code>a does b</code>, and lambda call rules.</p> <ul> <li> <p><code>a in b</code> allows to work when <code>b</code> is a name/unnamed tuple even when <code>a</code> is named.</p> </li> <li> <p><code>a does b</code> requires <code>b</code> to be named consistent with names in <code>a</code>.</p> </li> <li> <p>lambda call matches the arguments with the definition in a third different set of rules.</p> </li> </ul> <pre><code>cassert (a=1) in (1,a=1,3)\ncassert (a=1) !does (1,a=1,3)\n\nlet f = fun(a) { puts \"{}\",a }\nlet g = fun(long, short) { puts \"{}\",long }\n\nf(a=1)             // OK\nf(1)               // OK\n\ng(long=1, short=1) // OK\ng(1,1)             // compile error\nlet long=1\ng(long, short=1)   // OK\nlet short=1\ng(long, short)     // OK\n</code></pre> <p>Operators like <code>a == b</code>, <code>a case b</code>, <code>a equals b</code>, ... built on top of the previous functionality.</p>"},{"location":"pyrope/10-internals/#determinism","title":"Determinism","text":"<p>Pyrope is designed to be deterministic. This means that the result is always the same for synthesis. Simulation is also deterministic unless the random seed is changed or non-Pyrope (C++) calls to <code>procedures</code> add non-determinism.</p> <p>Expressions are deterministic because <code>procedures</code> have an explicit order in Pyrope. Only <code>functions</code> have a non-explicit order, but <code>functions</code> are pure without side-effects. <code>puts</code> can be called non-deterministically but the result is buffered and ordered at the end of the cycle to be deterministic.</p> <p>The only source of non-determinism is non-Pyrope (C++) calls from <code>procedures</code> executed at different pipeline stages. The pipeline stages could be executed in any order, it is just that the same order must be repeated deterministically during simulation. The non-Pyrope calls must be <code>::[comptime]</code> to affect synthesis. So the synthesis is deterministic, but the testing like cosimulation may not.</p> <p>The same non-Pyrope calls also represent a problem for the compiler optimizations. During the setup phase, several non-Pyrope can exist like reading the configuration file. If the non-Pyrope calls are not deterministic, the result could be a non-deterministic setup phase.</p> <p>The idea is that the non-Pyrope API is also divided in 2 categories: <code>functions</code> and <code>procedures</code>. A <code>function</code> can be called many times without non-Pyrope side-effects. Pyrope guarantees that the <code>procedures</code> are called in the same order given a source code, but does not guarantee the call order. This guarantee order slowdowns simulation and elaboration. Whenever possible, use <code>functions</code> instead of <code>procedures</code> for compilation speed reasons.</p>"},{"location":"pyrope/10-internals/#import","title":"Import","text":"<p><code>import</code> statement allows for circular dependencies of files, but not of variables. This means that if there is no dependency (<code>a imports b</code>), just running <code>a</code> before <code>b</code> is enough. If there is a dependency (<code>a imports b</code> and <code>b imports a</code>) a multiple compiler pass is proposed, but other solutions are allowed as long as it can handle not true circular dependences.</p> <p>The solution to this problem is to pick an order, and import at least three times the files involved in the cyclic dependency. The files involved in the cylic dependency are alphabetically sorted and called three times: (1) <code>a import b</code>, then <code>b import a</code>; (2) <code>a import b</code> and <code>b import a</code>; (3) <code>a import b</code> and <code>b import a</code>. Only the last import chain can perform procedure <code>proc</code> calls (Pyrope and non-Pyrope) and puts/debug statements.</p> <p>If the result of the last two imports has the same variables, the import has \"converged\", otherwise a compile error is generated. This multi-pass solution does not address all the false paths, but the common case of having two sets of independent variables. This should address most of the Pyrope cases because there is no concept of \"reference/pointer\" which is a common source of dependences.</p>"},{"location":"pyrope/10-internals/#register-reference","title":"Register reference","text":"<p>Register reference (<code>regfef</code>) can create a dependence update between files, but this is not a source of non-determinism because only one file can perform updates for the register <code>din</code> pin, and all the updated register can only read the register <code>q</code> pin.</p>"},{"location":"pyrope/10-internals/#dealing-with-unknowns","title":"Dealing with unknowns","text":"<p>Pyrope tries to be compatible with synthesizable Verilog but not equivalent. As such it must handle/understand unknowns. Compatible does not mean that it will generate the same <code>?</code> bits as Verilog, but that it will not generate an unknown when Verilog has known. It is allowed to generate a <code>0</code> or a <code>1</code> when the Verilog logical equivalence check generates an <code>?</code>.</p> <p>An example of different behavior is that Verilog semantics state <code>0 * 0sb?</code> is <code>0sb?</code> while most programmers would expect a zero.</p> <p>The previous definition of compatibility could allow the Pyrope compiler to randomly replace all the unknowns by <code>0</code> or <code>1</code> when doing internal compiler passes. This is not done at compile time to keep determinism, but simulation time should randomly pick 0/1 for unknown bits.</p> <p>The issue is that the most likely source of having unknowns in operations is either during reset or due to a bug on how to handle non initialized structures like memories.</p> <p>The compiler internal transformations use a 3-state logic that includes <code>0</code>, <code>1</code>, and <code>?</code> for each bit. Any register or memory initialized with unknowns will generate a Verilog with the same initialization.</p> <p>The compiler internals only needs to deal with unknowns during the copy propagation or peephole optimizations. The compile goes through 2 phases: LNAST and Lgraph.</p> <p>In the compiler passes, we have the following semantics:</p> <ul> <li> <p>In Pyrope, there are 3 array-like structures: non-persistent arrays, register   arrays, and custom RTL memories. Verilog and CHISEL memories get translated   to custom RTL memories. Non-persistent Verilog/CHISEL get translated to arrays.   In Verilog, the semantics is that an out of bounds access generates unknowns. In   CHISEL, the <code>Vec</code> is that an out of bound access uses the first index   of the array. A CHISEL out of bound memory is an unknown like in Verilog. These   are the semantics applied by the compiler optimization/transformations:</p> <ul> <li> <p>Custom RTL memories do not allow value propagation across the array, only   across non-persistent arrays, or register arrays explicitly marked with   <code>retime=true</code>.</p> </li> <li> <p>An out of bound RTL address drops the unused index bits. For non-power of   two arrays, out of bounds access triggers a compile error. The code must   be fixed to avoid access. An <code>if addr &lt; mem_size { ... mem[addr] ... }</code>   tends to be enough. This is to guarantee that passes like Verilog and   CHISEL have the same semantics, and trigger likely bugs in Pyrope code.</p> </li> <li> <p>An index with unknowns does not perform value propagation.</p> </li> </ul> </li> <li> <p>Shifts, additions and substractions propagate unknowns at computation. E.g:   <code>0b11?0 + 0b1</code> is <code>0b11?1</code>, <code>0b1?0 &gt;&gt; 1</code> is <code>0b1?</code>.</p> </li> <li> <p>Other arithmetic are more conservative. When an input is unknown, the result   is unknown only respecting the sign when possible. E.g: <code>0b1?0? * -1</code> is   <code>0sb1?</code>.</p> </li> <li> <p>Logic operations behave like Verilog. <code>0b000111??? | 0b01?01?01?</code> is   <code>0b01?111?1?</code>.</p> </li> <li> <p>Equality comparisons (<code>==</code> and <code>!=</code>) use unknowns, this means that at compile   time <code>0b1? != 0b10</code>. Comparisons is consistent with the equivalent logic   operations <code>a == b</code> is the same as <code>(a ^ b) == -1</code>.</p> </li> <li> <p>Other comparisons (<code>&lt;=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&gt;=</code>) return true if the comparison is   true for each possible unknown bit.</p> </li> <li> <p><code>match</code> statement and <code>unique if</code> will trigger a compile error if the unknown   semantics during compiler passes can trigger 2 options simultaneously. The   solution is to change to a sequence of <code>ifs</code> or change the code to guarantee   no unknowns.</p> </li> <li> <p><code>if</code> statement without <code>unique</code> logical expressions that have an unknown   (single bit) are a source of confusion. In Verilog, it depends on the   compiler options. A classic compiler will generate <code>?</code> in all the updated   variables.  A Tmerge option will propagate <code>?</code> only if both sides can   generate a different value. The LNAST optimization pass will behave like the   Tmerge when the if/mux control has unknowns:</p> <ul> <li> <p>If all the paths have the same constant value, the <code>if</code> is useless and   the correct value will be used.</p> </li> <li> <p>If any path has a different constant value, the generated result bits will   have unknowns if the source bits are different or unknown.</p> </li> <li> <p>If any paths are not constant, there is no LNAST optimization. Further   Lgraph optimizations could optimize if all the mux generated values are   proved to be the same.</p> </li> </ul> </li> <li> <p>The <code>for</code> loops are expanded, if the expression in the <code>for</code> is unknown, a   compile error is generated.</p> </li> <li> <p>The <code>while</code> loops are also expanded, if the condition on the <code>while</code> loop has   unknowns a compile error is generated.</p> </li> </ul> <p>At the end of the LNAST generation, a Lgraph is created. Only the registers and memory initialization are allowed to have unknowns in Lgraph.  Any invalid (<code>nil</code>) assigned to an output or register triggers a compile error. Any unknown constant bit is translated preserved (<code>0b10?</code>).</p> <p>The semantics on the generated simulator are similar to CHISEL, any unknowns are randomly translated to 0 or 1 at initialization.</p>"},{"location":"pyrope/10-internals/#optimize-directive","title":"Optimize directive","text":"<p>The <code>optimize</code> directive is like an <code>assert</code> but it also allows compiler optimizations. In a way, it is a safer version of Verilog <code>?</code>. Unlike other languages like C++23, Pyrope <code>optimize</code> verifies at simulation time that the <code>optimize</code> is correct. This means that the <code>optimize</code> is checked like an <code>assert</code> but it allows the compiler to optimize based on the condition. <code>asserts</code> do not trigger optimizations because their check can be disabled at simulation time, and hence create mismatches between simulation and synthesis if the compiler optimized over assertions.</p> Verilog x-optimizationPyrope <code>match</code>Generated Logic 1 bit f <pre><code>always_comb begin // one hot mux\n  case (sel)\n    3\u2019b001 : f=i0;\n    3\u2019b010 : f=i1;\n    3\u2019b100 : f=i2;\n    default: f=2\u2019b??;\n  endcase\nend\n</code></pre> <pre><code>optimize sel==1 or sel==2 or sel==4 // not needed. match sets it\nmatch sel {\n  == 0b001 { f = i0 }\n  == 0b010 { f = i2 }\n  == 0b100 { f = i3 }\n}\n</code></pre> <pre><code>f = (sel[0] &amp; i0)\n  | (sel[1] &amp; i1)\n  | (sel[2] &amp; i2)\n</code></pre> <p>Optimize allows more freedom, without dangerous Verilog x-optimizations:</p> Bad Verilog x-optimizationPyrope optimize <pre><code>if (a == 0) begin\n   assert(false);\n   out = '?;\nend else if (1 + a) == 1 begin // always false\n   out = 1;\nend else begin\n   out = 3;\nend\n\narray[3] = '?; // entry 3 will not be used\n// array = (1,2,3,'?,5,6,7,8)\nres = array[b]\n</code></pre> <pre><code>optimize a != 0\n\n\nif (1 + a) != 1 { // always false\n  out = 1\n}else{\n  out = 3\n}\n\noptimize b != 3\n// array = (1,2,3,4,5,6,7,8)\nres = array[b]\n</code></pre>"},{"location":"pyrope/10-internals/#unknown-no-optimization","title":"Unknown no optimization","text":"<p>In Verilog, unknowns can trigger synthesis optimizations. This is not the case in Pyrope. Each unknown bit (<code>?</code>) can result in random 0/1 at simulation time, but it will not trigger optimizations. The <code>optimize</code> statement should be use for such behavior.</p> <pre><code>assert cond==3     // Not cassert or optimize, so no optimized\nvar x1 = 0sb?\n\nif cond == 3 {\n  x1 = 1\n}\nassert x1==1 // still not optimized (cassert fails)\ncassert !x1.[comptime]\n\nvar x2 = 0sb?\noptimize cond==3\nif cond == 3 {\n  x2 = 1\n}\ncassert x2==1\ncassert x2.[comptime]\n</code></pre>"},{"location":"pyrope/10-internals/#lnast-optimization","title":"LNAST optimization","text":"<p>The compiler has three IR levels: The high level is the parse AST, the mid-level is the LNAST, and the low level is the Lgraph. This section explains the main steps in the LNAST optimizations/transformations before performing type synthesis and generating the lower level Lgraph. This is a minimum of optimizations without them several type conflicts would be affected.</p> <p>Unlike the parse AST, the LNAST nodes are guaranteed to be in topological order. This means that a single pass that visits the children first (deep first) is sufficient.</p> <p>The work can be performed as a single \"global\" topographical pass starting from the root/top, where each LNAST node performs these operations during traversal depending on the LNAST node:</p> <ul> <li> <p>If the node allows, perform these node input optimization first:</p> <ul> <li> <p>constant folding for existing node, also be performed as instruction   combining proceeds</p> </li> <li> <p>instruction combining from sources only for same type but not beyond 128   n-ary nodes. This step subsumes constant propagation and copy   propagation. E.g: <code>a+(x-3)+1</code> becomes <code>a+x-3+1</code></p> </li> <li> <p>create a canonical order by sorting the inputs by name/constant. E.g: <code>+   2 a b</code>. This simplifies the following steps but it is not needed for   semantics. Most commutative gates (<code>add/sub/and/or/...</code>) will have a   single constant as a result.</p> </li> <li> <p>trivial simplification with constants for existing node, also performed   as instruction combining proceeds. E.g.: <code>a+0 == a</code>, <code>a or true   == true</code> ...</p> </li> <li> <p>trivial identity simplification for existing node, also performed as   instruction combining proceeds. E.g: <code>a^a == a</code>, <code>a-a=0</code> ...</p> </li> </ul> </li> <li> <p>If the node is a <code>::[comptime]</code> trigger a compile error unless all the inputs are   constant</p> <ul> <li><code>cassert</code> should satisfy the condition or a compile error is generated</li> </ul> </li> <li> <p>If the node is a loop (<code>for</code>/<code>while</code>) that has at least one iteration expand   the loop. This is an iterative process because the loop exit condition may   depend on the loop body or functions called inside. After the loop   expansions, no <code>for</code>, <code>while</code>, <code>break</code>, <code>last</code>, <code>continue</code> statement   exists.</p> </li> <li> <p>If the node is a function call, iterate over the possible polymorphic calls.   Call the first call that is valid (input types). Call the function and pass   all the input constants needed. This requires specializing the function   by input constants and types. If no call matches a valid type trigger a   compile error</p> </li> <li> <p>Delete unreachable statements (<code>if false { delete his }</code>, <code>delete this when false</code>, ...)</p> </li> <li> <p>Compute these steps that may be needed in future steps:</p> <ul> <li> <p>Perform the \"Mark\" phase typical in dead-code-elimination (DCE) so that   dead nodes are not generated when creating the Lgraph.</p> </li> <li> <p>Update the tuple field in the Symbol Table</p> </li> <li> <p>Track the array accesses for memory/array Lgraph generation</p> </li> </ul> </li> </ul>"},{"location":"pyrope/10-internals/#type-synthesis","title":"Type synthesis","text":"<p>The type synthesis and check are performed during the LNAST pass. Pyrope uses a structural type system with global type inference.</p> <p>The type inference should be performed as the same time as the LNAST optimization traverses the tree. It can not be a separate pass because there can be interactions between the LNAST optimization and the Type synthesis. These are the additional checks performed for type synthesis:</p> <ul> <li> <p>If the node does type checks (<code>equals</code>, <code>does</code>) compute the outcome and   perform copy propagation. The result of this step is that the compiler is   effectively doing flow-type inference. All the types must be resolved before.   If the <code>equals</code>/<code>does</code> was in a <code>if</code> condition, the control is decided at   compile time.</p> </li> <li> <p>If the node reads bitwidth, replace the node with the computer Bitwidth value   (max, min, ubits, and/or sbits)</p> <ul> <li>Compute the max/min for the output[s] using the bitwidth algorithm.   Update the symbol table with the range. This is only needed because some   code like polymorphism functions can read the bits.</li> </ul> </li> <li> <p>If the node is a conditional (<code>if</code>/<code>match</code>), the pass performs narrowing<sup>1</sup>.</p> <ul> <li> <p>When the expression has these possible syntax <code>v &gt;= y</code>, <code>v &gt;   y</code> or the reciprocals, restrict the Bitwidth. E.g: in the <code>v &lt; y</code>   restricts the <code>v.max = y.min-1 ; y.min = v.min + 1</code></p> </li> <li> <p>When the expression is an equality format <code>eq [and eq]*</code> or <code>eq [or   eq]*</code> like <code>v1 == z1 and v2 != z2</code>, create a <code>v1=z1</code> and <code>v2=z2</code> in the   corresponding path. This will help bitwidth and copy propagation.   Complicated mixes of and/or have no path optimization</p> </li> <li> <p>When the expression is a single variable <code>a</code> or <code>!a</code>, set the variable   <code>true</code> and <code>false</code> in both paths</p> </li> </ul> </li> </ul> <p>No previous transformation could break the type checks. This means that the copy propagation, and final lgraph translation the type checks are respected.</p> <ul> <li> <p>All the entries on the comparator have the same type (<code>LHS equals RHS</code>)</p> </li> <li> <p>Left side assignments respect the assigned type (<code>LHS does RHS</code>)</p> </li> <li> <p>Any explicit type on any expression should respect the type (<code>var does type</code>)</p> </li> </ul> <p>The previous algorithm describes the semantics, the implementation may be different.  For example, to parallelize the algorithm, each LNAST tree can be processed locally, and then a global top pass is performed.</p>"},{"location":"pyrope/10-internals/#programming-warts","title":"Programming warts","text":"<p>In programming languages, warts are small code fragments that have unexpected or not great behavior. Every language has its warts. This section tries to list the Pyrope main ones to address and learn more about the language.</p>"},{"location":"pyrope/10-internals/#shadowing","title":"Shadowing","text":"<p>Pyrope does not allow shadowing, but you can still have it with tuples. To access the tuple field, the <code>self.field</code> is always required. This avoid the problem of true shadowing.</p> <pre><code>let f1 = fun() { 1 }\n\nlet tup = (\n  ,f1 = fun() { 2 }\n\n  ,code = fun() {\n     assert self.f1() == 2\n     assert f1() == 1\n  }\n)\n</code></pre>"},{"location":"pyrope/10-internals/#closures","title":"Closures","text":"<p>Closures capture extra state or inputs at definition. The capture variables are always immutable <code>let</code> no matter the outter scope definition. Therefore, capture variables behave like passed by value, not reference.</p> <p>One important thing is 'when' does the capture happens. Pyrope follows the model of most languages like C++ that captures at lambda definition, not lambda execution.</p> Pyrope capture timeC++17 capture time <pre><code>var x_s = 10\n\nlet call_captured = fun[x_s]() {\n  return fun[x_s]() {\n    assert x_s == 10\n    return x_s\n  }\n}\n\ntest \"capture test\" {\n  let tst = fun() {\n    var x_s = 20   // not variable shadowing because fun scope\n\n    let x1 = call_captured()\n    assert x1 == 10\n\n    x_s = 30;\n\n    let x2 = call_captured()\n    assert x2 == 10\n  }\n  tst // call the test\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n\n  int x_s{ 10 };\n\n  auto call_captured{\n    [x_s]() {\n      assert(x_s == 10);\n      return x_s;\n    }\n  };\n  }\n\n  x_s = 20;\n\n  auto x1 = call_captured();\n  assert(x1==10);\n\n  x_s = 30;\n\n  auto x2 = call_captured();\n  assert(x2==10);\n}\n</code></pre> <p>Some languages like ZIG do not allow closures, but they allow structs with a lambda to implement an equivalent functionality. It is possible in Pyrope to also create a tuple and populate the getter. This effectively behaves as the closures. Internally, Pyrope may do this implementation.</p> Pyrope tuple closure styleZIG closure style with struct <pre><code>let j = 1\nlet b = fun[j](x:i32)-&gt; :i32 {\n  return x+j\n}\n\nassert b(1) == 2\n\ntest \"closure with tuple\" {\n  var a: i32 = 1\n  a += 1\n\n  var addX = (\n    ,a: i32 = a                        // copy value, runtime or comptime\n    ,getter = fun(self, x: i32) {\n      return x + self.a\n    }\n  )\n\n  a += 100;\n\n  assert addX(2) == 4\n}\n\ntest \"plain closure\" {\n  var a:i32 = 1\n  a += 1\n\n  let addX = fun[a](x: i32) { // Same behaviour as closure with tuple\n    return x + a\n  }\n\n  a += 100;\n\n  assert addX(2) == 4\n}\n</code></pre> <pre><code>pub fn main() void {\n    const j = 1;\n    var b = struct{\n        fn function(x: i32) i32 {\n            return x+j;\n        }\n    }.function;\n\n    @import(\"std\").debug.assert(b(1) == 2);\n}\n\ntest \"closure with runtime\" {\n  var a: i32 = 1;\n  a += 1;\n\n  const addX = (struct {\n    a: i32,\n    fn call(self: @This(), x: i32) i32 {\n      return x + self.a;\n    }\n  } { .a = a }).call;\n\n  a += 100;\n\n  @import(\"std\").debug.assert(addX(2) == 4);\n}\n</code></pre> <p>Capture values must be explicit, or no capture happens. This means that <code>...fun[](...)...</code> is the same as <code>...fun(...)...</code>.</p> <pre><code>var x = 3\n\nlet f1 = fun[x]()-&gt;(_:int){\n   assert x == 3\n   var x = _    // compile error. Shadow captured x\n   return 200\n}\nlet f2 = fun()-&gt;(_:int){\n   var x = _    // OK, no captures 'x' variable\n   x = 100\n   return x\n}\n</code></pre> <p>Capture variables pass the value at capture time:</p> <pre><code>var x = 3\nvar y = 10\n\nlet fun2 = fun[y]()-&gt;(_:int){\n  y = 100              // compile error, y is immutable when captured\n  var x  = 200\n  return y + x\n}\nx = 1000\nassert fun2() == 203\n</code></pre>"},{"location":"pyrope/10-internals/#lambda-arguments","title":"Lambda arguments","text":"<p>Lambda calls happen whenever an identifer is followed by a list of expressions. If the first expression in the list has parenthesis, it can lead to unexpected behavior:</p> <pre><code>assert 0 == (0)  // OK, same as assert( 0 == (0) )\nassert (0) == 0  // compile error: (assert(0)) == 0 is an expression\nassert(0 == 0)   // OK\n</code></pre> <p>It is also easy to forget that parenthesis can be ommited in simple expressions, not when ranges or tuples are involed.</p> <pre><code>asssert 2 in (1,2)  // compile error, not allowed to drop parenthesis\nasssert(2 in (1,2)) // OK\n</code></pre>"},{"location":"pyrope/10-internals/#multiple-tuples","title":"Multiple tuples","text":"<p>The evaluation order is always the same program order starting from the top module. Remember that the setter method is the constructor called even when there is no initial value set.</p> <pre><code>let X_t = (\n  ,i1 = (\n    ,i1_field:u32 = 1\n    ,i2_field:u32 = 2\n    ,setter = proc(ref self, a) {\n       self.i1_field = a\n    }\n  )\n  ,i2 = (\n    ,i1_field:i32 = 11\n    ,setter = proc(ref self, a) {\n       self.i1_field = a\n    }\n  )\n)\n\nvar top = (\n  ,setter = proc(ref self) {\n    var x:X_t = _\n    assert x.i1.i1_field == 1\n    assert x.i1.i2_field == 2\n    assert x.i2.i1_field == 11\n\n    x.i1 = 400\n\n    assert x.i1.i1_field == 400\n    assert x.i1.i2_field == 2\n    assert x.i2.i1_field == 11\n\n    x.i2 = 1000\n\n    assert x.i1.i1_field == 400\n    assert x.i1.i2_field == 2\n    assert x.i2.i1_field == 1000\n  }\n)\n</code></pre> <p>If a lambda in the hierarchy does not have a setter/constructor, the program order follows the tuple scope which is in tuple ordered asignment.</p>"},{"location":"pyrope/10-internals/#unknowns","title":"Unknowns","text":"<p>Pyrope respects the same semantics as Verilog with unknowns. As such, there can be many unexpected behaviors in these cases. The difference is that in Pyrope everything is initialized and unknowns (<code>0sb?</code>) can happen only when explicitly enabled.</p> <p>The compare respects Verilog semantics. This means that it is true if and only if all the possible values are true, which is quite counter-intuitive behavior for programmers not used to 4 value logic.</p> <pre><code>assert !(0sb? == 0)\nassert !(0sb? != 0)\nassert !(0sb? == 0sb?)\nassert !(0sb? != 0sb?)\n</code></pre> <p>There is no way to know at run-time if a value is unknown, but a compile trick can work. The reason is that integers can be converted to strings in a C++ API</p> <pre><code>var x = 0sb10?\nlet str = __to_string(x) // only works for compile time constants\nassert x == \"0sb10?\"\n</code></pre>"},{"location":"pyrope/10-internals/#for-loop","title":"for loop","text":"<p>The <code>for</code> expects a tuple, and iterates over the tuple. This can lead to some unexpected behaviour. The most strange is that ranges are always from smallest to largest. It is not legal to do a <code>5..&lt;0</code> range, the solution is to use a <code>to</code> which creates a tuple not a range.</p> <pre><code>let s:string=\"hell\"\nfor (idx,i) in s.enumerate() {\n  let v = match idx {\n   == 0 { \"h\" }\n   == 1 { \"e\" }\n   == 2 { \"l\" }\n   == 3 { \"l\" }\n  }\n  assert v == i\n}\n\nlet t = (1,2,3)\nfor (idx,i) in t.enumerate() {\n  let v = match idx {\n   == 0 { 1 }\n   == 1 { 2 }\n   == 2 { 3 }\n  }\n  assert v == i\n}\n\nlet r=2..&lt;5\nfor (idx,i) in r.enumerate() {\n  let v = match idx {\n   == 0 { 2 }\n   == 1 { 3 }\n   == 2 { 4 }\n  }\n  assert v == i\n}\n\nlet r2=4..=2 step -1\nassert r2 == (4,3,2)\nfor (idx,i) in r2.enumerate() {\n  let v = match idx {\n   == 0 { 4 }\n   == 1 { 3 }\n   == 2 { 2 }\n  }\n  assert v == r2[i]\n}\n\nfor i in 2..&lt;5 {\n  let ri = 2+(4-i) // reverse index\n  // 2 == (2..&lt;5).trailing_one\n  // 4 == (2..&lt;5).leading_one\n  let v = match idx {\n   == 0 { 4 }\n   == 1 { 3 }\n   == 2 { 2 }\n  }\n  assert v == ri\n}\n\nfor (idx,i) in enumerate(123) {\n  assert i == 123 and idx==0\n}\n</code></pre>"},{"location":"pyrope/10-internals/#multiple-bit-selection","title":"Multiple bit selection","text":"<p>Ranges are sets, this creates potentially unexpected results in reverse <code>for</code> iterators, but also in bit section:</p> <pre><code>let v = 0xF0\n\nassert v@[0] == 0\nassert v@[4] == 1       // unsigned output\nassert v@sext[4] == -1  // signed output\n\nassert v@[3..=4] == 0b010 == v@[3,4]\nassert v@[4..=3 step -1] == 0b010\nassert v@[4,3] == v@[3,4] == 0b010\n\nlet tmp1 = (v@[4], v@[3])@[..]  // typecast from\nlet tmp2 = (v@[3], v@[4])@[..]\nlet tmp3 = v@[3,4]\nassert tmp1 == 0b01\nassert tmp2 == 0b100\nassert tmp3 == 0b10\n\nlet tmp1s = (v@sext[4], v@sext[3])@[..]  // typecast from\nlet tmp2s = (v@sext[3], v@sext[4])@[..]\nlet tmp3s = v@[4,3]\nassert tmp1s == 0b01\nassert tmp2s == 0b10\nassert tmp3s == 0b10\n\nlet tmp1ss = (v@sext[4], v@sext[3])@sext[..]  // typecast from\nlet tmp2ss = (v@sext[3], v@sext[4])@sext[..]\nlet tmp3ss = v@sext[3,4]\nassert tmp1ss == 0b01  ==  1\nassert tmp2ss == 0sb10 == -2\nassert tmp3ss == 0sb10 == -2 == v@sext[4,3]\n</code></pre> <p>The reason is that for multiple bit selection assumes a smaller to larger bits. If the opposite order is needed, support functions/code must explicitly do it.</p> <p>In Pyrope, there is no order in bit selection (<code>xx@[0,1,2,3] == xx@[3,2,1,0]</code>). This is done to avoid mistakes. If a bit swap is wanted, it must be explicit.</p> <pre><code>let reverse = fun(x:uint)-&gt;(total:uint) {\n  for i in 0..&lt;x.__bits {\n    total &lt;&lt;= 1\n    total  |= x@[i]\n  }\n}\nassert reverse(0b10110) == 0b01101\n</code></pre>"},{"location":"pyrope/10-internals/#unexpected-calls","title":"Unexpected calls","text":"<p>Passing a lambda argument with a <code>ref</code> does not have any side effect because lambdas without arguments need to be explicitly called or just passed as reference.</p> <pre><code>let args = fun(x) { puts \"args:{}\", b ; 1}\nlet here = fun()  { puts \"here\" ; 3}\n\nlet call_now   = fun(f:fun){ return f() }\nlet call_defer = fun(f:fun){ return f   }\n\nlet x0 = call_now(here)          // prints \"here\"\nlet e1 = call_now(args)          // compile error, args needs arguments\nlet x1 = call_defer(here)        // nothing printed\nlet e2 = call_defer(args)        // compile error, args needs arguments\nassert x0  == 3                  // nothing printed\nassert x1  == 3                  // nothing printed\n\nlet x2 = call_now(ref here)      // prints \"here\"\nlet e3 = call_now(ref args)      // compile error, args needs arguments\nlet x3 = call_defer(ref here)    // nothing printed\nlet x4 = call_defer(ref args)    // nothing printed\nassert x2  == 3                  // nothing printed\nassert x3()  == 3                // prints \"here\"\nassert x3  == 3                  // compile error, explicit call needed\nassert x4  == 1                  // compile error, args needs arguments\nassert x4(\"xx\") == 1             // prints \"args:xx\"\n</code></pre>"},{"location":"pyrope/10-internals/#if-is-an-expression","title":"<code>if</code> is an expression","text":"<p>Since <code>if</code>, <code>for</code>, <code>match</code> are expressions, you can build some strange code:</p> <pre><code>if if x == 3 { true }else{ false } {\n  puts \"x is 3\"\n}\n</code></pre>"},{"location":"pyrope/10-internals/#legal-but-weird","title":"Legal but weird","text":"<p>There is no <code>--</code> operator in Pyrope, but there is a <code>-</code> which can be followed by a negative number <code>-3</code>.</p> <pre><code>let v = (3)--3\nassert v == 6\n</code></pre> <ol> <li> <p>Narrowing is based on \"ABCD: eliminating array bounds checks on-demand\"   by Ras Bodik et al.\u00a0\u21a9</p> </li> </ol>"},{"location":"pyrope/10b-vslang/","title":"vs Other Languages","text":"<p>This section provides some snippet examples to understand the differences between Pyrope and a different set of languages.</p>"},{"location":"pyrope/10b-vslang/#generic-non-hdl","title":"Generic non-HDL","text":"<p>Pyrope is an HDL that tries to look like a non-HDL with modern/concise syntax. Some of the Pyrope semantics are simpler than most non-HDL because of several features like unlimited precision, lack of pointers and issues to manage memory do not exist in ASICs/FPGAs. These are explained in simpler HDL constructs section.</p> <p>There are some features in Pyrope that are non-existing in non-HDLs. ASICs/FPGAs design leverage some features like reset, pipelining, connecting modules that require syntax/semantics not needed in languages like Rust, C, Java. This section lists the main hardware specific syntax.</p>"},{"location":"pyrope/10b-vslang/#reset-vs-cycle","title":"reset vs cycle","text":"<p>Nearly all the programming languages have a \"main\" that starts execution. From the entry point, a precise control flow is followed until an \"exit\" is found. If there is no exit, the control flow eventually returns to the end of main and an implicit exit exist when \"main\" finishes. There is no concept of cycle or reset.</p> <p>Pyrope tries to imitate non-HDLs and it has the same entry point \"top\" and also follows a precise control flow from that entry point. This is what a non-hardware designer will expect, but there is no exit/abort. The control flow will continue until it returns to the end of the \"top\" or entry point.</p> <p>The key difference is that the \"top\" or entry point is called every cycle. From a hardware point of view, the whole program executes in a single clock cycle. All the program state is lost unless preserved in register variables.</p> <p>Those registers variables have an \"initialization\" step that in hardware corresponds to a reset phase. Each register declaration assignment has reset code only executed during reset.</p>"},{"location":"pyrope/10b-vslang/#defer","title":"Defer","text":"<p>Some programming languages like Zig or Odin have a defer statement. In non-HDLs, a defer means that the statements inside the defer are executed when the \"scope\" finishes. Usually, the defer statements are executed before the function return.</p> <p>Pyrope defers the statements not to the end of the scope but to the end of the clock cycle. The defer delays the \"write\" until the end of the clock cycle, the defer does not defer the reads, just the write or update. To read the value from the end of the cycle an attribute <code>variable.[defer]</code> must be used.</p> <p>These are constructs not existing in software but needed in hardware because it is necessary to connect blocks. Following the control flow from the top only allows to connect forward. Some contructs like connecting a ring require a \"backward edge\". The attribute <code>[defer]</code> allow such type of constructs.</p> <pre><code>var a = 1\nvar b = 2\n\ncassert a==1 and b==2\nb::[defer] = a\ncassert a==1 and b==2\n\ncassert b.[defer] == 1\n</code></pre>"},{"location":"pyrope/10b-vslang/#pipelining","title":"Pipelining","text":"<p>Pyrope should be easier to program than non-HDLs with the exception of dealing with cycles. While memory management tends to be the main complexity in non-HDLs, pipelining or dealing with interaction across cycles is the main complexity in HDLs.</p> <p>Pyrope has several constructs to help that do not apply to non-HDL, pipelining has most of the pipelining specific syntax.</p>"},{"location":"pyrope/10b-vslang/#c","title":"C++","text":"<p>Pyrope and C++ are quite different in syntax, but some nice C++23 syntax has similarities for Pyrope.</p> <pre><code>auto max_gap_count(std::vector&lt;int&gt; nums) {\n    std::ranges::sort(nums, std::greater{});\n    auto const diffs = nums\n        | std::views::adjacent_transform&lt;2&gt;(std::minus{});\n    return std::ranges::count(diffs, std::ranges::max(diffs));\n}\n</code></pre> <pre><code>let max_gap_count = fun(nums) {\n  let max  = import(\"std\").max\n  let sort = import(\"std\").sort\n  let adjacent_transform = fun(a,num,f) {\n    var res:[] = _\n    for i in 0..&lt;a.length step num {\n      res ++= f(a[i..+num])\n    }\n    return res\n  }\n  let count = fun(a,b) {\n    var r = 0\n    for i in a {\n      r += 1 when i == b\n    }\n    return r\n  }\n\n  return numbers\n     |&gt; sort(fun(a,b) { a&lt;b })\n     |&gt; adjacent_transform(num=2, fun(a,b) { a-b } )\n     |&gt; fun(a) { count(a, a.max) }\n}\n</code></pre> <p>A significant difference is that Pyrope everything is by value. In C++, you could do code with undefined behaviour very easily by mistake when dealing with pointers.</p> <pre><code>const T&amp; f2(T t)        { return t; } // returns pointer to local\n</code></pre>"},{"location":"pyrope/10b-vslang/#swift","title":"Swift","text":"<p>There are many diffirences with Swift, but this section just highlights a couple because it helps to understand the Pyrope semantics.</p>"},{"location":"pyrope/10b-vslang/#protocol-vs-pyrope-constrains","title":"Protocol vs Pyrope constrains","text":"<p>Swift protocols resemble type classes. As such require consent for implementing a functionality. Pyrope resembles C++ concepts that constraint functionality.</p> <pre><code>func add&lt;T&gt;(a:T, b:T) -&gt; T { a + b }  // compile error\nfunc add&lt;T:Numeric&gt;(a:T, b:T) -&gt; T { a + b }\n</code></pre> <pre><code>let add = fun(a,b) { a + b }            // OK, no constrains\nlet add = fun&lt;T:int&gt;(a:T,b:T) { a + b } // constrain both to have same type\n</code></pre> <p>When a protocol defines an interface, in Swift:</p> <pre><code>protocol Shape {\n  func name()      -&gt; String\n  func area()      -&gt; Float\n  func perimeter() -&gt; Float\n}\n\nclass Rectangle : Shape {  }\nclass Circle    : Shape {  }\n\nfunc print_share_info&lt;T:Shape&gt;(_ s:T) {\n\n}\n</code></pre> <p>In Pyrope: <pre><code>let Shape = (\n  ,name:fun(self)-&gt;(_:string)    = _\n  ,area:fun(self)-&gt;(_:Float)     = _  // NOTE: Pyrope does not have float type\n  ,perimeter:fun(self)-&gt;(_:Float)= _\n)\n\nlet Rectangle:(...Shape,...OtherAPI) = (...some_code_here)\nlet Circle:Shape = (...some_code_here)\n\nlet print_share_info = fun(s:Shape) { }\n</code></pre></p>"},{"location":"pyrope/10b-vslang/#rust","title":"Rust","text":"<p>Rust is not an HDL, as such it has to deal with many other issues like memory. This section is just a syntax comparison.</p>"},{"location":"pyrope/10b-vslang/#lambda","title":"Lambda","text":"<p>In Rust, the <code>self</code> keyword when applied to lambda arguments can be <code>&amp;self</code>, <code>self</code>, <code>&amp;mut self</code>. In Pyrope, there is only a <code>self</code> and <code>ref self</code>. The equivalent of the <code>&amp;mut self</code> is <code>ref self</code>. Pyrope does not have the equivalent of <code>mut self</code> that allows to modify a copy of self.</p> <pre><code>pub struct AnObject {\n  v:i32\n}\n\nimp AnObject {\n  pub fn f1(&amp;mut self) -&gt; i32 {\n    let res = self.v;\n    self.v += 1;\n    res\n  }\n  pub fn f2(self) -&gt; i32 {\n    self.v\n  }\n}\n</code></pre> <p>A Rust style Pyrope equivalent:</p> <pre><code>let AnObject = (\n  ,v:i32 = _\n)\n\nlet f1 = proc(ref self:AnObject) -&gt; :i32 { // unnamed output tuple\n  let res = self.v\n  self.v += 1\n  return res\n}\nlet f2 = fun(self:AnObject) -&gt; :i32 {\n  return self.v\n}\n</code></pre> <p>A more Pyrope style equivalent:</p> <pre><code>let AnObject = (\n  ,v:i32 = _\n  ,f1 = proc(ref self) -&gt; (res:i32) {\n    res = self.v\n    self.v += 1\n  }\n  ,f2 = fun(self) -&gt; :i32 { self.v }\n)\n</code></pre>"},{"location":"pyrope/10b-vslang/#typescript","title":"Typescript","text":"<p>Pyrope has a type system quite similar to Typescript, but there are significant differences. The main is that Pyrope does not allow union types.</p> <p>There are also difference in some semantics. For example, Typescript <code>\"foo\" in bar</code> is equivalent to the <code>bar has \"foo\"</code> in Pyrope. Both check if entry <code>foo</code> exists in the tuple <code>bar</code> (<code>bar.foo</code>). There is no Typescript equivalent to the Pyrope <code>\"foo\" in bar</code> which checks if <code>bar</code> is a tuple with an entry equal to string <code>\"foo\"</code>.</p>"},{"location":"pyrope/10b-vslang/#matlab","title":"Matlab","text":"<p>Matlab has a convenient multi-dimensional array or array initialization. It does not require comma. E.g: <code>a = [a b 100 c]</code> is valid Matlab.</p> <p>Pyrope requires commas to distinguish from multi-line statements, hence <code>a = [a,b,100,c]</code> To initialize a multi-dimensional array, it follows other languages syntax, but in Pyrope both <code>()</code> and <code>[]</code> are allowed and have the same meaning.</p> <pre><code>let x = [[1,2],[3,4]]\nassert x == ((1,2),[3,4])\nassert x[0,1] == 2 == x[0][1]\nassert x[1,0] == 3 == x[1][0]\n</code></pre>"},{"location":"pyrope/10b-vslang/#go","title":"Go","text":"<p>Pyrope and go have several similarities but with slightly different syntax. For example, functions capacity to have multiple name return values is quite similar.</p> <p>Some significant difference is the built-in and imports.</p> <p>In Go: <pre><code>func larger(a, b []string) []string {\n  len := len(a)\n  if len &gt; len(b) { // Error: invalid operation: cannot call non-function len (variable of type int)\n    return a\n  }\n  return b\n}\n</code></pre></p> <p>In Pyrope: <pre><code>import std as std\n\nfun larger(a:[]string, b:[]string) -&gt; (:[]string) {\n  let strlen := std.strlen(a)\n  if strlen &gt; std.strlen(b) {\n    return a\n  }\n  return b\n}\n\n// Using attributes (bits != strlen, but works too)\nfun larger(a:[]string, b:[]string) -&gt; (:[]string) {\n  if a.[bits] &gt; b.[bits] { a }else{ b }\n}\n</code></pre></p>"},{"location":"pyrope/11-deprecated/","title":"Deprecated or Future","text":"<p>Pyrope has been in internal development for many years, those are some features tried and deprecated or removed until a better solution is found.</p>"},{"location":"pyrope/11-deprecated/#step-options","title":"<code>step</code> options","text":"<p>The <code>step</code> command breaks the execution of the function in the statements before and after the step. In the next cycle, the statements after the step are executed. The issue was that the step could be placed inside complicated nests of 'if' and 'for' loops. This results in a difficult code to get right. </p> <p>The plan is to add something like this feature in the future, once a cleaner implementation is designed.</p>"},{"location":"pyrope/11-deprecated/#fluid-pipelines","title":"Fluid pipelines","text":"<p>The plan is to re-add the fluid pipelines syntax, but all the other features must be added first.</p>"},{"location":"pyrope/11-deprecated/#bundle-index-with-bundles","title":"Bundle index with bundles","text":"<p>Bundles do not allow an index with another bundle unless it is a trivial bundle (one element). To illustrate the current constraints:</p> Bundle index (not allowed)Current legal Pyrope <pre><code>type Person = (name:string, age:u32)\nvar a = (one:Person, two:Person)\n\nx = ('one', 'two')\na[x].age = 10\n</code></pre> <pre><code>let Person = (name:string=_, age:u32=_)\nvar a = (one:Person, two:Person)\n\nx = 'one'\ny = 'two'\na[x].age = 10\na[y].age = 10\n</code></pre> <p>In the future, it may be allowed but some options may not be allowed. For example, if the index bundle is not unordered, the result of the assignment may not be easy to predict by the programmer. </p>"},{"location":"pyrope/11-deprecated/#asyncawait-and-coroutines","title":"async/await and coroutines","text":"<p>In non-hardware languages, there are several constructs to handle asynchronicity.  Asynchronicity is not to leverage parallelism for speedup but software constructs to handle long latency operations. The most popular models/techniques are async/await, coroutines, and actors. </p> <p>In a way, pipelining could be expressed with similar constructs. This has the advantage of having a larger community (software) to understand/program hardware more easily.</p> <p>To illustrate the point, suppose a telescoping subtract-like unit that provides a response of the operation in 1 or 2 cycles depending on the value of the input.  If the <code>b</code> input is 0, the result is <code>a+1</code>. Otherwise, the result is <code>a-b+1</code>. The first finishes in 1 cycle, the second in 2 cycles. This seemly easy idea is not so easy to implement because it needs to handle 2 flops and there could be a structural hazard on the flop if the previous cycle was scheduled for 2 cycles and the current for 1 cycle.</p> <p>This example explicitly manages the valid output signals.</p> <pre><code>let telescope_unit = fun(a:u32,b:u32,start:bool) -&gt; (res:u32) {\n\n  reg result_done = 0\n  reg result_flop = 0\n\n  if result_done {\n    res = result_flop\n  }\n\n  reg int_done = _\n  reg int_flop = _\n  reg int_b = _\n\n  if int_done {  // pending work (2 cycle op, can not telescope)\n    result_flop = int_flop-int_b\n    result_done = int_done\n    int_flop = a+1\n    int_b    = b\n    int_done = start\n  }else{          // no pending work from before (telescoping is allowed)\n    if b == 0 {\n      result_flop = a+1\n      result_done = start\n    }else{\n      result_flop = int_flop-int_b\n      int_flop = a+1\n      int_b    = b\n      int_done = start\n    }\n  }\n}\n</code></pre> <p>In a simple telescoping use case, the <code>puts</code> command will be called 1 or 2 cycles after the <code>telescope_unit</code> starts. For the designer, this is quite difficult to handle. How many flops to add to remember the starting point for <code>a</code> and <code>b</code>.</p> <pre><code> let res1 =#[1,2] telescope_unit(a,b,start)\n\n if res1? {\n   puts \"{}-{}+1 is {}\", a, b, res1.res  // incorrect reference to a\n }\n</code></pre> <p>To address the issue that the <code>telescope_unit</code> can have multiple cycles to complete, a <code>yield</code> directive can behave like co-routines. Effectively, remembering the live-ins and continue executing when the condition is satisfied.</p> <pre><code> let res1 =#[1,2] telescope_unit(a,b,start)\n\n yield res1? // wait for condition to happen\n assert res1?\n\n // code executed 1 or 2 cycles after telescope_unit is called\n puts \"{}-{}+1 is {}\", a, b, res1.res\n</code></pre> <p>An alternative implementation is using the <code>#&gt;identifier[lat=cycles]</code> keyword. The disadvantage is that two operations could finish on the same cycle, and the circuits are not as efficient.</p> <pre><code>// implicit start/end (starts when called)\nlet telescope_unit3 = fun(a:u32,b:u32) -&gt; (_:u32) {\n\n  {\n    let tmp = a+1\n  } #&gt;one_pipe[lat=1] {\n    if b == 0 {\n      return tmp\n    }\n    let tmp2 = tmp-b\n  } #&gt; {\n    return tmp2\n  }\n}\n</code></pre> <p>The code sample for explicitly managed step function usage:</p> <pre><code> let res2 =#[1,2] telescope_unit3(a,b,start)\n\n if res2? { // code executed 1 or 2 cycles after telescope_unit is called\n   puts \"{}-{}+1 is {}\", a, b, res2\n }\n</code></pre> <p>The code sample for implicitly managed step function usage:</p> <pre><code> async res3 =#[1,2] telescope_unit3(a,b) when start\n\n await res3 {\n   // a and b could have the correct results due to the async/await\n   puts \"{}-{}+1 is {}\", a, b, res3.res\n }\n</code></pre>"},{"location":"pyrope/11-deprecated/#extensible-enums","title":"Extensible enums","text":"<p>Once an enum is created, it can not be modified. There is no reason not to support compile time addition/removal from an enum. Languages with union types could behave like extending an enum, but not reducing it. Some potential API for Pyrope</p> <p>Using the set operations:</p> <pre><code>enum Order = (One, Two, Three)\nenum Order2 = (...Order, Four)\nenum Order2 = Order ++ Four       // error on overlap?\nenum Order3 = Order except Three  // new \"remove\" tuple op\n</code></pre> <p>Overloading the logical operations is another option, but breaks the rule of lack of overloading in ops:</p> <pre><code>enum Order2 = Order or (Four)\nenum Order3 = Order and not (Three)\n</code></pre> <p>Using the trait syntax creates some confusion on the meaning, but an option is to have custom keywords for enum:</p> <pre><code>enum Order2 = Order with (Four)\nenum Order3 = Order except Three\n</code></pre> <p>Once we support adding/removing to enums, operations like this would make sense:</p> <pre><code>match x:Order {\n  in Order2      { puts \"1 or 2\" }\n  == Order.Three { puts \"3\"      }\n}\n</code></pre>"},{"location":"pyrope/11-deprecated/#repipe","title":"repipe","text":"<p>Note</p> <p>The <code>repipe</code> statement was deprecated because the <code>pipestage</code> could  achieve similar results more cleanly in most of the cases that it was  tried. Also, <code>repipe</code> would have required a custom lgraph pass to balance  pipeline stages.</p> <p>The <code>repipe</code> statement tries to balance the number of pipeline stages by inserting registers. If it can not guarantee the same pipeline depth, a compile error is generated. If there is any feedback loop, likely, the pipeline can not be rebalanced with <code>repipe</code>.</p> <p>The syntax for <code>repipe</code> is <code>repipe res = (list of variables)</code>. The result is a tuple with as many fields as the list of input variables but with enough flops so that the pipeline is balanced from the list of variables and the function inputs.</p>"},{"location":"pyrope/11-deprecated/#liam-constructs","title":"Liam constructs","text":"<p>In most HDLs loops have to be compile time unrolled, in an earlier version of Pyrope<sup>1</sup> allowed for extra keywords to create an actor model and create state machines where each loop iteration will be executed in a cycle.</p> <pre><code>while some_condition {\n\n  step   // next cycle starts here\n}\n</code></pre> <p>Fluid constructs:</p> <ul> <li><code>variable?</code> check if <code>variable</code> valid bit is set</li> <li><code>variable!</code> check if <code>variable</code> has a fluid backpressure</li> <li><code>keep</code> do not consume variable on use</li> <li><code>step</code> stop the cycle here, continue next cycle after the yield statement</li> </ul> <ol> <li> <p>Liam: An Actor Based Programming Model for HDLs, Haven Skinner, Rafael T. Possignolo, and Jose Renau. 15th ACM-IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE), October 2017.\u00a0\u21a9</p> </li> </ol>"},{"location":"pyrope/12-lnast/","title":"LNAST","text":"<p>This document is to showcase some of the Pyrope to LNAST translation. This is useful to have a more \"formal\" description of the language semantics.</p>"},{"location":"pyrope/12-lnast/#variable-names","title":"Variable names","text":"<p>LNAST does not rename variables to be SSA, it relies in a symbol table to track past entries. Nevertheless, to reduce amount of tracking information when a variable starts with underscores (<code>___foo</code> or <code>_._foo</code>), the variable can not be updated, BUT it is still legal to update tuple fields inside <code>___foo</code> like <code>___foo.bar = 3</code>. Program variables names that do not need SSA (<code>let</code>) can use <code>_._foo</code> to reduce tracking. Special variable names like the ones needing an underscore use double tick in the name <code>_foo here</code>. Those are special variables names that do not allow to use compact tuple representation like <code>foo here.field</code>.</p> PyropeLNAST directLNAST optimized <pre><code>let x = 3 + 1\nvar z = 4\n`foo x` = x + z + 2\n</code></pre> <pre><code>plus\n  ref ___1\n  const 3\n  const 1\nlet\n  ref  x\n  ref  ___1\nvar\n  ref z\n  const 4\nplus\n  ref ___2\n  ref x\n  ref z\n  const 2\nassign\n  ref `foo x`\n  ref ___2\n</code></pre> <pre><code>plus\n  ref ___1\n  const 3\n  const 1\nlet\n  ref  x\n  ref  ___1\nvar\n  ref z\n  const 4\nplus\n  ref `foo x`\n  ref x\n  ref z\n  const 2\n</code></pre> <p>The three LNAST nodes to set values in variables are <code>let</code>/<code>var</code>/<code>assign</code>. Each can have types and/or attributes.</p> PyropeLNAST <pre><code>let a:u2:[foo] = b:u1:[bar]\n\nx:u2:[foo] = y:u1:[bar]\n</code></pre> <pre><code>set\n  ref a\n    prim_type_uint\n      const 2\n    attr_ref_set\n      const foo\n      const true\n  ref b\n    prim_type_uint\n      const 2\n    attr_check\n      const bar\n      const true\n\nassign\n  ref x\n    prim_type_uint\n      const 2\n    attr_ref_set\n      const foo\n      const true\n  ref y\n    prim_type_uint\n      const 2\n    attr_check\n      const bar\n      const true\n</code></pre>"},{"location":"pyrope/12-lnast/#tuples","title":"Tuples","text":"<p>Tuples are \"ordered\" sequences that can be named. There are LNAST tuple specific nodes (<code>tup_add</code>, <code>tup_set</code>, <code>tup_get</code>, <code>tup_concat</code>) but in many cases the direct LNAST operations can handle tuples directly.</p> <ul> <li><code>tup_add</code> creates a new tuple with entries</li> <li><code>tup_set</code> adds/updates a field to an existing tuple.</li> <li><code>tup_get</code> gets the contents of a tuple entry</li> <li><code>tup_concat</code> concatenates two or more tuples</li> </ul> <p>To indicate the tuple position, identifiers can have <code>:pos:name</code>. For example <code>x.:3:foo = 2</code> is legal. It is the same as <code>x[3] = 2</code> or <code>x.foo=2</code> and check that entry <code>3</code> has label <code>foo</code>. This allows to create more compact LNAST. Direct access in operations like <code>plus</code> behave like a <code>tup_set</code> or <code>tup_get</code>.</p> Tuple in PyropeLNAST directLNAST optimizedLNAST Alternative <pre><code>x = 3\na = (b=2, x=x+1, y=self.b+1)\n</code></pre> <pre><code>assign\n  ref      x\n  const    3\nassign\n  ref      ___t1\n  const    2\nplus\n  ref      ___t2\n  ref      x\n  const    1\nplus\n  ref      ___t3\n  ref      ___t1\n  const    1\ntup_add\n  ref      a\n  var\n    ref      b\n    ref      __t1\n  var\n    ref      x\n    ref      ___t2\n  var\n    ref      y\n    ref      ___t4\n</code></pre> <pre><code>assign\n  ref      x\n  const    3\nplus\n  ref      ___t2\n  ref      x\n  const    1\nplus\n  ref      ___t3\n  ref      ___t1\n  const    1\ntup_add\n  ref      a\n  var\n    ref      b\n    const   2\n  var\n    ref      x\n    ref      ___t2\n  var\n    ref      y\n    ref      ___t4\n</code></pre> <pre><code>assign\n  ref      x\n  const    3\nvar\n  ref      a.0b\n  ref      2\nplus\n  ref      ___t1\n  ref      x\n  const    1\nvar\n  ref      a.1x\n  ref      ___t1\nplus\n  ref      ___t2\n  const    a.0b\n  const    1\nvar\n  ref     a.2y\n  ref     ___t2\n</code></pre> <p><code>tup_set</code> and <code>tup_get</code> can access through several levels in one command. <code>tup_add</code> does not allow recursive entrances, it requires intermediate tuple construction. <code>attr_get</code> and <code>attr_set</code> follow the same syntax as <code>tup_get</code>/<code>tup_set</code>.</p> PyropeLNAST <pre><code>x = tup[1].foo[xx]\ntup[4].foo[yy] = y\n\nz = (foo=(bar=1))\n</code></pre> <pre><code>tup_get\n  ref x\n  ref tup\n  const 1\n  const foo\n  ref xx\n\ntup_set\n  ref tup\n  const 4\n  const foo\n  ref yy\n  ref y\n\ntup_add\n  ref ___1\n  var\n    ref bar\n    const 1\n\ntup_add\n  ref z\n  var\n    ref foo\n    ref ___1\n</code></pre> <p>Tuples can have a <code>let</code> in declaration to indicate that the field is immutable.</p> Tuple in PyropeLNAST directLNAST Optimized <pre><code>var a = (b=2, let x=1+1)\n</code></pre> <pre><code>assign\n  ref    ___t1\n  const  2\nplus\n  ref    ___t2\n  const  1\n  const  1\ntup_add:\n  ref     a\n  var\n    ref     b\n    ref     __t1\n  let\n    ref     x\n    ref     ___t2\n</code></pre> <pre><code>var\n  ref    a.:0:b\n  const  2\nplus\n  ref    ___2\n  const  1\n  const  1\nlet\n  ref    a.:1:x\n  ref    ___2\n</code></pre> <p>Tuple concatenation does not use <code>plus</code> but the <code>tup_concat</code> operator.</p> Tuple in PyropeLNAST directLNAST optimized <pre><code>var a = (2, 1+1)\nlet x = a ++ (c=3) ++ 1\n</code></pre> <pre><code>assign\n  ref    ___1\n  const  2\nplus\n  ref    ___2\n  const  1\n  const  1\ntup_add:\n  ref    ___33\n  ref    ___1\n  ref    ___2\nvar\n  ref    a\n  ref    ___33\ntup_add\n  ref    ___3\n  const  c\n  const  3\ntup_concat\n  ref    ___4\n  ref    a\n  ref    ___3\n  const  1\nlet\n  ref    x\n  ref    ___4\n</code></pre> <pre><code>var\n  ref    a.:0:\n  const  2\nplus\n  ref    ___2\n  const  1\n  const  1\nvar\n  ref    a.:1:\n  ref    ___2\ntup_add\n  ref    ___3\n  const  c\n  const  3\ntup_concat\n  ref    ___4\n  ref    a\n  ref    ___3\n  const  1\nlet\n  ref    x\n  ref    ___4\n</code></pre>"},{"location":"pyrope/12-lnast/#attributes","title":"Attributes","text":"<p>There are 3 main operations with attributes: set, get, check, but 4 types of LNAST nodes (<code>attr_get</code>/<code>attr_set</code> and <code>attr_ref_set</code>/<code>attr_ref_check</code>). <code>attr_get</code>/<code>attr_set</code> operate at the root level and have the same syntax as <code>tup_set</code>/<code>tup_get</code> but the last entry is an attribute name. <code>attr_ref_set</code>/<code>attr_ref_check</code> are sub-nodes of <code>ref</code>, as such they operate over the associated <code>ref</code> node destination.</p> <p><code>attr_ref_check</code> only works comparing equal to a <code>const</code> or <code>ref</code>. More complex attribute comparisons needs <code>attr_get</code> and <code>casserts</code> to operate. </p> <p>Attribute set are in left-hand-side of assignments which can also be in tuple entries.</p> PyropeLNAST direct <pre><code>a::[f=3,b] = 1\nx = (y::[z=7]=2, 4)\n</code></pre> <pre><code>assign\n  ref a\n    attr_ref_set\n      const f\n      const 3\n    attr_ref_set\n      const b\n      const true\n  const 1\n\ntup_add\n  ref ___1\n  var\n    ref y\n      attr_ref_set\n        const z\n        const 7\n    const 2\n  const 4\n\nassign\n  ref x\n  ref ___1\n</code></pre> <p>Attribute checks are always right-hand-side. The constraint in all the cases is that an attribute name can be check against an expression but only 3 basic comparisons are valid (<code>[attr==(expr)</code> or <code>[attr]</code> or <code>[!attr]</code>). The expression can not use other attribute fields. If complex relationships must be checked between attributes a <code>cassert</code> must be used.</p> PyropeLNAST option 1LNAST option 2 <pre><code>var x = (let z=x::[!y], 4::[foo])\nlet y = a::[f==3,b] + 1\n</code></pre> <pre><code>assign\n  ref ___tmp\n  const 4\n\ntup_add\n  ref ___4\n  let\n    ref z\n    ref x\n      attr_ref_check\n        const y\n        const false\n  ref ___tmp\n    attr_ref_check\n      const foo\n      const true\nvar\n  ref x\n  ref ___4\n\nplus\n  ref ___1\n  ref a\n    attr_ref_check\n      const f\n      const 3\n    attr_ref_check\n      const b\n      const true\n  const 1\nlet\n  ref y\n  ref ___1\n</code></pre> <pre><code>tup_add\n  ref ___4\n  let\n    ref z\n    ref x\n      attr_ref_check\n        const y\n        const false\n  const 4\n\nattr_get\n  ref ___no_attr_const_check\n  const 4\n  const foo\n\nfcall\n  ref ___0\n  ref cassert\n  ref ___no_attr_const_check\n\nvar\n  ref x\n  ref ___4\n\nplus\n  ref ___1\n  ref a\n    attr_ref_check\n      const f\n      const 3\n    attr_ref_check\n      const b\n      const true\n  const 1\nlet\n  ref y\n  ref ___1\n</code></pre>"},{"location":"pyrope/12-lnast/#sticky-attributes","title":"Sticky attributes","text":"<p>Attributes can be sticky or not. A sticky attribute \"polutes\" or keeps the attribute to the left-hand-side expression. Non-sticky attributes do not affect or propagate.</p> <p>Attributes are not sticky by default, but some like <code>.[debug]</code> is a sticky attribute. This means that if any of the elements in any operation has a debug attribute, the result also has a <code>.[debug]</code> attribute. There is no way to remove these attributes.</p> PyropeLNAST <pre><code>let d::[debug] = 3\n\nvar a = d + 100\n\ncassert a.[debug]  // debug is sticky\n</code></pre> <pre><code>let\n  ref d\n    attr_ref_set\n      const debug\n      const true\n  const 3\n\nplus\n  ___tmp\n  ref d\n  const 100\n\nvar\n  ref a\n  ref ___tmp\n\nattr_get\n  ref ___get\n  ref a\n  const debug\n\nfcall\n  ___unused\n  ref cassert\n  ref ___get\n</code></pre> <p>Once a variable gets assigned an attribute, the attribute stays with the variable and any variables that got a direct copy. The only way to remove it is with arithmetic operations and/or bit selection.</p> <pre><code>let foo::[attr1=2] = 3\n\nvar foo2 = foo\ncassert foo2.[attr1] == 2\n\nlet foo3 = foo@[..]\ncassert foo3 !has _::[attr1]\n\nvar xx = 4\nxx::[attr2=5] = 1\n\nlet xx2 = xx\ncassert xx2.[attr2] == 5\ncassert xx2 has _::[attr2]\n\nlet xx3 = xx + 0\ncassert xx3 !has _::[attr2]\n</code></pre>"},{"location":"pyrope/12-lnast/#bit-selection","title":"Bit selection","text":"<p>Pyrope has several bit selection operations. The default maps <code>get_mask</code> and <code>set_mask</code> LNAST nodes. One important thing is that both <code>get_mask</code> and <code>set_mask</code> operate over a MASK. This means that it is a one-hot encoding if a single bit is operated. The one-hot encoding can be created with a <code>range</code> or with a <code>shl</code> operator.</p> PyropeLNAST directLNAST optimized <pre><code>foo@[1,2] = xx\nyy = foo@[5] + xx@[1..&lt;4]\n</code></pre> <pre><code>shl\n  ref ___c1\n  const 1\n  const 1\n\nshl\n  ref ___c2\n  const 1\n  const 2\n\ntup_add\n  ref ___t\n  ref ___c1\n  ref ___c2\n\nset_mask\n  ref foo\n  ref foo\n  ref ___t\n  ref xx\n\nrange\n  ref ___c5\n  const 5\n  const 5\n  const 1\n\nget_mask\n  ref ___3\n  ref foo\n  ref ___c5\n\nrange\n  ref ___4\n  const 1\n  const 3\n  const 1\n\nget_mask\n  ref ___5\n  ref xx\n  ref ___4\n\nadd\n  ref yy\n  ref ___4\n  ref ___5\n</code></pre> <pre><code>shl\n  ref ___t\n  const 1\n  const 1\n  const 2\n\nset_mask\n  ref foo\n  ref foo\n  ref ___t\n  ref xx\n\nget_mask\n  ref ___3\n  ref foo\n  const 5\n\nrange\n  ref ___4\n  const 1\n  const 3\n  const 1\n\nget_mask\n  ref ___5\n  ref xx\n  ref ___4\n\nadd\n  ref yy\n  ref ___4\n  ref ___5\n</code></pre> <p>It is possible to use a <code>foo@sext[range]</code> to perform a bit selection with sign extension. The <code>sext</code> LNAST node is equivalent to the Lgraph <code>sext</code> that has 2 inputs. The variable and from what bit to perform sign-extension. This means that the LNAST translation needs a <code>get_mask</code> and a <code>sext</code> node. The <code>sext</code>, <code>+</code>, <code>|</code>, <code>^</code> bit selection modifiers can only be applied to right-hand-side operations.</p> PyropeLNAST <pre><code>let t1 = foo@sext[..=4]\nlet t2 = foo@|[..=4]\nlet t3 = foo@&amp;[..=4]\nlet t4 = foo@^[..=4]\nlet t5 = foo@+[..=4]\n</code></pre> <pre><code>range\n  ref ___r\n  const 0\n  const 4\n  const 1\n\nget_mask\n  ref ___t\n  ref foo\n  ref ___r\n\nsext\n  ref ___t1\n  ref ___t\n  const 4\nlet\n  ref t1\n  ref ___t1\n\nreduce_or\n  ref ___t2\n  ref ___t\nlet\n  ref t2\n  ref ___t2\n\nreduce_and       // reduce_and(x) == (sext(x) == -1)\n  ref ___t3\n  ref ___t\nlet\n  ref t3\n  ref ___t3\n\nreduce_xor\n  ref ___t4\n  ref ___t\nlet\n  ref t4\n  ref ___t4\n\npopcount\n  ref ___t5\n  ref ___t\nlet\n  ref t5\n  ref ___t5\n</code></pre>"},{"location":"pyrope/12-lnast/#direct-lnastlgraph-call","title":"Direct LNAST/Lgraph call","text":"<p>A direct Lgraph call can be done with <code>__cell</code> where <code>cell</code> is the Lgraph cell like <code>plus</code>, <code>LUT</code>, <code>memory</code>. In LNAST this is translated like a lambda call.</p> PyropeLNAST <pre><code>let foo = 3\nlet bar = 300\nlet b = __plus(1,2,foo,bar)\n</code></pre> <pre><code>let\n  ref foo\n  const 3\nlet\n  ref bar\n  const 300\ntup_add\n  ref ___0\n  const 1\n  const 2\n  ref foo\n  ref bar\nfcall\n  ref b\n  ref __plus\n  ref ___0\n</code></pre> <p>A direct LNAST call can be done calling an LNAST method, where the first entry is the root LNAST node, and rest follow a tree syntax with strings.</p> PyropeLNAST <pre><code>LNAST(\"let\", (\"ref\", \"x\"), (\"const\", \"5\"))\n</code></pre> <pre><code>let\n  ref x\n  const 5\n</code></pre>"},{"location":"pyrope/12-lnast/#basic-operators","title":"Basic operators","text":"<p>Basic operators are binary or unary operators in Pyrope that have a one-to-one translation to LNAST nodes.</p>"},{"location":"pyrope/12-lnast/#unary","title":"Unary","text":"<ul> <li><code>!a</code> or <code>not a</code> translates to <code>lnot</code></li> <li><code>~a</code> translates to <code>not</code></li> <li><code>-a</code> translates to <code>minus(0,a)</code></li> </ul>"},{"location":"pyrope/12-lnast/#binary-integer","title":"Binary integer","text":"<ul> <li><code>a + b</code> translates to <code>plus</code></li> <li><code>a - b</code> translates to <code>minus</code></li> <li><code>a * b</code> translates to <code>mult</code></li> <li><code>a / b</code> translates to <code>div</code></li> <li><code>a &amp; b</code> translates to <code>and</code></li> <li><code>a | b</code> translates to <code>or</code></li> <li><code>a ^ b</code> translates to <code>xor</code></li> <li><code>a &gt;&gt; b</code> translates to <code>sra</code></li> <li><code>a &lt;&lt; b</code> translates to <code>shl</code></li> </ul> <p>There is a <code>mod</code> LNAST operator that performs module operations. It does not have a direct Pyrope syntax, but it can be called directly <code>__mod(a,b)</code>.</p>"},{"location":"pyrope/12-lnast/#binary-boolean","title":"Binary boolean","text":"<ul> <li><code>a and b</code> translated to <code>land</code></li> <li><code>a or b</code> translates to <code>lor</code></li> </ul>"},{"location":"pyrope/12-lnast/#complex-operators","title":"Complex operators","text":"<p>Complex operators are binary operators in Pyrope that require more than one LNAST statement.</p>"},{"location":"pyrope/12-lnast/#binary-integer_1","title":"Binary integer","text":"<p>Binary nand (<code>x=a ~&amp; b</code>): <pre><code>and\n  ref ___0\n  ref a\n  ref b\nnot\n  ref x\n  ref ___0\n</code></pre></p> <p>Binary nor (<code>x=a ~| b</code>): <pre><code>or\n  ref ___0\n  ref a\n  ref b\nnot\n  ref x\n  ref ___0\n</code></pre></p> <p>Binary xor (<code>x=a ~^ b</code>): <pre><code>xor\n  ref ___0\n  ref a\n  ref b\nnot\n  ref x\n  ref ___0\n</code></pre></p> <p>Logical shift right (<code>x = a@[..] &gt;&gt; b</code>): <pre><code>get_mask\n  ref ___0\n  ref a\nsra\n  ref x\n  ref ___0\n  ref b\n</code></pre></p>"},{"location":"pyrope/12-lnast/#binary-logical","title":"Binary logical","text":"<p>Logical implication (<code>x = a implies b</code>): <pre><code>not\n  ref ___0\n  ref a\nlor\n  ref x\n  ref ___0\n  ref b\n</code></pre></p> <p>Logical nand (<code>x = a !and b</code>): <pre><code>land\n  ref ___0\n  ref a\n  ref b\nnot\n  ref x\n  ref ___0\n</code></pre></p> <p>Logical nor (<code>x = a !or b</code>): <pre><code>lor\n  ref ___0\n  ref a\n  ref b\nnot\n  ref x\n  ref ___0\n</code></pre></p> <p>Logical not implication (<code>x = a !implies b</code>): <pre><code>not\n  ref ___0\n  ref b\nland\n  ref x\n  ref a\n  ref ___0\n</code></pre></p> <p>Short-circuit boolean (<code>and_then</code>/<code>or_else</code>)</p> <p>The short-circuit boolean prevent expressions from being evaluated. This only matters if there is a procedure call, but at LNAST it is not possible to know due to getter overload. As a result, the sequence of statments is translated to a sequence of nested if statements.</p> PyropeLNAST <pre><code>a = b and_then c and_then (d or e)\n</code></pre> <pre><code>land\n  ref ___0\n  ref b\n  ref c\nassign\n  ref a\n  ref ___0\nif\n  ref ___0\n  stmts\n    lor ___1\n      ref d\n      ref e\n    assign\n      ref a\n      ref ___1\n</code></pre> PyropeLNAST <pre><code>a = b or_else c or_else (d and e)\n</code></pre> <pre><code>lor\n  ref ___0\n  ref b\n  ref c\nassign\n  ref a\n  ref ___0\nif\n  ref ___0\n  stmts\n  stmts  // else only\n    land\n      ref ___1\n      ref d\n      ref e\n    assign\n      ref a\n      ref ___1\n</code></pre>"},{"location":"pyrope/12-lnast/#tupleset-operators","title":"Tuple/Set operators","text":"<p>The <code>in</code> operator does not have a Lgraph equivalent becuase it is type dependent: tuple, range, or enumerate. The range and enumerate can get translated to an AND gate over the bitcode translation, but the tuple check requires a tuple check.</p> <pre><code>let tup=(1,2,3)\nlet ran=1..&lt;5\nlet enu = enum(a,b=(x,y),c)\n\ncassert 2 in tup\ncassert 3 in ran\ncassert enu.b.x in enu.b\n</code></pre> <p>The resul is a common <code>in</code> LNAST operation that gets different functionality dependent on the input type.</p> PyropeLNAST <pre><code>c = a in b\nd = a !in b\n</code></pre> <pre><code>in\n  ref c\n  ref a\n  ref b\nnot\n  ref d\n  ref c\n</code></pre> <p>There are two tuple concatenate operator  <code>a ++ b</code> and <code>(a,...b)</code>. <code>x=a++b</code> translates to:</p> <pre><code>tup_concat\n  ref x\n  ref a\n  ref b\n</code></pre> <p>The inplace concatenate is equivalent but it has a check (<code>cassert</code>) to detect overlap. After the concatenation, the fields in <code>a</code> and <code>b</code> should be found in the result <code>x</code> or there was an overlap.</p> <p><code>x=(a,..b)</code> translates to: <pre><code>tup_concat\n  ref x\n  ref a\n  ref b\nin\n  ref ___1\n  ref a\n  ref x\nin\n  ref ___2\n  ref b\n  ref x\nland\n  ref ___3\n  ref ___1\n  ref ___2\nfcall\n  ref ___0\n  ref cassert\n  ref ___3\n</code></pre></p>"},{"location":"pyrope/12-lnast/#tuple-to-operator","title":"Tuple to operator","text":"<p>The <code>to</code> is an iterator but instead of a range, it creates a tuple.</p> <p><code>tmp = a to b by c</code> translates to: <pre><code>to\n  ref tmp\n  ref a\n  ref b\n  ref c\n</code></pre></p> <p><code>tmp = 3 to b</code> translates to: <pre><code>to\n  ref tmp\n  const 3\n  ref b\n  const 1\n</code></pre></p>"},{"location":"pyrope/12-lnast/#range-operator","title":"Range operator","text":"<p>Ranges can be open or closed. The closed ranges have the start/end/step defined.</p> <p><code>x = a..&lt;=b by 2</code> translates to: <pre><code>range\n  ref x\n  ref a\n  ref b\n  const 2\n</code></pre></p> <p><code>x = a..&lt;=b by 2</code> translates to: <pre><code>range\n  ref x\n  ref a\n  ref b\n  const 2\n</code></pre></p> <p><code>x = a..&lt;b</code> translates to: <pre><code>sub\n  ref tmp\n  ref b\n  ref 1\n\nrange\n  ref x\n  ref a\n  ref tmp\n  const 1\n</code></pre></p>"},{"location":"pyrope/12-lnast/#type-operators","title":"Type operators","text":"<p>To check if a field name or position exists in a tuple, <code>x = a has b</code> translates: <pre><code>has\n  ref x\n  ref a\n  ref b\n</code></pre></p> <p>To check the tuple structure, Pyrope has <code>a does b</code>. It returns true if the tuple of <code>a</code> a subset of <code>b</code>. <code>x = a does b</code> translates to: <pre><code>does\n  ref x\n  ref a\n  ref b\n</code></pre></p> <p>To check equality of tuples <code>x = a equals b</code> same as <code>x = (a does b) and (b does a)</code>. Translates to: <pre><code>does\n  ref ___0\n  ref a\n  ref b\ndoes\n  ref ___1\n  ref b\n  ref a\nland\n  ref x\n  ref ___0\n  ref ___1\n</code></pre></p> <p>The <code>a case b</code> does match operation. <code>a case b</code> same as <code>cassert b does a</code> and for each <code>b</code> field with a defined value, the value matches <code>a</code> (<code>nil</code>, <code>0sb?</code> are undefined values). <code>x = a case b</code> translates to: <pre><code>does\n  ref ___0\n  ref b\n  ref a\nfcall\n  ref ___1\n  ref cassert\n  ref ___0\nin\n  ref x\n  ref b\n  ref a\n</code></pre></p> <p>To perform a nominal type check, the attributes can be accessed directly. <code>x = a is b</code> translates to: <pre><code>attr_get\n  ref ___0\n  ref a\n  const typename\nattr_get\n  ref ___1\n  ref b\n  const typename\neq\n  ref x\n  ref ___0\n  ref ___1\n</code></pre></p>"},{"location":"pyrope/12-lnast/#ifunique-if","title":"if/unique if","text":"<p>Like many modern languages, <code>if</code> accepts not only a boolean expression but a sequence of statements. Like C++17, before a condition, there can be a sequence of statements that can include variable declarations. Pyrope variables initial statement declarations are visiable in the <code>if</code> and <code>else</code> statements like C++17 does.</p> <p>A special constraint from Pyrope is that the initial statements and condition check can not have side-effects. Hence, they can not have <code>procedure</code> calls, only <code>function</code> calls.</p> PyropePyrope EquivalentC++17 equivalent <pre><code>var total=3\nif var x=3; x&lt;3 {\n  total+=x\n}elif var z=3; z&lt;4 {\n  total+=x+z\n}\n</code></pre> <pre><code>var total=3\n{\n  var x=3\n  if x&lt;3 {\n    total+=x\n  }else{\n    var z=3\n    if z&lt;4 {\n      total+=x+z\n    }\n  }\n}\n</code></pre> <pre><code>int total=3;\nif (int x=3; x&lt;3) {\n  total+=x;\n}else if (int z=3; z&lt;4) {\n  total+=x+z;\n}\n</code></pre> <p>Pyrope has <code>if</code> and <code>unique if</code>. The difference is that <code>unique if</code> guarantees that only one of the branch conditions is taken. It is possible to have all the conditions not taken. This allows synthesis optimizations because it implies that the condition is a one-hot encoding.</p> PyropeLNAST <pre><code>if var x=a ; x&lt;3 {\n  t = 100+x               // z not in scope\n}elif var z = x+c ; z&gt;5 {\n  t = 200+z+x             // z and x in scope\n}\n</code></pre> <pre><code>stmts\n  var\n    ref x\n    ref a\n  lt\n    ref ___1\n    ref x\n    const 3\n  if\n    ref ___1\n    stmts\n      add\n        ref t\n        const 100\n        ref x\n    stmts\n      add\n        ref ___2\n        ref x\n        ref c\n      var\n        ref z\n        ref ___2\n      gt\n        ref ___3\n        ref z\n        const 5\n      if\n        ref ___3\n        stmts\n          add\n            ref t\n            const 200\n            ref z\n            ref x\n</code></pre> <p>The <code>unique if</code> is similar, but all the conditions include and <code>optimize</code> directive to be checked. This means that the conditions must be checked even if the <code>else</code> is not reached. This is fine because neither the statements nor the condition checks are allowed to have side-effects.</p> <p>An important limitation of <code>unique if</code> is that only the first condition can have initial statement. It is not allowed to have initialization statements in the <code>elif</code> conditions.</p> PyropePyrope EquivalentLNAST <pre><code>unique if a&lt;3 {\n  y = 10\n}elif a&gt;40 {  // not allowed to do 'elif var z=40; a&gt;z'\n  y = 20+x\n}\n</code></pre> <pre><code>let tmp1 = a&lt;3\nlet tmp2 = a&gt;40\nlet tmp3 = 1&lt;&lt;(tmp1,tmp2)\noptimize tmp3@+[..]&lt;=1        // at most one bit set\n\nif tmp1 {\n  y = 10\n}elif tmp2 {\n  y = 20+x\n}\n</code></pre> <pre><code>lt\n  ref ___1\n  ref z\n  const 3\ngt\n  ref ___2\n  ref a\n  const 40\nshl           // create one-hot encoding\n  ref ___3\n  const 1\n  ref ___1\n  ref ___2\npopcount\n  ref ___4\n  ref ___3\nle\n  ref ___5\n  ref ___4\n  const 1\nfcall\n  ref nil\n  ref optimize\n  ref ___5\nif\n  ref ___1\n  stmts\n    assign\n      ref y\n      const 10\n  ref ___2\n  stmts\n    add\n      ref y\n      const 20\n      ref x\n</code></pre>"},{"location":"pyrope/12-lnast/#match","title":"match","text":"<p>The match statement behaves like a <code>unique if</code> but it also checks that at least one of the paths is taken. This means that if the <code>else</code> exists in the match, it behaves like a <code>unique if</code>. If the else does not exist, an <code>else { assert false }</code> is created.</p> PyropeLNAST <pre><code>var z = 0\nmatch x {\n == 3 { z = 1 }\n in 4..&lt;6 { z = 2 }\n}\n\nmatch x {\n &lt;  5 { z = 1 }\n else { z = 3 }\n}\n</code></pre> <pre><code>var\n  ref z\n  const 0\n\neq\n  ref ___0\n  ref x\n  const 3\nrange\n  ref ___2\n  const 4\n  const 5\nin\n  ref ___1\n  ref x\n  ref ___2\n\nshl\n  ref ___3\n  const 1\n  ref ___1\n  ref ___2\npopcount\n  ref ___4\n  ref ___3\nle\n  ref ___5\n  ref ___4\n  const 1\nfcall\n  ref nil\n  ref optimize\n  ref ___5\n\nif\n  ref ___1\n  stmts\n    assign\n      ref z\n      const 1\n  ref ___2\n  stmts\n    assign\n      ref z\n      const 2\n  stmts\n    fcall\n      ref ___6\n      ref assert\n      const false\n\n// 2nd match\nlt\n  ref ___6\n  ref x\n  const 5\nshl\n  ref ___7\n  const 1\n  ref ___6\npopcount\n  ref ___8\n  ref ___7\nle\n  ref ___9\n  ref ___8\n  const 1\nfcall\n  ref nil\n  ref optimize\n  ref ___9\nif\n  ref ___6\n  stmts\n    assign\n      ref z\n      const 1\n  stmts\n    assign\n      ref z\n      const 3\n</code></pre>"},{"location":"pyrope/12-lnast/#scope","title":"Scope","text":"<p>Like most languages Pyrope has variable scope, but it does not allow variable shadowing. This section showcases some cases on how the scope is generated.</p> <p>New variables can have a statement scope for <code>if</code>, <code>while</code>, and <code>match</code> statements.</p> PyropeLNAST <pre><code>if var x=3; x&lt;4 {\n  cassert x==3\n}\nwhile var z=1; x {\n  x -= z\n}\nvar z=0\nmatch var x=2 ; z+x {\n  == 2 { cassert true  }\n  != 7 { cassert true  }\n  else { cassert false }\n}\n</code></pre> <pre><code>stmts\n  var\n    ref x\n    const 3\n  lt\n    ref ___1\n    ref x\n    const 4\n  if\n    ref ___1\n    stmts\n      eq\n        ref ___2\n        ref x\n        const 3\n      fcall\n        ref ___0\n        ref cassert\n        ref ___2\n\nstmts\n  var\n    ref z\n    const 1\n  loop\n    if\n      ref x\n      stmts\n        break\n    sub\n      ref x\n      ref x\n      ref z\n\nvar\n  ref z\n  const 0\nstmts\n  var\n    ref x\n    const 2\n  add\n    ref ___3\n    ref z\n    ref x\n  eq\n    ref ___t1\n    ref ___3\n    const 2\n  ne\n    ref ___t2\n    ref ___3\n    const 7\n  shl           // create one-hot encoding\n    ref ___x\n    const 1\n    ref ___t1\n    ref ___t2\n  popcount\n    ref ___y\n    ref ___x\n  le\n    ref ___z\n    ref ___y\n    const 1\n  fcall\n    ref nil\n    ref optimize\n    ref ___z\n  if\n    ref ___t1\n    stmts\n      fcall\n        ref ___4\n        ref cassert\n        const true\n    ref ___t2\n    stmts\n      fcall\n        ref ___5\n        ref cassert\n        const true\n    stmts\n      fcall\n        ref ___6\n        ref cassert\n        const false\n</code></pre>"},{"location":"pyrope/12-lnast/#whileloopfor","title":"while/loop/for","text":"<p>Pyrope has <code>loop</code>, <code>while</code>, and <code>for</code> constructs to handle different types of loops. In all the cases, the loops must be expanded at LNAST compile time. In LNAST, there is only <code>loop</code> construct.</p> Pyrope loopLNAST <pre><code>loop {\n  i += 1\n  break when i==3\n}\n</code></pre> <pre><code>loop\n  add\n    ref i\n    ref i\n    const 1\n  eq\n    ref ___1\n    ref i\n    const 3\n  if\n    ref ___1\n    stmts\n      break\n</code></pre> <p>The <code>while</code> translates to a <code>loop</code> with a <code>break</code> statement.</p> Pyrope whileLNAST <pre><code>while var i=0 ; i!=3 {\n  i += 1\n}\n</code></pre> <pre><code>stmts\n  var\n    ref i\n    const 0\n  loop\n    neq\n      ref ___1\n      ref i\n      const 3\n    not\n      ref ___2\n      ref ___1\n    if\n      ref ___2\n      stmts\n        break\n</code></pre> <p>The <code>for</code> construct is also a loop, but it can have element, index, and key in the iterator. Also, it can allow a <code>ref</code> to mutate the contents.</p> Pyrope forPyrope ref forLNAST forLNAST ref for <pre><code>for (index,key,value) in enumerate(key(tup)) {\n  mycall(value,index,key)\n}\n</code></pre> <pre><code>for value in ref tup {\n  mycall(value)\n  value = 0\n}\n</code></pre> <pre><code>attr_get\n  ref ___tup_size\n  ref tup\n  const size\ngt\n  ref ___2\n  ref ___tup_size\n  const 0\nif\n  ref ___2\n  stmts\n    var\n      ref value\n      ref _\n    var\n      ref index\n      const 0\n    var\n      ref key\n      const \"\"\n    loop\n      attr_get\n        ref key\n        ref tup\n        ref index\n        const \"key\"\n      tup_get\n        ref value\n        ref tup\n        ref index\n      tup_add\n        ref ___6\n        ref index\n        ref key\n        ref value\n      fcall\n        ref ___empty\n        ref mycall\n        ref ___6\n      add\n        ref index\n        ref index\n        const 1\n      eq\n        ref ___3\n        ref ___tup_size\n        ref index\n      if\n        ref ___3\n        stmts\n          break\n</code></pre> <pre><code>attr_get\n  ref ___tup_size\n  ref tup\n  const size\ngt\n  ref ___2\n  ref ___tup_size\n  const 0\nif\n  ref ___2\n  stmts\n    var\n      ref value\n      ref _\n    var\n      ref index\n      const 0\n    var\n      ref key\n      const \"\"\n    loop\n      attr_get\n        ref key\n        ref tup\n        ref index\n        const \"key\"\n      tup_get\n        ref tup\n        ref index\n        ref value\n      tup_add\n        ref ___6\n        ref index\n        ref key\n        ref value\n      fcall\n        ref ___empty\n        ref mycall\n        ref ___6\n      tup_set\n        ref tup\n        ref index\n        ref value\n      add\n        ref index\n        ref index\n        const 1\n      eq\n        ref ___3\n        ref ___tup_size\n        ref index\n      if\n        ref ___3\n        stmts\n          break\n</code></pre> <p>The <code>for</code> comprehensions behave similarly, but the <code>cont</code>/<code>brk</code> statements have the value that must be concatenated (<code>tup_concat</code>) to the result. If the last statement is an expression, the value is contatenated.</p>"},{"location":"pyrope/12-lnast/#putsprintformat","title":"puts/print/format","text":"<p>All the string variables must be known at compile time, but it is still OK to pass strings as arguments to simulation functions that have no side-effects in the running simulation like <code>puts</code> and <code>print</code>.</p> <p><code>format</code> uses C++ fmt::format syntax and returns a string, so it must be solved at compile time. This means that the LNAST passes should have a <code>format</code> implementation to allow copy propagation to proceed. When format is used, a single  quote should be used to avoid string interpolation.</p> <p>The LNAST translation for all these instructions is just a normal function call. The <code>format</code> must be executed at compile time and propagate/copy as needed. The <code>puts</code>/<code>print</code> should generate simulation calls but not synthesis code.</p> PyropeLNAST <pre><code>let num = 1\nlet color = \"blue\"\nlet extension = \"s\"\n\nlet txt1 = \"I have {num} {color} potato{extension}\"  // interpolation\nlet txt2 = format('I have {:d} {} potato{}', num, color, extension)\n</code></pre> <pre><code>let\n  ref num\n  const 1\nlet\n  ref color\n  const blue\nlet\n  ref extension\n  const s\n\ntup_add\n  ref ___tmp\n  const \"I have {} {} potato{}\"\n  ref num\n  ref color\n  ref extension\n\nfcall\n  ref txt1\n  ref format\n  ref ___tmp\n\ntup_add\n  ref ___tmp2\n  const 'I have {:d} {} potato{}'\n  ref num\n  ref color\n  ref extension\n\nfcall\n  ref txt2\n  ref format\n  ref ___tmp2\n</code></pre>"},{"location":"pyrope/12-lnast/#lambda-call","title":"Lambda call","text":"<p>A lambda call arguments requires do not always require to be named like when a variable used matches a calling argument. To support the matching while processing the LNAST, the arguments tuple must be named for all the arguments unless an argument is an expression.</p> PyropeLNAST <pre><code>x = fcall(a,b=3,foo,1+2)\n</code></pre> <pre><code>add\n  ref ___t\n  const 1\n  const 2\n\ntup_add\n  ref ___args\n  let\n    ref a\n    ref a\n  let\n    ref b\n    const 3\n  let\n    ref foo\n    ref foo\n  ref ___t\n\nfcall\n  ref x\n  ref fcall\n  ref ___args\n</code></pre>"},{"location":"pyrope/13-stdlib/","title":"Pyrope Standard Library","text":"<p>This is a list of functionality that <code>import prp</code> should produce.</p>"},{"location":"pyrope/13-stdlib/#basic-operations","title":"Basic operations","text":"<p>All the LNAST node have an associated function matching name to simplify the creation of operations: <code>plus</code>, <code>minus</code>, <code>mult</code>, <code>div</code>, <code>mod</code>, <code>ror</code>... </p> <pre><code>let prp = import(\"prp\")\ncassert prp.plus(1,2,3) == 6\n</code></pre> <p>Library code: <pre><code>let plus = fun(...a:int)-&gt;(_:int) {\n  var r = 0\n  for e in a {\n    r += e\n  }\n  r\n}\n</code></pre></p>"},{"location":"pyrope/13-stdlib/#arraytuple-operators","title":"Array/Tuple operators","text":""},{"location":"pyrope/13-stdlib/#size-of-length","title":"Size of length","text":"<p>Sample use: <pre><code>let x = (1,2,23)\n\ncassert p.len(x) == 3\n</code></pre></p> <p>Library code: <pre><code>let len = fun(x) { x.[size] }\n</code></pre></p>"},{"location":"pyrope/13-stdlib/#map","title":"map","text":"<p>Sample use:</p> <pre><code>let x = (1,2,3)\n\ncassert x.map(fun(i){ i+1 }) == (2,3,4)\n</code></pre> <p>Library code: <pre><code>let map = fun&lt;T&gt;(f:fun(a:T),...x:[]T) {\n  return f(e) for e in x\n}\n</code></pre></p>"},{"location":"pyrope/13-stdlib/#filter","title":"filter","text":"<p>Sample use:</p> <pre><code>cassert (1,2,3).filter(fun(i){ i!=2 }) == (1,3)\n</code></pre> <p>Library code:</p> <pre><code>let filter = fun&lt;T&gt;(f:fun(a:T)-&gt;(_:Bool),...x:[]T) {\n  return e for e in x if not f(e)\n}\n</code></pre>"},{"location":"pyrope/13-stdlib/#reduce","title":"reduce","text":"<p>Sample use:</p> <pre><code>cassrt (1,2,3).reduce(prp.plus) == 6\n</code></pre> <p>Library code:</p> <pre><code>let reduce = fun(op:fun&lt;T&gt;(a:T,b:T)-&gt;(_:T), ...x) {\n  return x when x.[size] &lt;= 1\n\n  var res = x[0]\n  for i in x[1..] {\n    res = op(res, i)\n  }\n  return res\n}\n</code></pre>"},{"location":"pyrope/13-stdlib/#todo","title":"TODO","text":"<p>It would be nice to have the same methods (and names) as the c++20 <code>std::views</code>  adaptors so that it is easier for developers to get familiar. E.g: filter,  transform, drop, join, split, reverse, common, counted...</p> <p>https://en.cppreference.com/w/cpp/ranges</p>"}]}